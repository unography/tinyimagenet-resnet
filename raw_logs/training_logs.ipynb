{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_CLR_60Reached.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n_H3HqYH4Ve",
        "colab_type": "text"
      },
      "source": [
        "#### Imports, Data Fetching and Google Drive handling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQRg9hsUH_IZ",
        "colab_type": "code",
        "outputId": "b5f5e292-4ebd-4108-8dad-0f94e2df6c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install imgaug"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.9)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.15.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.16.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.4.5.20)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.6.4.post2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (4.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.12.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.0.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->imgaug) (0.46)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.4.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHoUHv5GKEx7",
        "colab_type": "code",
        "outputId": "fd01dae3-5531-42d7-bd9b-b428d31470f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import glob\n",
        "import six\n",
        "import keras\n",
        "import imgaug as ia\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from imgaug import augmenters as iaa\n",
        "from sklearn import preprocessing\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D,\n",
        "    SeparableConv2D\n",
        ")\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import *\n",
        "from keras.activations import softmax\n",
        "from keras.models import load_model \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "NUM_CLASSES = 200\n",
        "NUM_IMAGES_PER_CLASS = 500\n",
        "NUM_IMAGES = NUM_CLASSES * NUM_IMAGES_PER_CLASS\n",
        "BASE_IMAGE_PATH = 'tiny-imagenet-200'\n",
        "TRAINING_IMAGES_DIR = 'tiny-imagenet-200/train/'\n",
        "TRAIN_SIZE = NUM_IMAGES\n",
        "\n",
        "NUM_TRAIN_IMAGES = 100000\n",
        "NUM_VAL_IMAGES = 10000\n",
        "VAL_IMAGES_DIR = 'tiny-imagenet-200/val/'\n",
        "\n",
        "IMAGE_SIZE = 64\n",
        "NUM_CHANNELS = 3\n",
        "IMAGE_ARR_SIZE = IMAGE_SIZE * IMAGE_SIZE * NUM_CHANNELS\n",
        "SEED_NUM = 42\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1c7QqFSKmUI",
        "colab_type": "code",
        "outputId": "75a6304f-4a4c-4f10-ddda-adfd6e0ff851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "def fetch_data():\n",
        "  if not os.path.isdir('tiny-imagenet-200'):\n",
        "    os.system('wget http://cs231n.stanford.edu/tiny-imagenet-200.zip')\n",
        "    os.system('unzip -qq tiny-imagenet-200.zip')\n",
        "\n",
        "\n",
        "def reset_graph(seed):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "reset_graph(seed=SEED_NUM)\n",
        "drive.mount('/content/gdrive')\n",
        "fetch_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hicQIgqcLNRY",
        "colab_type": "text"
      },
      "source": [
        "#### Image loading, augmentation utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sloyiu02LVe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(img, random_crop_size):\n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "\n",
        "def crop_generator(batches, crop_length):\n",
        "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
        "    crops from the image batches generated by the original iterator.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        batch_x, batch_y = next(batches)\n",
        "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
        "        yield (batch_crops, batch_y)\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "  def on_train_begin(self, logs={}):\n",
        "     self.losses = []\n",
        "     self.lr = []\n",
        "\n",
        "  def on_epoch_end(self, batch, logs={}):\n",
        "     self.losses.append(logs.get('loss'))\n",
        "     self.lr.append(step_decay(len(self.losses)))\n",
        "\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer.lr\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxjb71BrLct9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_generators(batch_size_train, batch_size_val, no_aug=False, \n",
        "                   img_size_train=(64,64), img_size_val=(64,64),\n",
        "                    use_random_crop=True, random_crop_size=32):\n",
        "  val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "  val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "  \n",
        "  sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "#   seq = iaa.Sequential([\n",
        "#       iaa.Fliplr(0.5),\n",
        "#       iaa.CoarseDropout(0.3, size_percent=(0,0.02))\n",
        "#   ])\n",
        "  \n",
        "  seq = iaa.Sequential([\n",
        "      iaa.Fliplr(0.5), # horizontal flips\n",
        "      iaa.Flipud(0.2), # vertical flips\n",
        "      \n",
        "      # Small gaussian blur with random sigma between 0 and 0.5.\n",
        "      # But we only blur about 50% of all images.\n",
        "      sometimes(iaa.GaussianBlur(sigma=(0, 2.0))),\n",
        "      \n",
        "      # crop images by -10% to 20% of their height/width\n",
        "      sometimes(iaa.CropAndPad(\n",
        "          percent=(-0.1, 0.2),\n",
        "          pad_mode=ia.ALL,\n",
        "          pad_cval=(0, 255)\n",
        "        )),\n",
        "      \n",
        "      # Apply affine transformations to some of the images\n",
        "      # - scale to 80-120% of image height/width (each axis independently)\n",
        "      # - translate by -20 to +20 relative to height/width (per axis)\n",
        "      # - rotate by -45 to +45 degrees\n",
        "      # - shear by -16 to +16 degrees\n",
        "      # - order: use nearest neighbour or bilinear interpolation (fast)\n",
        "      # - mode: use any available mode to fill newly created pixels\n",
        "      #         see API or scikit-image for which modes are available\n",
        "      # - cval: if the mode is constant, then use a random brightness\n",
        "      #         for the newly created pixels (e.g. sometimes black,\n",
        "      #         sometimes white)\n",
        "      sometimes(iaa.Affine(\n",
        "          scale={\"x\": (0.8, 1.5), \"y\": (0.8, 1.5)},\n",
        "          translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "          rotate=(-45, 45),\n",
        "          shear=(-16, 16),\n",
        "          order=[0, 1],\n",
        "          cval=(0, 255),\n",
        "          mode=ia.ALL\n",
        "      )),\n",
        "      \n",
        "      #drop 2-5% percent of the original size, leading to large dropped\n",
        "      # rectangles.\n",
        "      sometimes(iaa.CoarseDropout(\n",
        "                        (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                        per_channel=0.2\n",
        "                    )),\n",
        "                \n",
        "      # Make some images brighter and some darker.\n",
        "      # In 20% of all cases, we sample the multiplier once per channel,\n",
        "      # which can end up changing the color of the images.\n",
        "#       sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.2)),\n",
        "      \n",
        "      #Improve or worsen the contrast of images.\n",
        "      #Comment it out after third model run (extreme saturation)\n",
        "#       sometimes(iaa.ContrastNormalization((0.75, 1.5), per_channel=0.5)), \n",
        "     ],\n",
        "     # do all of the above augmentations in random order\n",
        "     random_order = True) # apply augmenters in random order\n",
        "  \n",
        "  if no_aug:\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale= 1./255\n",
        "    )\n",
        "  else:\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=seq.augment_image,\n",
        "        rescale= 1./255\n",
        "    )\n",
        "  \n",
        "\n",
        "  valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  \n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "    r'./tiny-imagenet-200/train/', \n",
        "    target_size=img_size_train, \n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size_train, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED_NUM\n",
        "  )\n",
        "  if use_random_crop:\n",
        "    train_generator_final = crop_generator(train_generator, random_crop_size)\n",
        "  else:\n",
        "    train_generator_final = train_generator\n",
        "    \n",
        "  validation_generator = valid_datagen.flow_from_dataframe(\n",
        "      val_data, \n",
        "      directory='./tiny-imagenet-200/val/images/', \n",
        "      x_col='File', y_col='Class', \n",
        "      target_size=img_size_val,\n",
        "      color_mode='rgb', \n",
        "      class_mode='categorical', \n",
        "      batch_size=batch_size_val, \n",
        "      shuffle=True, \n",
        "      seed=SEED_NUM\n",
        "  )\n",
        "  \n",
        "  return train_generator_final, validation_generator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgzq1HUtq-b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MixupImageDataGenerator():\n",
        "    def __init__(self, generator, directory, batch_size, img_height, img_width, alpha=0.2, subset=None):\n",
        "        \"\"\"Constructor for mixup image data generator.\n",
        "        Arguments:\n",
        "            generator {object} -- An instance of Keras ImageDataGenerator.\n",
        "            directory {str} -- Image directory.\n",
        "            batch_size {int} -- Batch size.\n",
        "            img_height {int} -- Image height in pixels.\n",
        "            img_width {int} -- Image width in pixels.\n",
        "        Keyword Arguments:\n",
        "            alpha {float} -- Mixup beta distribution alpha parameter. (default: {0.2})\n",
        "            subset {str} -- 'training' or 'validation' if validation_split is specified in\n",
        "            `generator` (ImageDataGenerator).(default: {None})\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_index = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # First iterator yielding tuples of (x, y)\n",
        "        print(directory)\n",
        "        self.generator1 = generator.flow_from_directory(\n",
        "    directory=r'./tiny-imagenet-200/train/', \n",
        "    target_size=(img_height, img_width), \n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED_NUM\n",
        "  )\n",
        "        \n",
        "\n",
        "        # Second iterator yielding tuples of (x, y)\n",
        "        self.generator2 = generator.flow_from_directory(\n",
        "    r'./tiny-imagenet-200/train/', \n",
        "    target_size=(img_height, img_width), \n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED_NUM\n",
        "  )\n",
        "        \n",
        "\n",
        "        # Number of images across all classes in image directory.\n",
        "        self.n = self.generator1.samples\n",
        "\n",
        "    def reset_index(self):\n",
        "        \"\"\"Reset the generator indexes array.\n",
        "        \"\"\"\n",
        "\n",
        "        self.generator1._set_index_array()\n",
        "        self.generator2._set_index_array()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.reset_index()\n",
        "\n",
        "    def reset(self):\n",
        "        self.batch_index = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        # round up\n",
        "        return (self.n + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def get_steps_per_epoch(self):\n",
        "        \"\"\"Get number of steps per epoch based on batch size and\n",
        "        number of images.\n",
        "        Returns:\n",
        "            int -- steps per epoch.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.n // self.batch_size\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Get next batch input/output pair.\n",
        "        Returns:\n",
        "            tuple -- batch of input/output pair, (inputs, outputs).\n",
        "        \"\"\"\n",
        "\n",
        "        if self.batch_index == 0:\n",
        "            self.reset_index()\n",
        "\n",
        "        current_index = (self.batch_index * self.batch_size) % self.n\n",
        "        if self.n > current_index + self.batch_size:\n",
        "            self.batch_index += 1\n",
        "        else:\n",
        "            self.batch_index = 0\n",
        "\n",
        "        # random sample the lambda value from beta distribution.\n",
        "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
        "\n",
        "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
        "        y_l = l.reshape(self.batch_size, 1)\n",
        "\n",
        "        # Get a pair of inputs and outputs from two iterators.\n",
        "        X1, y1 = self.generator1.next()\n",
        "        X2, y2 = self.generator2.next()\n",
        "        \n",
        "        if X1.shape[0] != self.batch_size:\n",
        "          l = np.random.beta(self.alpha, self.alpha, X1.shape[0])\n",
        "          X_l = l.reshape(X1.shape[0], 1, 1, 1)\n",
        "          y_l = l.reshape(X1.shape[0], 1)\n",
        "          \n",
        "\n",
        "        # Perform the mixup.\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "        y = y1 * y_l + y2 * (1 - y_l)\n",
        "          \n",
        "        return X, y\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            yield next(self)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPZ6zsYSrAMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mixup_generator(batch_size_train, batch_size_val, no_aug=False, \n",
        "                   img_size_train=(64,64), img_size_val=(64,64),\n",
        "                    use_random_crop=True, random_crop_size=32):\n",
        "  \n",
        "  val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "  val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "  \n",
        "  sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "#   seq = iaa.Sequential([\n",
        "#       iaa.Fliplr(0.5),\n",
        "#       iaa.CoarseDropout(0.3, size_percent=(0,0.02))\n",
        "#   ])\n",
        "  \n",
        "  seq = iaa.Sequential([\n",
        "      iaa.Fliplr(0.5), # horizontal flips\n",
        "      iaa.Flipud(0.2), # vertical flips\n",
        "      \n",
        "      # Small gaussian blur with random sigma between 0 and 0.5.\n",
        "      # But we only blur about 50% of all images.\n",
        "      sometimes(iaa.GaussianBlur(sigma=(0, 2.0))),\n",
        "      \n",
        "      # crop images by -10% to 20% of their height/width\n",
        "      sometimes(iaa.CropAndPad(\n",
        "          percent=(-0.1, 0.2),\n",
        "          pad_mode=ia.ALL,\n",
        "          pad_cval=(0, 255)\n",
        "        )),\n",
        "      \n",
        "      # Apply affine transformations to some of the images\n",
        "      # - scale to 80-120% of image height/width (each axis independently)\n",
        "      # - translate by -20 to +20 relative to height/width (per axis)\n",
        "      # - rotate by -45 to +45 degrees\n",
        "      # - shear by -16 to +16 degrees\n",
        "      # - order: use nearest neighbour or bilinear interpolation (fast)\n",
        "      # - mode: use any available mode to fill newly created pixels\n",
        "      #         see API or scikit-image for which modes are available\n",
        "      # - cval: if the mode is constant, then use a random brightness\n",
        "      #         for the newly created pixels (e.g. sometimes black,\n",
        "      #         sometimes white)\n",
        "      sometimes(iaa.Affine(\n",
        "          scale={\"x\": (0.8, 1.5), \"y\": (0.8, 1.5)},\n",
        "          translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "          rotate=(-45, 45),\n",
        "          shear=(-16, 16),\n",
        "          order=[0, 1],\n",
        "          cval=(0, 255),\n",
        "          mode=ia.ALL\n",
        "      )),\n",
        "      \n",
        "      #drop 2-5% percent of the original size, leading to large dropped\n",
        "      # rectangles.\n",
        "      sometimes(iaa.CoarseDropout(\n",
        "                        (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                        per_channel=0.2\n",
        "                    )),\n",
        "                \n",
        "      # Make some images brighter and some darker.\n",
        "      # In 20% of all cases, we sample the multiplier once per channel,\n",
        "      # which can end up changing the color of the images.\n",
        "      sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.2)),\n",
        "      \n",
        "      #Improve or worsen the contrast of images.\n",
        "      #Comment it out after third model run (extreme saturation)\n",
        "      sometimes(iaa.ContrastNormalization((0.75, 1.5), per_channel=0.5)), \n",
        "     ],\n",
        "     # do all of the above augmentations in random order\n",
        "     random_order = True) # apply augmenters in random order\n",
        "  \n",
        "  if no_aug:\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale= 1./255\n",
        "    )\n",
        "  else:\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=seq.augment_image,\n",
        "        rescale= 1./255\n",
        "    )\n",
        "  \n",
        "\n",
        "  valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  \n",
        "#   train_generator = train_datagen.flow_from_directory(\n",
        "#     r'./tiny-imagenet-200/train/', \n",
        "#     target_size=img_size_train, \n",
        "#     color_mode='rgb',\n",
        "#     batch_size=batch_size_train, \n",
        "#     class_mode='categorical', \n",
        "#     shuffle=True, \n",
        "#     seed=SEED_NUM\n",
        "#   )\n",
        "\n",
        "  train_generator = MixupImageDataGenerator(generator=train_datagen,\n",
        "                                              directory=r'./tiny-imagenet-200/train/',\n",
        "                                              batch_size=batch_size_train,\n",
        "                                              img_height=img_size_train[1],\n",
        "                                              img_width=img_size_train[0],\n",
        "                                              alpha=0.4)\n",
        "  \n",
        "  \n",
        "  if use_random_crop:\n",
        "    train_generator_final = crop_generator(train_generator, random_crop_size)\n",
        "  else:\n",
        "    train_generator_final = train_generator\n",
        "    \n",
        "  validation_generator = valid_datagen.flow_from_dataframe(\n",
        "      val_data, \n",
        "      directory='./tiny-imagenet-200/val/images/', \n",
        "      x_col='File', y_col='Class', \n",
        "      target_size=img_size_val,\n",
        "      color_mode='rgb', \n",
        "      class_mode='categorical', \n",
        "      batch_size=batch_size_val, \n",
        "      shuffle=True, \n",
        "      seed=SEED_NUM\n",
        "  )\n",
        "  \n",
        "  return train_generator_final, validation_generator\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPT9YYgMP3u",
        "colab_type": "text"
      },
      "source": [
        "#### Weighted Cross Entropy Loss Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pkP43HXMURa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_report(class_labels, report,factor):\n",
        "  label_idx_map = {}\n",
        "  for i in range(len(class_labels)):\n",
        "    label_idx_map[class_labels[i]] = i\n",
        "    \n",
        "  wts = [1] * len(label_idx_map)\n",
        "  report_data = []\n",
        "  lines = report.split('\\n')\n",
        "  label_score_map = {}\n",
        "  for line in lines[2:-4]:\n",
        "      row = {}\n",
        "      row_data = line.split('      ')\n",
        "      if not row_data[0]:\n",
        "        break\n",
        "      label = row_data[0].strip()\n",
        "      f1_score = float(row_data[3])\n",
        "      label_score_map[label] = f1_score\n",
        "      idx = label_idx_map[label]\n",
        "      wts[idx]= 1 + (1 - f1_score) * factor\n",
        "  \n",
        "  return wts\n",
        "      \n",
        "def get_wts(model, factor=1, debug=False):\n",
        "  from sklearn.metrics import classification_report\n",
        "\n",
        "#   K.clear_session()\n",
        "  BATCH_SIZE_TRAIN = 128\n",
        "\n",
        "  BATCH_SIZE_VAL = 100\n",
        "  steps = NUM_VAL_IMAGES // BATCH_SIZE_VAL\n",
        "\n",
        "  valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "  val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "\n",
        "  validation_generator_eval = valid_datagen.flow_from_dataframe(\n",
        "      val_data, \n",
        "      directory='./tiny-imagenet-200/val/images/', \n",
        "      x_col='File', y_col='Class', \n",
        "      target_size=(64,64),\n",
        "      color_mode='rgb', \n",
        "      class_mode='categorical', \n",
        "      batch_size=BATCH_SIZE_VAL, \n",
        "      shuffle=False, \n",
        "      seed=SEED_NUM\n",
        "  )\n",
        "\n",
        "  predictions = model.predict_generator(validation_generator_eval, steps=steps, verbose=1)\n",
        "  predicted_classes = np.argmax(predictions, axis=1) \n",
        "  true_classes = validation_generator_eval.classes\n",
        "  class_labels = list(validation_generator_eval.class_indices.keys())\n",
        "  report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "  f1_wts = parse_report(class_labels, report,factor)\n",
        "  \n",
        "  return f1_wts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKdUN3KpMtQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    weights = K.variable(weights)\n",
        "        \n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Kjlo-nMzYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahvukiwFOHEn",
        "colab_type": "text"
      },
      "source": [
        "#### Cyclic LR utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IxZrD5OOOcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OneCycleLR(Callback):\n",
        "    def __init__(self,\n",
        "                 max_lr,\n",
        "                 batch_size=None,\n",
        "                 samples=None,\n",
        "                 end_percentage=0.1,\n",
        "                 scale_percentage=None,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 verbose=True):\n",
        "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
        "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
        "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
        "        100th its initial lowest value.\n",
        "        # Arguments:\n",
        "            max_lr: Float. Initial learning rate. This also sets the\n",
        "                starting learning rate (which will be 10x smaller than\n",
        "                this), and will increase to this value during the first cycle.\n",
        "            end_percentage: Float. The percentage of all the epochs of training\n",
        "                that will be dedicated to sharply decreasing the learning\n",
        "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
        "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
        "                If None, it will compute the scale_percentage automatically\n",
        "                based on the `end_percentage`.\n",
        "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
        "                value, which gradually drops to its lowest value in half-cycle,\n",
        "                then gradually increases again to stay constant at this max value.\n",
        "                Can only be used with SGD Optimizer.\n",
        "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
        "                the half-cycle. Can only be used with SGD Optimizer.\n",
        "            verbose: Bool. Whether to print the current learning rate after every\n",
        "                epoch.\n",
        "        # Reference\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
        "        \"\"\"\n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
        "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.batch_size = batch_size\n",
        "        self.samples=samples\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = None\n",
        "        self.steps = None\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the learning rate based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the learning rate gradually increases.\n",
        "        - If in the second half of training, the learning rate gradually decreases.\n",
        "        - If in the final `end_percentage` portion of training, the learning rate\n",
        "            is quickly reduced to near 100th of the original min learning rate.\n",
        "        # Returns:\n",
        "            the new learning rate\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
        "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
        "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
        "                                              (1. - 100.) / 100.)) * self.scale\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - (\n",
        "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
        "            new_lr = self.initial_lr * (1. + current_percentage *\n",
        "                                        (self.scale * 100 - 1.)) * self.scale\n",
        "\n",
        "        if self.clr_iterations == self.num_iterations:\n",
        "            self.clr_iterations = 0\n",
        "\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "        \"\"\"\n",
        "         Compute the momentum based on which phase of the cycle it is in.\n",
        "        - If in the first half of training, the momentum gradually decreases.\n",
        "        - If in the second half of training, the momentum gradually increases.\n",
        "        - If in the final `end_percentage` portion of training, the momentum value\n",
        "            is kept constant at the maximum initial value.\n",
        "        # Returns:\n",
        "            the new momentum value\n",
        "        \"\"\"\n",
        "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
        "            new_momentum = self.max_momentum\n",
        "\n",
        "        elif self.clr_iterations > self.mid_cycle_id:\n",
        "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
        "                                        self.mid_cycle_id))\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        else:\n",
        "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
        "            new_momentum = self.max_momentum - current_percentage * (\n",
        "                self.max_momentum - self.min_momentum)\n",
        "\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "        \n",
        "        print('--->params', self.params)\n",
        "\n",
        "        self.epochs = self.params['epochs']\n",
        "        if 'batch_size' in self.params:\n",
        "          self.batch_size = self.params['batch_size']\n",
        "        if 'samples' in self.params:\n",
        "          self.samples = self.params['samples']\n",
        "        self.steps = self.params['steps']\n",
        "\n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    def __init__(self,\n",
        "                 num_samples,\n",
        "                 batch_size,\n",
        "                 minimum_lr=1e-5,\n",
        "                 maximum_lr=10.,\n",
        "                 lr_scale='exp',\n",
        "                 validation_data=None,\n",
        "                 validation_generator=None,\n",
        "                 validation_sample_rate=5,\n",
        "                 stopping_criterion_factor=4.,\n",
        "                 loss_smoothing_beta=0.98,\n",
        "                 save_dir=None,\n",
        "                 verbose=True):\n",
        "        \"\"\"\n",
        "        This class uses the Cyclic Learning Rate history to find a\n",
        "        set of learning rates that can be good initializations for the\n",
        "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
        "        below.\n",
        "        A port of the Fast.ai implementation for Keras.\n",
        "        # Note\n",
        "        This requires that the model be trained for exactly 1 epoch. If the model\n",
        "        is trained for more epochs, then the metric calculations are only done for\n",
        "        the first epoch.\n",
        "        # Interpretation\n",
        "        Upon visualizing the loss plot, check where the loss starts to increase\n",
        "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
        "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
        "        Choose the max value as this value when passing the `max_val` argument\n",
        "        to OneCycleLR callback.\n",
        "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
        "        # Arguments:\n",
        "            num_samples: Integer. Number of samples in the dataset.\n",
        "            batch_size: Integer. Batch size during training.\n",
        "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
        "            maximum_lr: Float. Final learning rate (and the maximum).\n",
        "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
        "                scaling for each update to the learning rate during subsequent\n",
        "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
        "            validation_data: Requires the validation dataset as a tuple of\n",
        "                (X, y) belonging to the validation set. If provided, will use the\n",
        "                validation set to compute the loss metrics. Else uses the training\n",
        "                batch loss. Will warn if not provided to alert the user.\n",
        "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
        "                validation set per iteration of the LRFinder. Larger number of\n",
        "                samples will reduce the variance but will take longer time to execute\n",
        "                per batch.\n",
        "                If Positive > 0, will sample from the validation dataset\n",
        "                If Megative, will use the entire dataset\n",
        "            stopping_criterion_factor: Integer or None. A factor which is used\n",
        "                to measure large increase in the loss value during training.\n",
        "                Since callbacks cannot stop training of a model, it will simply\n",
        "                stop logging the additional values from the epochs after this\n",
        "                stopping criterion has been met.\n",
        "                If None, this check will not be performed.\n",
        "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
        "                average of the loss function.\n",
        "            save_dir: Optional, String. If passed a directory path, the callback\n",
        "                will save the running loss and learning rates to two separate numpy\n",
        "                arrays inside this directory. If the directory in this path does not\n",
        "                exist, they will be created.\n",
        "            verbose: Whether to print the learning rate after every batch of training.\n",
        "        # References:\n",
        "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
        "        \"\"\"\n",
        "        super(LRFinder, self).__init__()\n",
        "\n",
        "        if lr_scale not in ['exp', 'linear']:\n",
        "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
        "\n",
        "        if validation_data is not None:\n",
        "            self.validation_data = validation_data\n",
        "            self.use_validation_set = True\n",
        "\n",
        "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
        "                self.validation_sample_rate = validation_sample_rate\n",
        "            else:\n",
        "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than 0\")\n",
        "            self.validation_generator = None\n",
        "        elif validation_generator is not None:\n",
        "            self.validation_generator = validation_generator\n",
        "            self.use_validation_set = True\n",
        "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
        "                self.validation_sample_rate = validation_sample_rate\n",
        "            else:\n",
        "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than 0\")\n",
        "            \n",
        "        else:\n",
        "            self.use_validation_set = False\n",
        "            self.validation_sample_rate = 0\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        self.initial_lr = minimum_lr\n",
        "        self.final_lr = maximum_lr\n",
        "        self.lr_scale = lr_scale\n",
        "        self.stopping_criterion_factor = stopping_criterion_factor\n",
        "        self.loss_smoothing_beta = loss_smoothing_beta\n",
        "        self.save_dir = save_dir\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.num_batches_ = num_samples // batch_size\n",
        "        self.current_lr_ = minimum_lr\n",
        "\n",
        "        if lr_scale == 'exp':\n",
        "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
        "                1. / float(self.num_batches_))\n",
        "        else:\n",
        "            extra_batch = int((num_samples % batch_size) != 0)\n",
        "            self.lr_multiplier_ = np.linspace(\n",
        "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
        "\n",
        "        # If negative, use entire validation set\n",
        "        if self.validation_sample_rate < 0:\n",
        "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
        "\n",
        "        self.current_batch_ = 0\n",
        "        self.current_epoch_ = 0\n",
        "        self.best_loss_ = 1e6\n",
        "        self.running_loss_ = 0.\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "\n",
        "        self.current_epoch_ = 1\n",
        "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
        "\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.current_batch_ = 0\n",
        "\n",
        "        if self.current_epoch_ > 1:\n",
        "            warnings.warn(\n",
        "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
        "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.current_batch_ += 1\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if self.current_epoch_ > 1:\n",
        "            return\n",
        "\n",
        "        if self.use_validation_set:\n",
        "            if self.validation_generator is not None:\n",
        "                x, y = self.validation_generator.next()\n",
        "            else:\n",
        "                X, Y = self.validation_data[0], self.validation_data[1]\n",
        "\n",
        "                # use 5 random batches from test set for fast approximate of loss\n",
        "                num_samples = self.batch_size * self.validation_sample_rate\n",
        "\n",
        "                if num_samples > X.shape[0]:\n",
        "                    num_samples = X.shape[0]\n",
        "\n",
        "                idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
        "                x = X[idx]\n",
        "                y = Y[idx]\n",
        "\n",
        "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
        "            loss = values[0]\n",
        "        else:\n",
        "            loss = logs['loss']\n",
        "\n",
        "        # smooth the loss value and bias correct\n",
        "        running_loss = self.loss_smoothing_beta * loss + (\n",
        "            1. - self.loss_smoothing_beta) * loss\n",
        "        running_loss = running_loss / (\n",
        "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
        "\n",
        "        # stop logging if loss is too large\n",
        "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
        "                running_loss >\n",
        "                self.stopping_criterion_factor * self.best_loss_):\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
        "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
        "            return\n",
        "\n",
        "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
        "            self.best_loss_ = running_loss\n",
        "\n",
        "        current_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
        "        if self.lr_scale == 'exp':\n",
        "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
        "        else:\n",
        "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
        "\n",
        "        # compute the lr for the next batch and update the optimizer lr\n",
        "        if self.lr_scale == 'exp':\n",
        "            current_lr *= self.lr_multiplier_\n",
        "        else:\n",
        "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
        "\n",
        "        K.set_value(self.model.optimizer.lr, current_lr)\n",
        "\n",
        "        # save the other metrics as well\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        if self.verbose:\n",
        "            if self.use_validation_set:\n",
        "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
        "                      (values[0], current_lr))\n",
        "            else:\n",
        "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
        "            if not os.path.exists(self.save_dir):\n",
        "                os.makedirs(self.save_dir)\n",
        "\n",
        "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
        "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
        "\n",
        "            np.save(losses_path, self.losses)\n",
        "            np.save(lrs_path, self.lrs)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
        "                      % (self.save_dir))\n",
        "\n",
        "        self.current_epoch_ += 1\n",
        "\n",
        "        warnings.simplefilter(\"default\")\n",
        "\n",
        "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the callback itself.\n",
        "        # Arguments:\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\n",
        "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses = self.losses\n",
        "        lrs = self.lrs\n",
        "\n",
        "        if clip_beginning:\n",
        "            losses = losses[clip_beginning:]\n",
        "            lrs = lrs[clip_beginning:]\n",
        "\n",
        "        if clip_endding:\n",
        "            losses = losses[:clip_endding]\n",
        "            lrs = lrs[:clip_endding]\n",
        "\n",
        "        plt.plot(lrs, losses)\n",
        "        plt.title('Learning rate vs Loss')\n",
        "        plt.xlabel('learning rate')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "    @classmethod\n",
        "    def restore_schedule_from_dir(cls,\n",
        "                                  directory,\n",
        "                                  clip_beginning=None,\n",
        "                                  clip_endding=None):\n",
        "        \"\"\"\n",
        "        Loads the training history from the saved numpy files in the given directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        Returns:\n",
        "            tuple of (losses, learning rates)\n",
        "        \"\"\"\n",
        "        if clip_beginning is not None and clip_beginning < 0:\n",
        "            clip_beginning = -clip_beginning\n",
        "\n",
        "        if clip_endding is not None and clip_endding > 0:\n",
        "            clip_endding = -clip_endding\n",
        "\n",
        "        losses_path = os.path.join(directory, 'losses.npy')\n",
        "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
        "\n",
        "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
        "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
        "                  (losses_path, lrs_path, directory))\n",
        "\n",
        "            losses = None\n",
        "            lrs = None\n",
        "\n",
        "        else:\n",
        "            losses = np.load(losses_path)\n",
        "            lrs = np.load(lrs_path)\n",
        "\n",
        "            if clip_beginning:\n",
        "                losses = losses[clip_beginning:]\n",
        "                lrs = lrs[clip_beginning:]\n",
        "\n",
        "            if clip_endding:\n",
        "                losses = losses[:clip_endding]\n",
        "                lrs = lrs[:clip_endding]\n",
        "\n",
        "        return losses, lrs\n",
        "\n",
        "    @classmethod\n",
        "    def plot_schedule_from_file(cls,\n",
        "                                directory,\n",
        "                                clip_beginning=None,\n",
        "                                clip_endding=None):\n",
        "        \"\"\"\n",
        "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
        "        rate values in the specified directory.\n",
        "        # Arguments:\n",
        "            directory: String. Path to the directory where the serialized numpy\n",
        "                arrays of the loss and learning rates are saved.\n",
        "            clip_beginning: Integer or None. If positive integer, it will\n",
        "                remove the specified portion of the loss graph to remove the large\n",
        "                loss values in the beginning of the graph.\n",
        "            clip_endding: Integer or None. If negative integer, it will\n",
        "                remove the specified portion of the ending of the loss graph to\n",
        "                remove the sharp increase in the loss values at high learning rates.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            plt.style.use('seaborn-white')\n",
        "        except ImportError:\n",
        "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
        "            return\n",
        "\n",
        "        losses, lrs = cls.restore_schedule_from_dir(\n",
        "            directory,\n",
        "            clip_beginning=clip_beginning,\n",
        "            clip_endding=clip_endding)\n",
        "\n",
        "        if losses is None or lrs is None:\n",
        "            return\n",
        "        else:\n",
        "            plt.plot(lrs, losses)\n",
        "            plt.title('Learning rate vs Loss')\n",
        "            plt.xlabel('learning rate')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show()\n",
        "\n",
        "    @property\n",
        "    def lrs(self):\n",
        "        return np.array(self.history['log_lrs'])\n",
        "\n",
        "    @property\n",
        "    def losses(self):\n",
        "        return np.array(self.history['running_loss_'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvIqus9dM1s4",
        "colab_type": "text"
      },
      "source": [
        "#### Training utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQuDppnkM_eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "get_model: loads and returns the model - used for resuming training, or for evaluating\n",
        "\"\"\"\n",
        "def get_model(epochs, batch_size, initial_epoch=None, weights_path=None, \n",
        "                c_lr=None, lr=None,rlr=False, any_size=False, no_train=False, \n",
        "                model=None, rlr_factor=0.1, use_focal_loss=False, \n",
        "              use_weighted_loss=False, weighted_loss_weights=None):\n",
        "  \n",
        "  base_checkpoint_path = '/content/gdrive/My Drive/Resnet-imgaug'\n",
        "  if not os.path.isdir(base_checkpoint_path):\n",
        "    os.makedirs(base_checkpoint_path)\n",
        "  filepath = base_checkpoint_path + \"/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=rlr_factor, patience=3)\n",
        "\n",
        "  callbacks_list = [checkpoint]\n",
        "  if rlr:\n",
        "    callbacks_list = [reduce_lr, checkpoint]\n",
        "  \n",
        "  if not model:\n",
        "    if any_size:\n",
        "      model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES)\n",
        "    else:\n",
        "      model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), NUM_CLASSES)\n",
        "  if weights_path:\n",
        "    model.load_weights(weights_path)\n",
        "    \n",
        "  op=Adam()\n",
        "  if lr:\n",
        "    op=Adam(lr=lr)\n",
        "    \n",
        "  lr_metric = get_lr_metric(op)\n",
        "  if use_focal_loss:\n",
        "    loss = [focal_loss(alpha=.25, gamma=2)]\n",
        "  elif use_weighted_loss and weighted_loss_weights is not None:\n",
        "    loss = weighted_categorical_crossentropy(weighted_loss_weights)\n",
        "  else:\n",
        "    loss = 'categorical_crossentropy'\n",
        "  \n",
        "  print(\"Loss used: \", loss)\n",
        "  model.compile(loss=loss,\n",
        "              optimizer=op,\n",
        "              metrics=['accuracy', lr_metric])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\"\"\"\n",
        "train_model: function which handles the actual training\n",
        "\"\"\"\n",
        "def train_model(epochs, batch_size, initial_epoch=None, weights_path=None, \n",
        "                c_lr=None, lr=None,rlr=False, any_size=False, no_train=False, \n",
        "                model=None, rlr_factor=0.1, use_focal_loss=False,\n",
        "               use_weighted_loss=False, weighted_loss_weights=None,\n",
        "               patience=3):\n",
        "  \n",
        "  base_checkpoint_path = '/content/gdrive/My Drive/Resnet-imgaug'\n",
        "  if not os.path.isdir(base_checkpoint_path):\n",
        "    os.makedirs(base_checkpoint_path)\n",
        "  filepath = base_checkpoint_path + \"/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=rlr_factor, patience=patience)\n",
        "\n",
        "  callbacks_list = [checkpoint]\n",
        "  if rlr:\n",
        "    callbacks_list = [reduce_lr, checkpoint]\n",
        "  \n",
        "  if not model:\n",
        "    if any_size:\n",
        "      model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES)\n",
        "    else:\n",
        "      model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), NUM_CLASSES)\n",
        "  if weights_path:\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    \n",
        "  op=SGD(momentum=0.9, nesterov=True)\n",
        "  if lr:\n",
        "    op=SGD(lr=lr, momentum=0.9, nesterov=True)\n",
        "    \n",
        "  lr_metric = get_lr_metric(op)\n",
        "  if use_weighted_loss and weighted_loss_weights is not None:\n",
        "    loss = weighted_categorical_crossentropy(weighted_loss_weights)\n",
        "  else:\n",
        "    loss = 'categorical_crossentropy'\n",
        "  \n",
        "  print(\"Loss used: \", loss)\n",
        "  model.compile(loss=loss,\n",
        "              optimizer=op,\n",
        "              metrics=['accuracy', lr_metric])\n",
        "  print(model.summary())\n",
        "  \n",
        "  if no_train:\n",
        "    return model\n",
        "  \n",
        "  if initial_epoch:\n",
        "    model.fit_generator(train_generator,\n",
        "                      steps_per_epoch=NUM_TRAIN_IMAGES // batch_size,\n",
        "                      validation_data=validation_generator,\n",
        "                      epochs=epochs, verbose=1,\n",
        "                      initial_epoch=initial_epoch,\n",
        "                      callbacks=callbacks_list,\n",
        "                      validation_steps=NUM_VAL_IMAGES // batch_size\n",
        "                     )\n",
        "  else:\n",
        "    model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // batch_size,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=epochs, verbose=1,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_steps=NUM_VAL_IMAGES // batch_size\n",
        "                   )\n",
        "  model.save_weights(base_checkpoint_path + \"/epochs:{}_end\".format(str(epochs)))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex23tw_1NbNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCLmYi68Ndar",
        "colab_type": "text"
      },
      "source": [
        "#### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpCdTOF7NhnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv2D_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params[\"kernel_regularizer\"]\n",
        "#     kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      use_bias=False,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "  \n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params[\"kernel_regularizer\"]\n",
        "#     kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      use_bias=False,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params[\"kernel_regularizer\"]\n",
        "#     kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      use_bias=False,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual, kernel_regularizer):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "\n",
        "#     print(input.shape, residual.shape)\n",
        "    input_channels = int(input.shape[-1])\n",
        "#     r = Conv2D(filters=input_channels, kernel_size=(1, 1),\n",
        "#                            strides=(1,1),\n",
        "#                            padding=\"same\",\n",
        "#                            kernel_initializer=\"he_normal\",\n",
        "#                            kernel_regularizer=l2(1e-4))(residual)\n",
        "    r = residual\n",
        "    c = concatenate([input, r])\n",
        "#     c = Conv2D(filters=input_channels, kernel_size=(1, 1),\n",
        "#                            strides=(1,1),\n",
        "#                            padding=\"same\",\n",
        "#                            use_bias=False,\n",
        "#                            kernel_initializer=\"he_normal\",\n",
        "#                            kernel_regularizer=kernel_regularizer)(c)\n",
        "    return c\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, kernel_regularizer, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        new_filters = filters\n",
        "        og_input = input\n",
        "#         print(' - - - -')\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "#             print('--> newf', new_filters)\n",
        "            input = block_function(filters=new_filters, init_strides=init_strides,kernel_regularizer=kernel_regularizer,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "#             new_filters *= 2\n",
        "\n",
        "        return _shortcut(input, og_input,kernel_regularizer)\n",
        "#         return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, kernel_regularizer, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           use_bias=False,\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=kernel_regularizer)(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides,kernel_regularizer=kernel_regularizer)(input)\n",
        "\n",
        "#         residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3),kernel_regularizer=kernel_regularizer)(conv1)\n",
        "#         return _shortcut(input, residual,kernel_regularizer)\n",
        "        return conv1\n",
        "#         return residual\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              use_bias=False,\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions,weight_decay=None):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "        if weight_decay is None:\n",
        "          weight_decay = l2(1.e-4)\n",
        "        kernel_regularizer = l2(weight_decay)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv2D_bn_relu(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_regularizer=kernel_regularizer)(input)\n",
        "        conv1 = _conv2D_bn_relu(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_regularizer=kernel_regularizer)(conv1)\n",
        "        conv1 = _conv2D_bn_relu(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_regularizer=kernel_regularizer)(conv1)\n",
        "\n",
        "        # Receptive -> 7x7\n",
        "        block = conv1\n",
        "        filters = 128\n",
        "        for i, r in enumerate(repetitions):\n",
        "#             print('going in, block', block.shape)\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, kernel_regularizer=kernel_regularizer, is_first_layer=(i == 0))(block)\n",
        "#             print('after residual, block', block.shape)\n",
        "#             block = _conv_bn_relu(filters=filters*2, kernel_size=(1, 1), padding=\"same\")(block)\n",
        "#             print('after 1x1, block', block.shape)\n",
        "            # if i != len(repetitions) - 1 and i != len(repetitions) - 2:\n",
        "            if i != len(repetitions) - 1:\n",
        "              block = MaxPooling2D(pool_size=(2, 2))(block)\n",
        "#             print('after max, block', block.shape)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block = Conv2D(filters=200, kernel_size=(1,1), padding='same',activation='softmax',kernel_regularizer=kernel_regularizer,use_bias=False)(block)\n",
        "        block = GlobalAveragePooling2D()(block)\n",
        "#         block = Activation('softmax')(block)\n",
        "\n",
        "        model = Model(inputs=input, outputs=block)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs,weight_decay):\n",
        "      return ResnetBuilder.build(input_shape, num_outputs, basic_block, [4, 4, 4],weight_decay=weight_decay)\n",
        "#       return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 3, 2],weight_decay=weight_decay)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI1wyCmhTxM4",
        "colab_type": "code",
        "outputId": "c3b75e63-6ac4-49be-be3b-21d856f6fe05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2264
        }
      },
      "source": [
        "model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, 64, 64), NUM_CLASSES,weight_decay=1e-4)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0616 04:49:07.121325 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0616 04:49:07.124509 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0616 04:49:07.137533 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0616 04:49:07.188423 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0616 04:49:07.189310 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0616 04:49:10.171871 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0616 04:49:10.646025 140505902831488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, 64, 128)  3456        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 64, 64, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 128)  147456      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64, 64, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 128)  147456      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 64, 64, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 128)  147456      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64, 64, 128)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 128)  147456      activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 64, 64, 128)  147456      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_7[0][0]                   \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 256)  0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 256)  1024        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 256)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 256)  589824      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 256)  1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 256)  589824      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 256)  1024        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 256)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 256)  589824      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 256)  1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 256)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 256)  589824      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 512)  0           conv2d_11[0][0]                  \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 512)  0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 512)  2359296     activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 512)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 512)  2359296     activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 512)  2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 512)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 512)  2359296     activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 512)  2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 512)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 512)  2359296     activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 16, 16, 1024) 0           conv2d_15[0][0]                  \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 1024) 4096        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 200)  204800      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 200)          0           conv2d_16[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 12,908,928\n",
            "Trainable params: 12,899,200\n",
            "Non-trainable params: 9,728\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAZRhKl3nl2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='/content/gdrive/My Drive/model.png') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_k3aFhj6xBF",
        "colab_type": "text"
      },
      "source": [
        "#### Find optimum LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SSju4St60ht",
        "colab_type": "code",
        "outputId": "ee593f1a-65e5-49fc-f55c-8a192a9dfaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13603
        }
      },
      "source": [
        "K.clear_session()\n",
        "model = None\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath_wts = base_checkpoint_path + \"/_clr_wts_epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "filepath = base_checkpoint_path + \"/_clr_epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "model_checkpoint_wts = ModelCheckpoint(filepath_wts, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=True, img_size_train=(64,64), img_size_val=(64,64))\n",
        "\n",
        "\n",
        "\n",
        "# lr_finder = LRFinder(NUM_TRAIN_IMAGES, BATCH_SIZE_TRAIN, minimum_lr=1e-, maximum_lr=10.,\n",
        "#                      lr_scale='exp',\n",
        "#                      # validation_data=(X_test, Y_test),  # use the validation data for losses\n",
        "#                      validation_sample_rate=5,\n",
        "#                      save_dir=base_checkpoint_path, verbose=True)\n",
        "\n",
        "validation_sample_rate = 500\n",
        "_, lrfind_val_generator = get_generators(validation_sample_rate, validation_sample_rate, no_aug=True, img_size_train=(64,64), img_size_val=(64,64))\n",
        "lr_finder = LRFinder(NUM_TRAIN_IMAGES, BATCH_SIZE_TRAIN, minimum_lr=1e-5, maximum_lr=3,\n",
        "                         validation_generator=lrfind_val_generator,\n",
        "                         validation_sample_rate=validation_sample_rate,\n",
        "                         lr_scale='linear', save_dir=base_checkpoint_path + '/lr/',\n",
        "                         verbose=True)\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# optimizer = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "optimizer = Adam()\n",
        "model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES,weight_decay=1e-4)\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=1, \n",
        "                    callbacks=[lr_finder]\n",
        "                   )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/1\n",
            "  1/781 [..............................] - ETA: 2:34:58 - loss: 6.6356 - acc: 0.0000e+00 - LRFinder: val_loss: 6.6812 - lr = 0.00001000 \n",
            "  2/781 [..............................] - ETA: 2:24:49 - loss: 6.6328 - acc: 0.0078     - LRFinder: val_loss: 6.6517 - lr = 0.00385122 \n",
            "  3/781 [..............................] - ETA: 1:41:56 - loss: 6.6456 - acc: 0.0052 - LRFinder: val_loss: 17.1535 - lr = 0.00769243 \n",
            "  4/781 [..............................] - ETA: 1:20:26 - loss: 6.6495 - acc: 0.0039 - LRFinder: val_loss: 17.1963 - lr = 0.01153365 \n",
            "  5/781 [..............................] - ETA: 1:07:30 - loss: 6.6621 - acc: 0.0031 - LRFinder: val_loss: 17.2439 - lr = 0.01537487 \n",
            "  6/781 [..............................] - ETA: 58:53 - loss: 6.6873 - acc: 0.0026   - LRFinder: val_loss: 17.6384 - lr = 0.01921608 \n",
            "  7/781 [..............................] - ETA: 52:42 - loss: 6.7411 - acc: 0.0022 - LRFinder: val_loss: 17.7937 - lr = 0.02305730 \n",
            "  8/781 [..............................] - ETA: 48:06 - loss: 6.8256 - acc: 0.0020 - LRFinder: val_loss: 18.4492 - lr = 0.02689851 \n",
            "  9/781 [..............................] - ETA: 44:29 - loss: 6.9362 - acc: 0.0026 - LRFinder: val_loss: 18.9961 - lr = 0.03073973 \n",
            " 10/781 [..............................] - ETA: 41:36 - loss: 7.0840 - acc: 0.0023 - LRFinder: val_loss: 19.8783 - lr = 0.03458095 \n",
            " 11/781 [..............................] - ETA: 39:14 - loss: 7.2732 - acc: 0.0028 - LRFinder: val_loss: 20.5794 - lr = 0.03842216 \n",
            " 12/781 [..............................] - ETA: 37:16 - loss: 7.4954 - acc: 0.0026 - LRFinder: val_loss: 21.4733 - lr = 0.04226338 \n",
            " 13/781 [..............................] - ETA: 35:35 - loss: 7.7487 - acc: 0.0030 - LRFinder: val_loss: 22.3654 - lr = 0.04610460 \n",
            " 14/781 [..............................] - ETA: 34:09 - loss: 8.0305 - acc: 0.0028 - LRFinder: val_loss: 23.3165 - lr = 0.04994581 \n",
            " 15/781 [..............................] - ETA: 32:55 - loss: 8.3342 - acc: 0.0031 - LRFinder: val_loss: 23.8438 - lr = 0.05378703 \n",
            " 16/781 [..............................] - ETA: 31:50 - loss: 8.6496 - acc: 0.0034 - LRFinder: val_loss: 24.9285 - lr = 0.05762825 \n",
            " 17/781 [..............................] - ETA: 30:52 - loss: 8.9742 - acc: 0.0032 - LRFinder: val_loss: 25.5865 - lr = 0.06146946 \n",
            " 18/781 [..............................] - ETA: 30:00 - loss: 9.2998 - acc: 0.0030 - LRFinder: val_loss: 26.0128 - lr = 0.06531068 \n",
            " 19/781 [..............................] - ETA: 29:13 - loss: 9.6193 - acc: 0.0033 - LRFinder: val_loss: 26.3821 - lr = 0.06915190 \n",
            " 20/781 [..............................] - ETA: 28:32 - loss: 9.9243 - acc: 0.0031 - LRFinder: val_loss: 26.6843 - lr = 0.07299311 \n",
            " 21/781 [..............................] - ETA: 27:54 - loss: 10.2100 - acc: 0.0030 - LRFinder: val_loss: 26.7246 - lr = 0.07683433 \n",
            " 22/781 [..............................] - ETA: 27:19 - loss: 10.4709 - acc: 0.0028 - LRFinder: val_loss: 26.5094 - lr = 0.08067554 \n",
            " 23/781 [..............................] - ETA: 26:48 - loss: 10.7047 - acc: 0.0027 - LRFinder: val_loss: 26.2466 - lr = 0.08451676 \n",
            " 24/781 [..............................] - ETA: 26:19 - loss: 10.9067 - acc: 0.0026 - LRFinder: val_loss: 25.9646 - lr = 0.08835798 \n",
            " 25/781 [..............................] - ETA: 25:53 - loss: 11.0777 - acc: 0.0028 - LRFinder: val_loss: 25.2701 - lr = 0.09219919 \n",
            " 26/781 [..............................] - ETA: 25:28 - loss: 11.2172 - acc: 0.0027 - LRFinder: val_loss: 24.6732 - lr = 0.09604041 \n",
            " 27/781 [>.............................] - ETA: 25:05 - loss: 11.3248 - acc: 0.0026 - LRFinder: val_loss: 24.1045 - lr = 0.09988163 \n",
            " 28/781 [>.............................] - ETA: 24:44 - loss: 11.4034 - acc: 0.0025 - LRFinder: val_loss: 23.3740 - lr = 0.10372284 \n",
            " 29/781 [>.............................] - ETA: 24:24 - loss: 11.4548 - acc: 0.0030 - LRFinder: val_loss: 23.0599 - lr = 0.10756406 \n",
            " 30/781 [>.............................] - ETA: 24:05 - loss: 11.4824 - acc: 0.0029 - LRFinder: val_loss: 22.3968 - lr = 0.11140528 \n",
            " 31/781 [>.............................] - ETA: 23:48 - loss: 11.4886 - acc: 0.0028 - LRFinder: val_loss: 21.8256 - lr = 0.11524649 \n",
            " 32/781 [>.............................] - ETA: 23:31 - loss: 11.4767 - acc: 0.0029 - LRFinder: val_loss: 21.1900 - lr = 0.11908771 \n",
            " 33/781 [>.............................] - ETA: 23:16 - loss: 11.4504 - acc: 0.0028 - LRFinder: val_loss: 20.7892 - lr = 0.12292892 \n",
            " 34/781 [>.............................] - ETA: 23:01 - loss: 11.4110 - acc: 0.0030 - LRFinder: val_loss: 20.2494 - lr = 0.12677014 \n",
            " 35/781 [>.............................] - ETA: 22:47 - loss: 11.3631 - acc: 0.0029 - LRFinder: val_loss: 19.4099 - lr = 0.13061136 \n",
            " 36/781 [>.............................] - ETA: 22:34 - loss: 11.3068 - acc: 0.0033 - LRFinder: val_loss: 18.1996 - lr = 0.13445257 \n",
            " 37/781 [>.............................] - ETA: 22:21 - loss: 11.2461 - acc: 0.0032 - LRFinder: val_loss: 16.9762 - lr = 0.13829379 \n",
            " 38/781 [>.............................] - ETA: 22:09 - loss: 11.1812 - acc: 0.0031 - LRFinder: val_loss: 14.3379 - lr = 0.14213501 \n",
            " 39/781 [>.............................] - ETA: 21:58 - loss: 11.1136 - acc: 0.0032 - LRFinder: val_loss: 16.3066 - lr = 0.14597622 \n",
            " 40/781 [>.............................] - ETA: 21:47 - loss: 11.0438 - acc: 0.0031 - LRFinder: val_loss: 15.5517 - lr = 0.14981744 \n",
            " 41/781 [>.............................] - ETA: 21:37 - loss: 10.9731 - acc: 0.0032 - LRFinder: val_loss: 12.9393 - lr = 0.15365866 \n",
            " 42/781 [>.............................] - ETA: 21:29 - loss: 10.9009 - acc: 0.0035 - LRFinder: val_loss: 15.1423 - lr = 0.15749987 \n",
            " 43/781 [>.............................] - ETA: 21:21 - loss: 10.8296 - acc: 0.0035 - LRFinder: val_loss: 16.6440 - lr = 0.16134109 \n",
            " 44/781 [>.............................] - ETA: 21:13 - loss: 10.7582 - acc: 0.0034 - LRFinder: val_loss: 14.0120 - lr = 0.16518230 \n",
            " 45/781 [>.............................] - ETA: 21:05 - loss: 10.6863 - acc: 0.0033 - LRFinder: val_loss: 9.4636 - lr = 0.16902352 \n",
            " 46/781 [>.............................] - ETA: 20:58 - loss: 10.6140 - acc: 0.0032 - LRFinder: val_loss: 9.2615 - lr = 0.17286474 \n",
            " 47/781 [>.............................] - ETA: 20:51 - loss: 10.5417 - acc: 0.0033 - LRFinder: val_loss: 9.6927 - lr = 0.17670595 \n",
            " 48/781 [>.............................] - ETA: 20:44 - loss: 10.4706 - acc: 0.0033 - LRFinder: val_loss: 8.8737 - lr = 0.18054717 \n",
            " 49/781 [>.............................] - ETA: 20:37 - loss: 10.3995 - acc: 0.0032 - LRFinder: val_loss: 7.6670 - lr = 0.18438839 \n",
            " 50/781 [>.............................] - ETA: 20:30 - loss: 10.3290 - acc: 0.0031 - LRFinder: val_loss: 7.0199 - lr = 0.18822960 \n",
            " 51/781 [>.............................] - ETA: 20:23 - loss: 10.2590 - acc: 0.0031 - LRFinder: val_loss: 6.7250 - lr = 0.19207082 \n",
            " 52/781 [>.............................] - ETA: 20:16 - loss: 10.1896 - acc: 0.0030 - LRFinder: val_loss: 6.6533 - lr = 0.19591204 \n",
            " 53/781 [=>............................] - ETA: 20:10 - loss: 10.1209 - acc: 0.0031 - LRFinder: val_loss: 6.5516 - lr = 0.19975325 \n",
            " 54/781 [=>............................] - ETA: 20:04 - loss: 10.0530 - acc: 0.0030 - LRFinder: val_loss: 6.4434 - lr = 0.20359447 \n",
            " 55/781 [=>............................] - ETA: 19:58 - loss: 9.9858 - acc: 0.0030  - LRFinder: val_loss: 6.4073 - lr = 0.20743569 \n",
            " 56/781 [=>............................] - ETA: 19:52 - loss: 9.9198 - acc: 0.0029 - LRFinder: val_loss: 6.3198 - lr = 0.21127690 \n",
            " 57/781 [=>............................] - ETA: 19:47 - loss: 9.8555 - acc: 0.0030 - LRFinder: val_loss: 6.3693 - lr = 0.21511812 \n",
            " 58/781 [=>............................] - ETA: 19:41 - loss: 9.7928 - acc: 0.0030 - LRFinder: val_loss: 6.2876 - lr = 0.21895933 \n",
            " 59/781 [=>............................] - ETA: 19:36 - loss: 9.7301 - acc: 0.0030 - LRFinder: val_loss: 6.2498 - lr = 0.22280055 \n",
            " 60/781 [=>............................] - ETA: 19:31 - loss: 9.6690 - acc: 0.0030 - LRFinder: val_loss: 6.6053 - lr = 0.22664177 \n",
            " 61/781 [=>............................] - ETA: 19:26 - loss: 9.6085 - acc: 0.0029 - LRFinder: val_loss: 7.1000 - lr = 0.23048298 \n",
            " 62/781 [=>............................] - ETA: 19:21 - loss: 9.5497 - acc: 0.0030 - LRFinder: val_loss: 7.3742 - lr = 0.23432420 \n",
            " 63/781 [=>............................] - ETA: 19:17 - loss: 9.4922 - acc: 0.0032 - LRFinder: val_loss: 7.6716 - lr = 0.23816542 \n",
            " 64/781 [=>............................] - ETA: 19:12 - loss: 9.4355 - acc: 0.0032 - LRFinder: val_loss: 7.7107 - lr = 0.24200663 \n",
            " 65/781 [=>............................] - ETA: 19:07 - loss: 9.3794 - acc: 0.0035 - LRFinder: val_loss: 7.1239 - lr = 0.24584785 \n",
            " 66/781 [=>............................] - ETA: 19:03 - loss: 9.3266 - acc: 0.0034 - LRFinder: val_loss: 7.8007 - lr = 0.24968907 \n",
            " 67/781 [=>............................] - ETA: 18:59 - loss: 9.2747 - acc: 0.0034 - LRFinder: val_loss: 6.9879 - lr = 0.25353028 \n",
            " 68/781 [=>............................] - ETA: 18:54 - loss: 9.2237 - acc: 0.0033 - LRFinder: val_loss: 6.7394 - lr = 0.25737150 \n",
            " 69/781 [=>............................] - ETA: 18:50 - loss: 9.1751 - acc: 0.0034 - LRFinder: val_loss: 5.8113 - lr = 0.26121271 \n",
            " 70/781 [=>............................] - ETA: 18:46 - loss: 9.1261 - acc: 0.0033 - LRFinder: val_loss: 5.7426 - lr = 0.26505393 \n",
            " 71/781 [=>............................] - ETA: 18:41 - loss: 9.0791 - acc: 0.0034 - LRFinder: val_loss: 5.7406 - lr = 0.26889515 \n",
            " 72/781 [=>............................] - ETA: 18:37 - loss: 9.0329 - acc: 0.0034 - LRFinder: val_loss: 5.7431 - lr = 0.27273636 \n",
            " 73/781 [=>............................] - ETA: 18:33 - loss: 8.9877 - acc: 0.0034 - LRFinder: val_loss: 5.8065 - lr = 0.27657758 \n",
            " 74/781 [=>............................] - ETA: 18:29 - loss: 8.9436 - acc: 0.0034 - LRFinder: val_loss: 5.7287 - lr = 0.28041880 \n",
            " 75/781 [=>............................] - ETA: 18:25 - loss: 8.9008 - acc: 0.0033 - LRFinder: val_loss: 6.9399 - lr = 0.28426001 \n",
            " 76/781 [=>............................] - ETA: 18:21 - loss: 8.8590 - acc: 0.0034 - LRFinder: val_loss: 7.6142 - lr = 0.28810123 \n",
            " 77/781 [=>............................] - ETA: 18:17 - loss: 8.8177 - acc: 0.0034 - LRFinder: val_loss: 8.0913 - lr = 0.29194245 \n",
            " 78/781 [=>............................] - ETA: 18:13 - loss: 8.7785 - acc: 0.0034 - LRFinder: val_loss: 9.9641 - lr = 0.29578366 \n",
            " 79/781 [==>...........................] - ETA: 18:09 - loss: 8.7395 - acc: 0.0034 - LRFinder: val_loss: 11.1109 - lr = 0.29962488 \n",
            " 80/781 [==>...........................] - ETA: 18:06 - loss: 8.7023 - acc: 0.0033 - LRFinder: val_loss: 10.7514 - lr = 0.30346609 \n",
            " 81/781 [==>...........................] - ETA: 18:02 - loss: 8.6654 - acc: 0.0033 - LRFinder: val_loss: 8.8208 - lr = 0.30730731 \n",
            " 82/781 [==>...........................] - ETA: 17:58 - loss: 8.6289 - acc: 0.0034 - LRFinder: val_loss: 7.7934 - lr = 0.31114853 \n",
            " 83/781 [==>...........................] - ETA: 17:55 - loss: 8.5932 - acc: 0.0034 - LRFinder: val_loss: 6.8315 - lr = 0.31498974 \n",
            " 84/781 [==>...........................] - ETA: 17:51 - loss: 8.5583 - acc: 0.0034 - LRFinder: val_loss: 7.4638 - lr = 0.31883096 \n",
            " 85/781 [==>...........................] - ETA: 17:48 - loss: 8.5238 - acc: 0.0036 - LRFinder: val_loss: 7.0632 - lr = 0.32267218 \n",
            " 86/781 [==>...........................] - ETA: 17:44 - loss: 8.4898 - acc: 0.0036 - LRFinder: val_loss: 8.1691 - lr = 0.32651339 \n",
            " 87/781 [==>...........................] - ETA: 17:41 - loss: 8.4561 - acc: 0.0038 - LRFinder: val_loss: 8.0189 - lr = 0.33035461 \n",
            " 88/781 [==>...........................] - ETA: 17:38 - loss: 8.4240 - acc: 0.0038 - LRFinder: val_loss: 9.1982 - lr = 0.33419583 \n",
            " 89/781 [==>...........................] - ETA: 17:34 - loss: 8.3944 - acc: 0.0038 - LRFinder: val_loss: 8.8843 - lr = 0.33803704 \n",
            " 90/781 [==>...........................] - ETA: 17:31 - loss: 8.3642 - acc: 0.0037 - LRFinder: val_loss: 13.1097 - lr = 0.34187826 \n",
            " 91/781 [==>...........................] - ETA: 17:28 - loss: 8.3346 - acc: 0.0038 - LRFinder: val_loss: 16.2109 - lr = 0.34571948 \n",
            " 92/781 [==>...........................] - ETA: 17:25 - loss: 8.3054 - acc: 0.0037 - LRFinder: val_loss: 16.1884 - lr = 0.34956069 \n",
            " 93/781 [==>...........................] - ETA: 17:22 - loss: 8.2761 - acc: 0.0037 - LRFinder: val_loss: 8.1196 - lr = 0.35340191 \n",
            " 94/781 [==>...........................] - ETA: 17:19 - loss: 8.2476 - acc: 0.0037 - LRFinder: val_loss: 8.3706 - lr = 0.35724312 \n",
            " 95/781 [==>...........................] - ETA: 17:16 - loss: 8.2197 - acc: 0.0039 - LRFinder: val_loss: 7.8845 - lr = 0.36108434 \n",
            " 96/781 [==>...........................] - ETA: 17:12 - loss: 8.1929 - acc: 0.0039 - LRFinder: val_loss: 8.0590 - lr = 0.36492556 \n",
            " 97/781 [==>...........................] - ETA: 17:09 - loss: 8.1662 - acc: 0.0039 - LRFinder: val_loss: 7.9884 - lr = 0.36876677 \n",
            " 98/781 [==>...........................] - ETA: 17:06 - loss: 8.1404 - acc: 0.0038 - LRFinder: val_loss: 7.3559 - lr = 0.37260799 \n",
            " 99/781 [==>...........................] - ETA: 17:04 - loss: 8.1151 - acc: 0.0038 - LRFinder: val_loss: 6.5713 - lr = 0.37644921 \n",
            "100/781 [==>...........................] - ETA: 17:01 - loss: 8.0903 - acc: 0.0038 - LRFinder: val_loss: 5.8716 - lr = 0.38029042 \n",
            "101/781 [==>...........................] - ETA: 16:58 - loss: 8.0660 - acc: 0.0038 - LRFinder: val_loss: 5.7462 - lr = 0.38413164 \n",
            "102/781 [==>...........................] - ETA: 16:55 - loss: 8.0419 - acc: 0.0038 - LRFinder: val_loss: 6.3564 - lr = 0.38797286 \n",
            "103/781 [==>...........................] - ETA: 16:53 - loss: 8.0191 - acc: 0.0038 - LRFinder: val_loss: 6.9215 - lr = 0.39181407 \n",
            "104/781 [==>...........................] - ETA: 16:50 - loss: 7.9980 - acc: 0.0038 - LRFinder: val_loss: 11.4361 - lr = 0.39565529 \n",
            "105/781 [===>..........................] - ETA: 16:48 - loss: 7.9793 - acc: 0.0037 - LRFinder: val_loss: 8.9324 - lr = 0.39949650 \n",
            "106/781 [===>..........................] - ETA: 16:46 - loss: 7.9634 - acc: 0.0037 - LRFinder: val_loss: 7.9647 - lr = 0.40333772 \n",
            "107/781 [===>..........................] - ETA: 16:43 - loss: 7.9501 - acc: 0.0037 - LRFinder: val_loss: 7.6088 - lr = 0.40717894 \n",
            "108/781 [===>..........................] - ETA: 16:41 - loss: 7.9395 - acc: 0.0036 - LRFinder: val_loss: 7.5138 - lr = 0.41102015 \n",
            "109/781 [===>..........................] - ETA: 16:39 - loss: 7.9312 - acc: 0.0036 - LRFinder: val_loss: 7.5559 - lr = 0.41486137 \n",
            "110/781 [===>..........................] - ETA: 16:36 - loss: 7.9249 - acc: 0.0036 - LRFinder: val_loss: 7.6762 - lr = 0.41870259 \n",
            "111/781 [===>..........................] - ETA: 16:34 - loss: 7.9202 - acc: 0.0036 - LRFinder: val_loss: 7.7414 - lr = 0.42254380 \n",
            "112/781 [===>..........................] - ETA: 16:31 - loss: 7.9167 - acc: 0.0036 - LRFinder: val_loss: 7.6957 - lr = 0.42638502 \n",
            "113/781 [===>..........................] - ETA: 16:28 - loss: 7.9140 - acc: 0.0035 - LRFinder: val_loss: 7.6571 - lr = 0.43022624 \n",
            "114/781 [===>..........................] - ETA: 16:26 - loss: 7.9117 - acc: 0.0036 - LRFinder: val_loss: 7.6619 - lr = 0.43406745 \n",
            "115/781 [===>..........................] - ETA: 16:23 - loss: 7.9095 - acc: 0.0036 - LRFinder: val_loss: 7.6334 - lr = 0.43790867 \n",
            "116/781 [===>..........................] - ETA: 16:21 - loss: 7.9072 - acc: 0.0036 - LRFinder: val_loss: 7.5769 - lr = 0.44174988 \n",
            "117/781 [===>..........................] - ETA: 16:18 - loss: 7.9044 - acc: 0.0037 - LRFinder: val_loss: 7.4988 - lr = 0.44559110 \n",
            "118/781 [===>..........................] - ETA: 16:16 - loss: 7.9009 - acc: 0.0036 - LRFinder: val_loss: 7.4039 - lr = 0.44943232 \n",
            "119/781 [===>..........................] - ETA: 16:13 - loss: 7.8967 - acc: 0.0037 - LRFinder: val_loss: 7.2968 - lr = 0.45327353 \n",
            "120/781 [===>..........................] - ETA: 16:11 - loss: 7.8917 - acc: 0.0036 - LRFinder: val_loss: 7.1814 - lr = 0.45711475 \n",
            "121/781 [===>..........................] - ETA: 16:08 - loss: 7.8859 - acc: 0.0037 - LRFinder: val_loss: 7.0616 - lr = 0.46095597 \n",
            "122/781 [===>..........................] - ETA: 16:06 - loss: 7.8791 - acc: 0.0037 - LRFinder: val_loss: 6.9403 - lr = 0.46479718 \n",
            "123/781 [===>..........................] - ETA: 16:04 - loss: 7.8715 - acc: 0.0037 - LRFinder: val_loss: 6.8203 - lr = 0.46863840 \n",
            "124/781 [===>..........................] - ETA: 16:01 - loss: 7.8630 - acc: 0.0038 - LRFinder: val_loss: 6.7036 - lr = 0.47247962 \n",
            "125/781 [===>..........................] - ETA: 15:59 - loss: 7.8537 - acc: 0.0037 - LRFinder: val_loss: 6.5917 - lr = 0.47632083 \n",
            "126/781 [===>..........................] - ETA: 15:56 - loss: 7.8437 - acc: 0.0037 - LRFinder: val_loss: 6.4858 - lr = 0.48016205 \n",
            "127/781 [===>..........................] - ETA: 15:54 - loss: 7.8330 - acc: 0.0037 - LRFinder: val_loss: 6.3865 - lr = 0.48400327 \n",
            "128/781 [===>..........................] - ETA: 15:52 - loss: 7.8218 - acc: 0.0037 - LRFinder: val_loss: 6.2967 - lr = 0.48784448 \n",
            "129/781 [===>..........................] - ETA: 15:49 - loss: 7.8099 - acc: 0.0037 - LRFinder: val_loss: 6.2178 - lr = 0.49168570 \n",
            "130/781 [===>..........................] - ETA: 15:48 - loss: 7.7976 - acc: 0.0038 - LRFinder: val_loss: 6.1507 - lr = 0.49552691 \n",
            "131/781 [====>.........................] - ETA: 15:46 - loss: 7.7850 - acc: 0.0039 - LRFinder: val_loss: 6.0964 - lr = 0.49936813 \n",
            "132/781 [====>.........................] - ETA: 15:44 - loss: 7.7725 - acc: 0.0038 - LRFinder: val_loss: 6.0590 - lr = 0.50320935 \n",
            "133/781 [====>.........................] - ETA: 15:42 - loss: 7.7596 - acc: 0.0039 - LRFinder: val_loss: 6.0582 - lr = 0.50705056 \n",
            "134/781 [====>.........................] - ETA: 15:40 - loss: 7.7468 - acc: 0.0039 - LRFinder: val_loss: 14.5508 - lr = 0.51089178 \n",
            "135/781 [====>.........................] - ETA: 15:39 - loss: 7.7340 - acc: 0.0039 - LRFinder: val_loss: 16.7359 - lr = 0.51473300 \n",
            "136/781 [====>.........................] - ETA: 15:37 - loss: 7.7213 - acc: 0.0038 - LRFinder: val_loss: 16.7502 - lr = 0.51857421 \n",
            "137/781 [====>.........................] - ETA: 15:35 - loss: 7.7087 - acc: 0.0038 - LRFinder: val_loss: 16.7620 - lr = 0.52241543 \n",
            "138/781 [====>.........................] - ETA: 15:33 - loss: 7.6962 - acc: 0.0038 - LRFinder: val_loss: 16.5902 - lr = 0.52625665 \n",
            "139/781 [====>.........................] - ETA: 15:30 - loss: 7.6836 - acc: 0.0039 - LRFinder: val_loss: 16.5911 - lr = 0.53009786 \n",
            "140/781 [====>.........................] - ETA: 15:28 - loss: 7.6710 - acc: 0.0039 - LRFinder: val_loss: 7.9830 - lr = 0.53393908 \n",
            "141/781 [====>.........................] - ETA: 15:26 - loss: 7.6583 - acc: 0.0039 - LRFinder: val_loss: 5.8949 - lr = 0.53778029 \n",
            "142/781 [====>.........................] - ETA: 15:24 - loss: 7.6456 - acc: 0.0039 - LRFinder: val_loss: 5.8106 - lr = 0.54162151 \n",
            "143/781 [====>.........................] - ETA: 15:22 - loss: 7.6327 - acc: 0.0038 - LRFinder: val_loss: 5.7730 - lr = 0.54546273 \n",
            "144/781 [====>.........................] - ETA: 15:20 - loss: 7.6198 - acc: 0.0039 - LRFinder: val_loss: 5.7360 - lr = 0.54930394 \n",
            "145/781 [====>.........................] - ETA: 15:17 - loss: 7.6068 - acc: 0.0038 - LRFinder: val_loss: 5.7003 - lr = 0.55314516 \n",
            "146/781 [====>.........................] - ETA: 15:15 - loss: 7.5938 - acc: 0.0038 - LRFinder: val_loss: 5.6662 - lr = 0.55698638 \n",
            "147/781 [====>.........................] - ETA: 15:13 - loss: 7.5807 - acc: 0.0038 - LRFinder: val_loss: 5.6340 - lr = 0.56082759 \n",
            "148/781 [====>.........................] - ETA: 15:11 - loss: 7.5675 - acc: 0.0039 - LRFinder: val_loss: 5.6039 - lr = 0.56466881 \n",
            "149/781 [====>.........................] - ETA: 15:09 - loss: 7.5543 - acc: 0.0038 - LRFinder: val_loss: 5.5758 - lr = 0.56851003 \n",
            "150/781 [====>.........................] - ETA: 15:07 - loss: 7.5411 - acc: 0.0039 - LRFinder: val_loss: 5.5499 - lr = 0.57235124 \n",
            "151/781 [====>.........................] - ETA: 15:05 - loss: 7.5280 - acc: 0.0039 - LRFinder: val_loss: 5.5263 - lr = 0.57619246 \n",
            "152/781 [====>.........................] - ETA: 15:03 - loss: 7.5148 - acc: 0.0039 - LRFinder: val_loss: 5.5047 - lr = 0.58003367 \n",
            "153/781 [====>.........................] - ETA: 15:01 - loss: 7.5016 - acc: 0.0039 - LRFinder: val_loss: 5.4851 - lr = 0.58387489 \n",
            "154/781 [====>.........................] - ETA: 14:59 - loss: 7.4886 - acc: 0.0039 - LRFinder: val_loss: 5.4674 - lr = 0.58771611 \n",
            "155/781 [====>.........................] - ETA: 14:57 - loss: 7.4755 - acc: 0.0039 - LRFinder: val_loss: 5.4514 - lr = 0.59155732 \n",
            "156/781 [====>.........................] - ETA: 14:55 - loss: 7.4625 - acc: 0.0039 - LRFinder: val_loss: 5.4370 - lr = 0.59539854 \n",
            "157/781 [=====>........................] - ETA: 14:53 - loss: 7.4496 - acc: 0.0038 - LRFinder: val_loss: 5.4240 - lr = 0.59923976 \n",
            "158/781 [=====>........................] - ETA: 14:51 - loss: 7.4368 - acc: 0.0039 - LRFinder: val_loss: 5.4124 - lr = 0.60308097 \n",
            "159/781 [=====>........................] - ETA: 14:49 - loss: 7.4241 - acc: 0.0039 - LRFinder: val_loss: 5.4019 - lr = 0.60692219 \n",
            "160/781 [=====>........................] - ETA: 14:47 - loss: 7.4114 - acc: 0.0040 - LRFinder: val_loss: 5.3927 - lr = 0.61076341 \n",
            "161/781 [=====>........................] - ETA: 14:45 - loss: 7.3989 - acc: 0.0040 - LRFinder: val_loss: 5.3843 - lr = 0.61460462 \n",
            "162/781 [=====>........................] - ETA: 14:43 - loss: 7.3865 - acc: 0.0040 - LRFinder: val_loss: 5.3770 - lr = 0.61844584 \n",
            "163/781 [=====>........................] - ETA: 14:41 - loss: 7.3741 - acc: 0.0040 - LRFinder: val_loss: 5.3703 - lr = 0.62228706 \n",
            "164/781 [=====>........................] - ETA: 14:39 - loss: 7.3619 - acc: 0.0040 - LRFinder: val_loss: 5.3644 - lr = 0.62612827 \n",
            "165/781 [=====>........................] - ETA: 14:37 - loss: 7.3498 - acc: 0.0039 - LRFinder: val_loss: 5.3590 - lr = 0.62996949 \n",
            "166/781 [=====>........................] - ETA: 14:36 - loss: 7.3378 - acc: 0.0040 - LRFinder: val_loss: 5.3541 - lr = 0.63381070 \n",
            "167/781 [=====>........................] - ETA: 14:34 - loss: 7.3259 - acc: 0.0040 - LRFinder: val_loss: 5.3497 - lr = 0.63765192 \n",
            "168/781 [=====>........................] - ETA: 14:32 - loss: 7.3142 - acc: 0.0040 - LRFinder: val_loss: 5.3457 - lr = 0.64149314 \n",
            "169/781 [=====>........................] - ETA: 14:30 - loss: 7.3025 - acc: 0.0039 - LRFinder: val_loss: 5.3419 - lr = 0.64533435 \n",
            "170/781 [=====>........................] - ETA: 14:29 - loss: 7.2910 - acc: 0.0039 - LRFinder: val_loss: 5.3385 - lr = 0.64917557 \n",
            "171/781 [=====>........................] - ETA: 14:27 - loss: 7.2796 - acc: 0.0039 - LRFinder: val_loss: 5.3354 - lr = 0.65301679 \n",
            "172/781 [=====>........................] - ETA: 14:25 - loss: 7.2683 - acc: 0.0039 - LRFinder: val_loss: 5.3326 - lr = 0.65685800 \n",
            "173/781 [=====>........................] - ETA: 14:23 - loss: 7.2571 - acc: 0.0038 - LRFinder: val_loss: 5.3306 - lr = 0.66069922 \n",
            "174/781 [=====>........................] - ETA: 14:21 - loss: 7.2460 - acc: 0.0039 - LRFinder: val_loss: 5.3297 - lr = 0.66454044 \n",
            "175/781 [=====>........................] - ETA: 14:19 - loss: 7.2351 - acc: 0.0038 - LRFinder: val_loss: 5.3292 - lr = 0.66838165 \n",
            "176/781 [=====>........................] - ETA: 14:17 - loss: 7.2242 - acc: 0.0038 - LRFinder: val_loss: 5.3287 - lr = 0.67222287 \n",
            "177/781 [=====>........................] - ETA: 14:15 - loss: 7.2135 - acc: 0.0038 - LRFinder: val_loss: 5.3279 - lr = 0.67606408 \n",
            "178/781 [=====>........................] - ETA: 14:13 - loss: 7.2029 - acc: 0.0038 - LRFinder: val_loss: 5.3270 - lr = 0.67990530 \n",
            "179/781 [=====>........................] - ETA: 14:11 - loss: 7.1925 - acc: 0.0038 - LRFinder: val_loss: 5.3258 - lr = 0.68374652 \n",
            "180/781 [=====>........................] - ETA: 14:09 - loss: 7.1821 - acc: 0.0037 - LRFinder: val_loss: 5.3243 - lr = 0.68758773 \n",
            "181/781 [=====>........................] - ETA: 14:08 - loss: 7.1718 - acc: 0.0037 - LRFinder: val_loss: 5.3229 - lr = 0.69142895 \n",
            "182/781 [=====>........................] - ETA: 14:06 - loss: 7.1617 - acc: 0.0037 - LRFinder: val_loss: 5.3212 - lr = 0.69527017 \n",
            "183/781 [======>.......................] - ETA: 14:04 - loss: 7.1516 - acc: 0.0037 - LRFinder: val_loss: 5.3195 - lr = 0.69911138 \n",
            "184/781 [======>.......................] - ETA: 14:02 - loss: 7.1417 - acc: 0.0037 - LRFinder: val_loss: 6.0668 - lr = 0.70295260 \n",
            "185/781 [======>.......................] - ETA: 14:00 - loss: 7.1359 - acc: 0.0036 - LRFinder: val_loss: 7.7667 - lr = 0.70679382 \n",
            "186/781 [======>.......................] - ETA: 13:58 - loss: 7.1392 - acc: 0.0036 - LRFinder: val_loss: 9.5638 - lr = 0.71063503 \n",
            "187/781 [======>.......................] - ETA: 13:56 - loss: 7.1522 - acc: 0.0037 - LRFinder: val_loss: 10.8450 - lr = 0.71447625 \n",
            "188/781 [======>.......................] - ETA: 13:54 - loss: 7.1719 - acc: 0.0037 - LRFinder: val_loss: 11.3695 - lr = 0.71831746 \n",
            "189/781 [======>.......................] - ETA: 13:53 - loss: 7.1941 - acc: 0.0038 - LRFinder: val_loss: 11.1988 - lr = 0.72215868 \n",
            "190/781 [======>.......................] - ETA: 13:51 - loss: 7.2151 - acc: 0.0039 - LRFinder: val_loss: 10.5586 - lr = 0.72599990 \n",
            "191/781 [======>.......................] - ETA: 13:49 - loss: 7.2326 - acc: 0.0038 - LRFinder: val_loss: 9.7104 - lr = 0.72984111 \n",
            "192/781 [======>.......................] - ETA: 13:47 - loss: 7.2455 - acc: 0.0038 - LRFinder: val_loss: 8.8683 - lr = 0.73368233 \n",
            "193/781 [======>.......................] - ETA: 13:46 - loss: 7.2540 - acc: 0.0039 - LRFinder: val_loss: 8.1633 - lr = 0.73752355 \n",
            "194/781 [======>.......................] - ETA: 13:44 - loss: 7.2586 - acc: 0.0039 - LRFinder: val_loss: 7.6446 - lr = 0.74136476 \n",
            "195/781 [======>.......................] - ETA: 13:42 - loss: 7.2606 - acc: 0.0039 - LRFinder: val_loss: 7.3009 - lr = 0.74520598 \n",
            "196/781 [======>.......................] - ETA: 13:40 - loss: 7.2608 - acc: 0.0039 - LRFinder: val_loss: 7.0883 - lr = 0.74904720 \n",
            "197/781 [======>.......................] - ETA: 13:39 - loss: 7.2600 - acc: 0.0039 - LRFinder: val_loss: 6.9544 - lr = 0.75288841 \n",
            "198/781 [======>.......................] - ETA: 13:37 - loss: 7.2584 - acc: 0.0039 - LRFinder: val_loss: 6.8544 - lr = 0.75672963 \n",
            "199/781 [======>.......................] - ETA: 13:35 - loss: 7.2564 - acc: 0.0039 - LRFinder: val_loss: 6.7590 - lr = 0.76057085 \n",
            "200/781 [======>.......................] - ETA: 13:33 - loss: 7.2539 - acc: 0.0039 - LRFinder: val_loss: 6.6559 - lr = 0.76441206 \n",
            "201/781 [======>.......................] - ETA: 13:32 - loss: 7.2509 - acc: 0.0039 - LRFinder: val_loss: 6.5433 - lr = 0.76825328 \n",
            "202/781 [======>.......................] - ETA: 13:30 - loss: 7.2474 - acc: 0.0039 - LRFinder: val_loss: 6.4234 - lr = 0.77209449 \n",
            "203/781 [======>.......................] - ETA: 13:28 - loss: 7.2434 - acc: 0.0040 - LRFinder: val_loss: 6.3023 - lr = 0.77593571 \n",
            "204/781 [======>.......................] - ETA: 13:27 - loss: 7.2388 - acc: 0.0040 - LRFinder: val_loss: 6.1877 - lr = 0.77977693 \n",
            "205/781 [======>.......................] - ETA: 13:25 - loss: 7.2336 - acc: 0.0040 - LRFinder: val_loss: 6.0846 - lr = 0.78361814 \n",
            "206/781 [======>.......................] - ETA: 13:23 - loss: 7.2280 - acc: 0.0041 - LRFinder: val_loss: 5.9959 - lr = 0.78745936 \n",
            "207/781 [======>.......................] - ETA: 13:21 - loss: 7.2221 - acc: 0.0040 - LRFinder: val_loss: 5.9218 - lr = 0.79130058 \n",
            "208/781 [======>.......................] - ETA: 13:20 - loss: 7.2158 - acc: 0.0042 - LRFinder: val_loss: 5.8603 - lr = 0.79514179 \n",
            "209/781 [=======>......................] - ETA: 13:18 - loss: 7.2094 - acc: 0.0041 - LRFinder: val_loss: 5.8090 - lr = 0.79898301 \n",
            "210/781 [=======>......................] - ETA: 13:16 - loss: 7.2027 - acc: 0.0042 - LRFinder: val_loss: 5.7650 - lr = 0.80282423 \n",
            "211/781 [=======>......................] - ETA: 13:15 - loss: 7.1959 - acc: 0.0042 - LRFinder: val_loss: 5.7261 - lr = 0.80666544 \n",
            "212/781 [=======>......................] - ETA: 13:13 - loss: 7.1889 - acc: 0.0043 - LRFinder: val_loss: 5.6906 - lr = 0.81050666 \n",
            "213/781 [=======>......................] - ETA: 13:11 - loss: 7.1819 - acc: 0.0043 - LRFinder: val_loss: 5.6579 - lr = 0.81434787 \n",
            "214/781 [=======>......................] - ETA: 13:10 - loss: 7.1748 - acc: 0.0043 - LRFinder: val_loss: 5.6273 - lr = 0.81818909 \n",
            "215/781 [=======>......................] - ETA: 13:08 - loss: 7.1676 - acc: 0.0043 - LRFinder: val_loss: 5.5985 - lr = 0.82203031 \n",
            "216/781 [=======>......................] - ETA: 13:06 - loss: 7.1603 - acc: 0.0043 - LRFinder: val_loss: 5.5719 - lr = 0.82587152 \n",
            "217/781 [=======>......................] - ETA: 13:04 - loss: 7.1530 - acc: 0.0044 - LRFinder: val_loss: 5.5473 - lr = 0.82971274 \n",
            "218/781 [=======>......................] - ETA: 13:03 - loss: 7.1456 - acc: 0.0043 - LRFinder: val_loss: 5.5244 - lr = 0.83355396 \n",
            "219/781 [=======>......................] - ETA: 13:01 - loss: 7.1382 - acc: 0.0044 - LRFinder: val_loss: 5.5036 - lr = 0.83739517 \n",
            "220/781 [=======>......................] - ETA: 12:59 - loss: 7.1308 - acc: 0.0044 - LRFinder: val_loss: 5.4848 - lr = 0.84123639 \n",
            "221/781 [=======>......................] - ETA: 12:58 - loss: 7.1234 - acc: 0.0043 - LRFinder: val_loss: 5.4678 - lr = 0.84507761 \n",
            "222/781 [=======>......................] - ETA: 12:56 - loss: 7.1159 - acc: 0.0044 - LRFinder: val_loss: 5.4528 - lr = 0.84891882 \n",
            "223/781 [=======>......................] - ETA: 12:54 - loss: 7.1084 - acc: 0.0043 - LRFinder: val_loss: 5.4391 - lr = 0.85276004 \n",
            "224/781 [=======>......................] - ETA: 12:53 - loss: 7.1010 - acc: 0.0043 - LRFinder: val_loss: 5.4266 - lr = 0.85660125 \n",
            "225/781 [=======>......................] - ETA: 12:51 - loss: 7.0935 - acc: 0.0044 - LRFinder: val_loss: 5.4151 - lr = 0.86044247 \n",
            "226/781 [=======>......................] - ETA: 12:50 - loss: 7.0861 - acc: 0.0045 - LRFinder: val_loss: 5.4043 - lr = 0.86428369 \n",
            "227/781 [=======>......................] - ETA: 12:48 - loss: 7.0787 - acc: 0.0045 - LRFinder: val_loss: 5.3946 - lr = 0.86812490 \n",
            "228/781 [=======>......................] - ETA: 12:47 - loss: 7.0713 - acc: 0.0045 - LRFinder: val_loss: 5.3855 - lr = 0.87196612 \n",
            "229/781 [=======>......................] - ETA: 12:45 - loss: 7.0640 - acc: 0.0045 - LRFinder: val_loss: 5.3771 - lr = 0.87580734 \n",
            "230/781 [=======>......................] - ETA: 12:44 - loss: 7.0566 - acc: 0.0044 - LRFinder: val_loss: 5.3696 - lr = 0.87964855 \n",
            "231/781 [=======>......................] - ETA: 12:42 - loss: 7.0493 - acc: 0.0045 - LRFinder: val_loss: 5.3627 - lr = 0.88348977 \n",
            "232/781 [=======>......................] - ETA: 12:41 - loss: 7.0421 - acc: 0.0045 - LRFinder: val_loss: 5.3566 - lr = 0.88733099 \n",
            "233/781 [=======>......................] - ETA: 12:39 - loss: 7.0348 - acc: 0.0046 - LRFinder: val_loss: 5.3512 - lr = 0.89117220 \n",
            "234/781 [=======>......................] - ETA: 12:37 - loss: 7.0276 - acc: 0.0045 - LRFinder: val_loss: 5.3466 - lr = 0.89501342 \n",
            "235/781 [========>.....................] - ETA: 12:36 - loss: 7.0205 - acc: 0.0046 - LRFinder: val_loss: 5.3425 - lr = 0.89885464 \n",
            "236/781 [========>.....................] - ETA: 12:34 - loss: 7.0134 - acc: 0.0046 - LRFinder: val_loss: 5.3392 - lr = 0.90269585 \n",
            "237/781 [========>.....................] - ETA: 12:32 - loss: 7.0063 - acc: 0.0045 - LRFinder: val_loss: 5.3365 - lr = 0.90653707 \n",
            "238/781 [========>.....................] - ETA: 12:31 - loss: 6.9993 - acc: 0.0046 - LRFinder: val_loss: 5.3340 - lr = 0.91037828 \n",
            "239/781 [========>.....................] - ETA: 12:29 - loss: 6.9923 - acc: 0.0046 - LRFinder: val_loss: 5.3322 - lr = 0.91421950 \n",
            "240/781 [========>.....................] - ETA: 12:27 - loss: 6.9854 - acc: 0.0046 - LRFinder: val_loss: 5.3302 - lr = 0.91806072 \n",
            "241/781 [========>.....................] - ETA: 12:26 - loss: 6.9785 - acc: 0.0046 - LRFinder: val_loss: 5.3284 - lr = 0.92190193 \n",
            "242/781 [========>.....................] - ETA: 12:24 - loss: 6.9717 - acc: 0.0046 - LRFinder: val_loss: 5.3268 - lr = 0.92574315 \n",
            "243/781 [========>.....................] - ETA: 12:22 - loss: 6.9649 - acc: 0.0046 - LRFinder: val_loss: 5.3255 - lr = 0.92958437 \n",
            "244/781 [========>.....................] - ETA: 12:21 - loss: 6.9582 - acc: 0.0045 - LRFinder: val_loss: 5.3240 - lr = 0.93342558 \n",
            "245/781 [========>.....................] - ETA: 12:19 - loss: 6.9516 - acc: 0.0046 - LRFinder: val_loss: 5.3228 - lr = 0.93726680 \n",
            "246/781 [========>.....................] - ETA: 12:17 - loss: 6.9449 - acc: 0.0047 - LRFinder: val_loss: 5.3219 - lr = 0.94110802 \n",
            "247/781 [========>.....................] - ETA: 12:16 - loss: 6.9384 - acc: 0.0048 - LRFinder: val_loss: 5.3209 - lr = 0.94494923 \n",
            "248/781 [========>.....................] - ETA: 12:14 - loss: 6.9318 - acc: 0.0048 - LRFinder: val_loss: 5.3203 - lr = 0.94879045 \n",
            "249/781 [========>.....................] - ETA: 12:12 - loss: 6.9254 - acc: 0.0047 - LRFinder: val_loss: 5.3194 - lr = 0.95263166 \n",
            "250/781 [========>.....................] - ETA: 12:11 - loss: 6.9189 - acc: 0.0047 - LRFinder: val_loss: 5.3188 - lr = 0.95647288 \n",
            "251/781 [========>.....................] - ETA: 12:09 - loss: 6.9126 - acc: 0.0047 - LRFinder: val_loss: 5.3184 - lr = 0.96031410 \n",
            "252/781 [========>.....................] - ETA: 12:07 - loss: 6.9062 - acc: 0.0047 - LRFinder: val_loss: 5.3180 - lr = 0.96415531 \n",
            "253/781 [========>.....................] - ETA: 12:06 - loss: 6.9000 - acc: 0.0047 - LRFinder: val_loss: 5.3177 - lr = 0.96799653 \n",
            "254/781 [========>.....................] - ETA: 12:04 - loss: 6.8937 - acc: 0.0047 - LRFinder: val_loss: 5.3175 - lr = 0.97183775 \n",
            "255/781 [========>.....................] - ETA: 12:02 - loss: 6.8876 - acc: 0.0047 - LRFinder: val_loss: 5.3173 - lr = 0.97567896 \n",
            "256/781 [========>.....................] - ETA: 12:01 - loss: 6.8814 - acc: 0.0047 - LRFinder: val_loss: 5.3170 - lr = 0.97952018 \n",
            "257/781 [========>.....................] - ETA: 11:59 - loss: 6.8753 - acc: 0.0047 - LRFinder: val_loss: 5.3171 - lr = 0.98336140 \n",
            "258/781 [========>.....................] - ETA: 11:57 - loss: 6.8693 - acc: 0.0047 - LRFinder: val_loss: 5.3169 - lr = 0.98720261 \n",
            "259/781 [========>.....................] - ETA: 11:56 - loss: 6.8633 - acc: 0.0046 - LRFinder: val_loss: 5.3168 - lr = 0.99104383 \n",
            "260/781 [========>.....................] - ETA: 11:54 - loss: 6.8574 - acc: 0.0046 - LRFinder: val_loss: 5.3172 - lr = 0.99488504 \n",
            "261/781 [=========>....................] - ETA: 11:53 - loss: 6.8515 - acc: 0.0047 - LRFinder: val_loss: 5.3173 - lr = 0.99872626 \n",
            "262/781 [=========>....................] - ETA: 11:51 - loss: 6.8456 - acc: 0.0047 - LRFinder: val_loss: 5.3176 - lr = 1.00256748 \n",
            "263/781 [=========>....................] - ETA: 11:49 - loss: 6.8398 - acc: 0.0047 - LRFinder: val_loss: 5.3181 - lr = 1.00640869 \n",
            "264/781 [=========>....................] - ETA: 11:48 - loss: 6.8340 - acc: 0.0047 - LRFinder: val_loss: 5.3178 - lr = 1.01024991 \n",
            "265/781 [=========>....................] - ETA: 11:46 - loss: 6.8283 - acc: 0.0047 - LRFinder: val_loss: 5.3186 - lr = 1.01409113 \n",
            "266/781 [=========>....................] - ETA: 11:44 - loss: 6.8226 - acc: 0.0047 - LRFinder: val_loss: 5.3189 - lr = 1.01793234 \n",
            "267/781 [=========>....................] - ETA: 11:43 - loss: 6.8170 - acc: 0.0047 - LRFinder: val_loss: 5.3192 - lr = 1.02177356 \n",
            "268/781 [=========>....................] - ETA: 11:41 - loss: 6.8114 - acc: 0.0047 - LRFinder: val_loss: 5.3201 - lr = 1.02561478 \n",
            "269/781 [=========>....................] - ETA: 11:40 - loss: 6.8059 - acc: 0.0047 - LRFinder: val_loss: 5.3205 - lr = 1.02945599 \n",
            "270/781 [=========>....................] - ETA: 11:38 - loss: 6.8004 - acc: 0.0047 - LRFinder: val_loss: 5.3211 - lr = 1.03329721 \n",
            "271/781 [=========>....................] - ETA: 11:36 - loss: 6.7949 - acc: 0.0047 - LRFinder: val_loss: 5.3214 - lr = 1.03713843 \n",
            "272/781 [=========>....................] - ETA: 11:35 - loss: 6.7895 - acc: 0.0047 - LRFinder: val_loss: 5.3224 - lr = 1.04097964 \n",
            "273/781 [=========>....................] - ETA: 11:33 - loss: 6.7841 - acc: 0.0047 - LRFinder: val_loss: 5.3229 - lr = 1.04482086 \n",
            "274/781 [=========>....................] - ETA: 11:31 - loss: 6.7788 - acc: 0.0046 - LRFinder: val_loss: 5.3235 - lr = 1.04866207 \n",
            "275/781 [=========>....................] - ETA: 11:30 - loss: 6.7735 - acc: 0.0046 - LRFinder: val_loss: 5.3244 - lr = 1.05250329 \n",
            "276/781 [=========>....................] - ETA: 11:28 - loss: 6.7682 - acc: 0.0046 - LRFinder: val_loss: 5.3247 - lr = 1.05634451 \n",
            "277/781 [=========>....................] - ETA: 11:27 - loss: 6.7630 - acc: 0.0046 - LRFinder: val_loss: 5.3252 - lr = 1.06018572 \n",
            "278/781 [=========>....................] - ETA: 11:25 - loss: 6.7579 - acc: 0.0046 - LRFinder: val_loss: 5.3260 - lr = 1.06402694 \n",
            "279/781 [=========>....................] - ETA: 11:24 - loss: 6.7527 - acc: 0.0046 - LRFinder: val_loss: 5.3271 - lr = 1.06786816 \n",
            "280/781 [=========>....................] - ETA: 11:22 - loss: 6.7476 - acc: 0.0046 - LRFinder: val_loss: 5.3276 - lr = 1.07170937 \n",
            "281/781 [=========>....................] - ETA: 11:20 - loss: 6.7426 - acc: 0.0046 - LRFinder: val_loss: 5.3289 - lr = 1.07555059 \n",
            "282/781 [=========>....................] - ETA: 11:19 - loss: 6.7376 - acc: 0.0046 - LRFinder: val_loss: 5.3301 - lr = 1.07939181 \n",
            "283/781 [=========>....................] - ETA: 11:17 - loss: 6.7326 - acc: 0.0046 - LRFinder: val_loss: 5.3299 - lr = 1.08323302 \n",
            "284/781 [=========>....................] - ETA: 11:15 - loss: 6.7276 - acc: 0.0046 - LRFinder: val_loss: 5.3311 - lr = 1.08707424 \n",
            "285/781 [=========>....................] - ETA: 11:14 - loss: 6.7227 - acc: 0.0046 - LRFinder: val_loss: 5.3325 - lr = 1.09091545 \n",
            "286/781 [=========>....................] - ETA: 11:12 - loss: 6.7179 - acc: 0.0046 - LRFinder: val_loss: 5.3328 - lr = 1.09475667 \n",
            "287/781 [==========>...................] - ETA: 11:11 - loss: 6.7131 - acc: 0.0046 - LRFinder: val_loss: 5.3343 - lr = 1.09859789 \n",
            "288/781 [==========>...................] - ETA: 11:09 - loss: 6.7083 - acc: 0.0046 - LRFinder: val_loss: 5.3352 - lr = 1.10243910 \n",
            "289/781 [==========>...................] - ETA: 11:08 - loss: 6.7035 - acc: 0.0046 - LRFinder: val_loss: 5.3357 - lr = 1.10628032 \n",
            "290/781 [==========>...................] - ETA: 11:06 - loss: 6.6988 - acc: 0.0046 - LRFinder: val_loss: 5.3372 - lr = 1.11012154 \n",
            "291/781 [==========>...................] - ETA: 11:05 - loss: 6.6941 - acc: 0.0045 - LRFinder: val_loss: 5.3383 - lr = 1.11396275 \n",
            "292/781 [==========>...................] - ETA: 11:03 - loss: 6.6895 - acc: 0.0045 - LRFinder: val_loss: 5.3395 - lr = 1.11780397 \n",
            "293/781 [==========>...................] - ETA: 11:02 - loss: 6.6849 - acc: 0.0045 - LRFinder: val_loss: 5.3409 - lr = 1.12164519 \n",
            "294/781 [==========>...................] - ETA: 11:00 - loss: 6.6803 - acc: 0.0045 - LRFinder: val_loss: 5.3412 - lr = 1.12548640 \n",
            "295/781 [==========>...................] - ETA: 10:59 - loss: 6.6758 - acc: 0.0046 - LRFinder: val_loss: 5.3427 - lr = 1.12932762 \n",
            "296/781 [==========>...................] - ETA: 10:57 - loss: 6.6713 - acc: 0.0045 - LRFinder: val_loss: 5.3445 - lr = 1.13316883 \n",
            "297/781 [==========>...................] - ETA: 10:56 - loss: 6.6668 - acc: 0.0045 - LRFinder: val_loss: 5.3450 - lr = 1.13701005 \n",
            "298/781 [==========>...................] - ETA: 10:54 - loss: 6.6624 - acc: 0.0045 - LRFinder: val_loss: 5.3463 - lr = 1.14085127 \n",
            "299/781 [==========>...................] - ETA: 10:53 - loss: 6.6580 - acc: 0.0045 - LRFinder: val_loss: 5.3478 - lr = 1.14469248 \n",
            "300/781 [==========>...................] - ETA: 10:51 - loss: 6.6536 - acc: 0.0046 - LRFinder: val_loss: 5.3485 - lr = 1.14853370 \n",
            "301/781 [==========>...................] - ETA: 10:50 - loss: 6.6493 - acc: 0.0046 - LRFinder: val_loss: 5.3498 - lr = 1.15237492 \n",
            "302/781 [==========>...................] - ETA: 10:48 - loss: 6.6450 - acc: 0.0046 - LRFinder: val_loss: 5.3512 - lr = 1.15621613 \n",
            "303/781 [==========>...................] - ETA: 10:47 - loss: 6.6407 - acc: 0.0045 - LRFinder: val_loss: 5.3526 - lr = 1.16005735 \n",
            "304/781 [==========>...................] - ETA: 10:45 - loss: 6.6364 - acc: 0.0045 - LRFinder: val_loss: 5.3547 - lr = 1.16389857 \n",
            "305/781 [==========>...................] - ETA: 10:44 - loss: 6.6322 - acc: 0.0045 - LRFinder: val_loss: 5.3557 - lr = 1.16773978 \n",
            "306/781 [==========>...................] - ETA: 10:42 - loss: 6.6281 - acc: 0.0045 - LRFinder: val_loss: 5.3574 - lr = 1.17158100 \n",
            "307/781 [==========>...................] - ETA: 10:41 - loss: 6.6239 - acc: 0.0045 - LRFinder: val_loss: 5.3584 - lr = 1.17542222 \n",
            "308/781 [==========>...................] - ETA: 10:39 - loss: 6.6198 - acc: 0.0046 - LRFinder: val_loss: 5.3603 - lr = 1.17926343 \n",
            "309/781 [==========>...................] - ETA: 10:38 - loss: 6.6157 - acc: 0.0046 - LRFinder: val_loss: 5.3620 - lr = 1.18310465 \n",
            "310/781 [==========>...................] - ETA: 10:36 - loss: 6.6117 - acc: 0.0045 - LRFinder: val_loss: 5.3632 - lr = 1.18694586 \n",
            "311/781 [==========>...................] - ETA: 10:34 - loss: 6.6077 - acc: 0.0045 - LRFinder: val_loss: 5.3654 - lr = 1.19078708 \n",
            "312/781 [==========>...................] - ETA: 10:33 - loss: 6.6037 - acc: 0.0046 - LRFinder: val_loss: 5.3668 - lr = 1.19462830 \n",
            "313/781 [===========>..................] - ETA: 10:31 - loss: 6.5998 - acc: 0.0045 - LRFinder: val_loss: 5.3687 - lr = 1.19846951 \n",
            "314/781 [===========>..................] - ETA: 10:30 - loss: 6.5958 - acc: 0.0046 - LRFinder: val_loss: 5.3703 - lr = 1.20231073 \n",
            "315/781 [===========>..................] - ETA: 10:28 - loss: 6.5919 - acc: 0.0045 - LRFinder: val_loss: 5.3729 - lr = 1.20615195 \n",
            "316/781 [===========>..................] - ETA: 10:27 - loss: 6.5881 - acc: 0.0046 - LRFinder: val_loss: 5.3742 - lr = 1.20999316 \n",
            "317/781 [===========>..................] - ETA: 10:25 - loss: 6.5843 - acc: 0.0046 - LRFinder: val_loss: 5.3760 - lr = 1.21383438 \n",
            "318/781 [===========>..................] - ETA: 10:24 - loss: 6.5805 - acc: 0.0046 - LRFinder: val_loss: 5.3783 - lr = 1.21767560 \n",
            "319/781 [===========>..................] - ETA: 10:22 - loss: 6.5767 - acc: 0.0046 - LRFinder: val_loss: 5.3801 - lr = 1.22151681 \n",
            "320/781 [===========>..................] - ETA: 10:21 - loss: 6.5729 - acc: 0.0046 - LRFinder: val_loss: 5.3812 - lr = 1.22535803 \n",
            "321/781 [===========>..................] - ETA: 10:19 - loss: 6.5692 - acc: 0.0046 - LRFinder: val_loss: 5.3830 - lr = 1.22919924 \n",
            "322/781 [===========>..................] - ETA: 10:18 - loss: 6.5656 - acc: 0.0046 - LRFinder: val_loss: 5.3853 - lr = 1.23304046 \n",
            "323/781 [===========>..................] - ETA: 10:16 - loss: 6.5619 - acc: 0.0046 - LRFinder: val_loss: 5.3862 - lr = 1.23688168 \n",
            "324/781 [===========>..................] - ETA: 10:15 - loss: 6.5583 - acc: 0.0046 - LRFinder: val_loss: 5.3885 - lr = 1.24072289 \n",
            "325/781 [===========>..................] - ETA: 10:13 - loss: 6.5547 - acc: 0.0045 - LRFinder: val_loss: 5.3899 - lr = 1.24456411 \n",
            "326/781 [===========>..................] - ETA: 10:12 - loss: 6.5511 - acc: 0.0045 - LRFinder: val_loss: 5.3937 - lr = 1.24840533 \n",
            "327/781 [===========>..................] - ETA: 10:10 - loss: 6.5476 - acc: 0.0045 - LRFinder: val_loss: 5.3957 - lr = 1.25224654 \n",
            "328/781 [===========>..................] - ETA: 10:09 - loss: 6.5440 - acc: 0.0045 - LRFinder: val_loss: 5.3978 - lr = 1.25608776 \n",
            "329/781 [===========>..................] - ETA: 10:07 - loss: 6.5406 - acc: 0.0045 - LRFinder: val_loss: 5.4011 - lr = 1.25992898 \n",
            "330/781 [===========>..................] - ETA: 10:06 - loss: 6.5371 - acc: 0.0045 - LRFinder: val_loss: 5.4021 - lr = 1.26377019 \n",
            "331/781 [===========>..................] - ETA: 10:04 - loss: 6.5337 - acc: 0.0045 - LRFinder: val_loss: 5.4048 - lr = 1.26761141 \n",
            "332/781 [===========>..................] - ETA: 10:03 - loss: 6.5303 - acc: 0.0045 - LRFinder: val_loss: 5.4068 - lr = 1.27145262 \n",
            "333/781 [===========>..................] - ETA: 10:01 - loss: 6.5269 - acc: 0.0045 - LRFinder: val_loss: 5.4101 - lr = 1.27529384 \n",
            "334/781 [===========>..................] - ETA: 10:00 - loss: 6.5236 - acc: 0.0045 - LRFinder: val_loss: 5.4126 - lr = 1.27913506 \n",
            "335/781 [===========>..................] - ETA: 9:58 - loss: 6.5202 - acc: 0.0045  - LRFinder: val_loss: 5.4146 - lr = 1.28297627 \n",
            "336/781 [===========>..................] - ETA: 9:57 - loss: 6.5170 - acc: 0.0045 - LRFinder: val_loss: 5.4183 - lr = 1.28681749 \n",
            "337/781 [===========>..................] - ETA: 9:55 - loss: 6.5137 - acc: 0.0045 - LRFinder: val_loss: 5.4216 - lr = 1.29065871 \n",
            "338/781 [===========>..................] - ETA: 9:54 - loss: 6.5105 - acc: 0.0045 - LRFinder: val_loss: 5.4238 - lr = 1.29449992 \n",
            "339/781 [============>.................] - ETA: 9:52 - loss: 6.5073 - acc: 0.0045 - LRFinder: val_loss: 5.4269 - lr = 1.29834114 \n",
            "340/781 [============>.................] - ETA: 9:51 - loss: 6.5041 - acc: 0.0045 - LRFinder: val_loss: 5.4296 - lr = 1.30218236 \n",
            "341/781 [============>.................] - ETA: 9:49 - loss: 6.5009 - acc: 0.0045 - LRFinder: val_loss: 5.4326 - lr = 1.30602357 \n",
            "342/781 [============>.................] - ETA: 9:48 - loss: 6.4978 - acc: 0.0045 - LRFinder: val_loss: 5.4351 - lr = 1.30986479 \n",
            "343/781 [============>.................] - ETA: 9:46 - loss: 6.4947 - acc: 0.0045 - LRFinder: val_loss: 5.4379 - lr = 1.31370601 \n",
            "344/781 [============>.................] - ETA: 9:45 - loss: 6.4916 - acc: 0.0045 - LRFinder: val_loss: 5.4424 - lr = 1.31754722 \n",
            "345/781 [============>.................] - ETA: 9:43 - loss: 6.4886 - acc: 0.0045 - LRFinder: val_loss: 5.4432 - lr = 1.32138844 \n",
            "346/781 [============>.................] - ETA: 9:42 - loss: 6.4856 - acc: 0.0045 - LRFinder: val_loss: 5.4467 - lr = 1.32522965 \n",
            "347/781 [============>.................] - ETA: 9:40 - loss: 6.4826 - acc: 0.0045 - LRFinder: val_loss: 5.4505 - lr = 1.32907087 \n",
            "348/781 [============>.................] - ETA: 9:39 - loss: 6.4796 - acc: 0.0046 - LRFinder: val_loss: 5.4528 - lr = 1.33291209 \n",
            "349/781 [============>.................] - ETA: 9:37 - loss: 6.4767 - acc: 0.0045 - LRFinder: val_loss: 5.4569 - lr = 1.33675330 \n",
            "350/781 [============>.................] - ETA: 9:36 - loss: 6.4738 - acc: 0.0045 - LRFinder: val_loss: 5.4609 - lr = 1.34059452 \n",
            "351/781 [============>.................] - ETA: 9:34 - loss: 6.4709 - acc: 0.0045 - LRFinder: val_loss: 5.4633 - lr = 1.34443574 \n",
            "352/781 [============>.................] - ETA: 9:33 - loss: 6.4680 - acc: 0.0045 - LRFinder: val_loss: 5.4669 - lr = 1.34827695 \n",
            "353/781 [============>.................] - ETA: 9:31 - loss: 6.4652 - acc: 0.0045 - LRFinder: val_loss: 5.4705 - lr = 1.35211817 \n",
            "354/781 [============>.................] - ETA: 9:30 - loss: 6.4624 - acc: 0.0045 - LRFinder: val_loss: 5.4743 - lr = 1.35595939 \n",
            "355/781 [============>.................] - ETA: 9:28 - loss: 6.4596 - acc: 0.0045 - LRFinder: val_loss: 5.4796 - lr = 1.35980060 \n",
            "356/781 [============>.................] - ETA: 9:27 - loss: 6.4568 - acc: 0.0045 - LRFinder: val_loss: 5.4821 - lr = 1.36364182 \n",
            "357/781 [============>.................] - ETA: 9:26 - loss: 6.4541 - acc: 0.0045 - LRFinder: val_loss: 5.4850 - lr = 1.36748303 \n",
            "358/781 [============>.................] - ETA: 9:24 - loss: 6.4514 - acc: 0.0045 - LRFinder: val_loss: 5.4881 - lr = 1.37132425 \n",
            "359/781 [============>.................] - ETA: 9:23 - loss: 6.4487 - acc: 0.0045 - LRFinder: val_loss: 5.4926 - lr = 1.37516547 \n",
            "360/781 [============>.................] - ETA: 9:22 - loss: 6.4461 - acc: 0.0045 - LRFinder: val_loss: 5.4984 - lr = 1.37900668 \n",
            "361/781 [============>.................] - ETA: 9:20 - loss: 6.4434 - acc: 0.0045 - LRFinder: val_loss: 5.5031 - lr = 1.38284790 \n",
            "362/781 [============>.................] - ETA: 9:19 - loss: 6.4408 - acc: 0.0045 - LRFinder: val_loss: 5.5073 - lr = 1.38668912 \n",
            "363/781 [============>.................] - ETA: 9:17 - loss: 6.4383 - acc: 0.0045 - LRFinder: val_loss: 5.5123 - lr = 1.39053033 \n",
            "364/781 [============>.................] - ETA: 9:16 - loss: 6.4357 - acc: 0.0045 - LRFinder: val_loss: 5.5148 - lr = 1.39437155 \n",
            "365/781 [=============>................] - ETA: 9:15 - loss: 6.4332 - acc: 0.0045 - LRFinder: val_loss: 5.5196 - lr = 1.39821277 \n",
            "366/781 [=============>................] - ETA: 9:13 - loss: 6.4307 - acc: 0.0045 - LRFinder: val_loss: 5.5235 - lr = 1.40205398 \n",
            "367/781 [=============>................] - ETA: 9:12 - loss: 6.4282 - acc: 0.0044 - LRFinder: val_loss: 5.5260 - lr = 1.40589520 \n",
            "368/781 [=============>................] - ETA: 9:10 - loss: 6.4258 - acc: 0.0045 - LRFinder: val_loss: 5.5333 - lr = 1.40973641 \n",
            "369/781 [=============>................] - ETA: 9:09 - loss: 6.4234 - acc: 0.0045 - LRFinder: val_loss: 5.5364 - lr = 1.41357763 \n",
            "370/781 [=============>................] - ETA: 9:07 - loss: 6.4210 - acc: 0.0045 - LRFinder: val_loss: 5.5412 - lr = 1.41741885 \n",
            "371/781 [=============>................] - ETA: 9:06 - loss: 6.4186 - acc: 0.0045 - LRFinder: val_loss: 5.5468 - lr = 1.42126006 \n",
            "372/781 [=============>................] - ETA: 9:04 - loss: 6.4162 - acc: 0.0045 - LRFinder: val_loss: 5.5511 - lr = 1.42510128 \n",
            "373/781 [=============>................] - ETA: 9:03 - loss: 6.4139 - acc: 0.0045 - LRFinder: val_loss: 5.5565 - lr = 1.42894250 \n",
            "374/781 [=============>................] - ETA: 9:01 - loss: 6.4116 - acc: 0.0045 - LRFinder: val_loss: 5.5611 - lr = 1.43278371 \n",
            "375/781 [=============>................] - ETA: 9:00 - loss: 6.4094 - acc: 0.0045 - LRFinder: val_loss: 5.5679 - lr = 1.43662493 \n",
            "376/781 [=============>................] - ETA: 8:59 - loss: 6.4071 - acc: 0.0045 - LRFinder: val_loss: 5.5717 - lr = 1.44046615 \n",
            "377/781 [=============>................] - ETA: 8:57 - loss: 6.4049 - acc: 0.0045 - LRFinder: val_loss: 5.5780 - lr = 1.44430736 \n",
            "378/781 [=============>................] - ETA: 8:56 - loss: 6.4027 - acc: 0.0045 - LRFinder: val_loss: 5.5825 - lr = 1.44814858 \n",
            "379/781 [=============>................] - ETA: 8:55 - loss: 6.4006 - acc: 0.0046 - LRFinder: val_loss: 5.5866 - lr = 1.45198980 \n",
            "380/781 [=============>................] - ETA: 8:53 - loss: 6.3984 - acc: 0.0046 - LRFinder: val_loss: 5.5949 - lr = 1.45583101 \n",
            "381/781 [=============>................] - ETA: 8:52 - loss: 6.3963 - acc: 0.0046 - LRFinder: val_loss: 5.5987 - lr = 1.45967223 \n",
            "382/781 [=============>................] - ETA: 8:51 - loss: 6.3942 - acc: 0.0046 - LRFinder: val_loss: 5.6056 - lr = 1.46351344 \n",
            "383/781 [=============>................] - ETA: 8:49 - loss: 6.3922 - acc: 0.0045 - LRFinder: val_loss: 5.6110 - lr = 1.46735466 \n",
            "384/781 [=============>................] - ETA: 8:48 - loss: 6.3901 - acc: 0.0046 - LRFinder: val_loss: 5.6160 - lr = 1.47119588 \n",
            "385/781 [=============>................] - ETA: 8:46 - loss: 6.3881 - acc: 0.0045 - LRFinder: val_loss: 5.6208 - lr = 1.47503709 \n",
            "386/781 [=============>................] - ETA: 8:45 - loss: 6.3861 - acc: 0.0045 - LRFinder: val_loss: 5.6258 - lr = 1.47887831 \n",
            "387/781 [=============>................] - ETA: 8:43 - loss: 6.3842 - acc: 0.0045 - LRFinder: val_loss: 5.6329 - lr = 1.48271953 \n",
            "388/781 [=============>................] - ETA: 8:42 - loss: 6.3822 - acc: 0.0045 - LRFinder: val_loss: 5.6394 - lr = 1.48656074 \n",
            "389/781 [=============>................] - ETA: 8:40 - loss: 6.3803 - acc: 0.0045 - LRFinder: val_loss: 5.6472 - lr = 1.49040196 \n",
            "390/781 [=============>................] - ETA: 8:39 - loss: 6.3784 - acc: 0.0045 - LRFinder: val_loss: 5.6533 - lr = 1.49424318 \n",
            "391/781 [==============>...............] - ETA: 8:38 - loss: 6.3766 - acc: 0.0045 - LRFinder: val_loss: 5.6624 - lr = 1.49808439 \n",
            "392/781 [==============>...............] - ETA: 8:36 - loss: 6.3748 - acc: 0.0045 - LRFinder: val_loss: 5.6682 - lr = 1.50192561 \n",
            "393/781 [==============>...............] - ETA: 8:35 - loss: 6.3730 - acc: 0.0045 - LRFinder: val_loss: 5.6747 - lr = 1.50576682 \n",
            "394/781 [==============>...............] - ETA: 8:33 - loss: 6.3712 - acc: 0.0045 - LRFinder: val_loss: 5.6807 - lr = 1.50960804 \n",
            "395/781 [==============>...............] - ETA: 8:32 - loss: 6.3694 - acc: 0.0045 - LRFinder: val_loss: 5.6879 - lr = 1.51344926 \n",
            "396/781 [==============>...............] - ETA: 8:30 - loss: 6.3677 - acc: 0.0045 - LRFinder: val_loss: 5.6952 - lr = 1.51729047 \n",
            "397/781 [==============>...............] - ETA: 8:29 - loss: 6.3660 - acc: 0.0045 - LRFinder: val_loss: 5.6999 - lr = 1.52113169 \n",
            "398/781 [==============>...............] - ETA: 8:28 - loss: 6.3643 - acc: 0.0045 - LRFinder: val_loss: 5.7072 - lr = 1.52497291 \n",
            "399/781 [==============>...............] - ETA: 8:26 - loss: 6.3627 - acc: 0.0045 - LRFinder: val_loss: 5.7136 - lr = 1.52881412 \n",
            "400/781 [==============>...............] - ETA: 8:25 - loss: 6.3611 - acc: 0.0045 - LRFinder: val_loss: 5.7220 - lr = 1.53265534 \n",
            "401/781 [==============>...............] - ETA: 8:23 - loss: 6.3595 - acc: 0.0045 - LRFinder: val_loss: 5.7291 - lr = 1.53649656 \n",
            "402/781 [==============>...............] - ETA: 8:22 - loss: 6.3579 - acc: 0.0045 - LRFinder: val_loss: 5.7366 - lr = 1.54033777 \n",
            "403/781 [==============>...............] - ETA: 8:21 - loss: 6.3564 - acc: 0.0045 - LRFinder: val_loss: 5.7449 - lr = 1.54417899 \n",
            "404/781 [==============>...............] - ETA: 8:19 - loss: 6.3549 - acc: 0.0045 - LRFinder: val_loss: 5.7554 - lr = 1.54802020 \n",
            "405/781 [==============>...............] - ETA: 8:18 - loss: 6.3534 - acc: 0.0045 - LRFinder: val_loss: 5.7633 - lr = 1.55186142 \n",
            "406/781 [==============>...............] - ETA: 8:16 - loss: 6.3519 - acc: 0.0045 - LRFinder: val_loss: 5.7695 - lr = 1.55570264 \n",
            "407/781 [==============>...............] - ETA: 8:15 - loss: 6.3505 - acc: 0.0045 - LRFinder: val_loss: 5.7776 - lr = 1.55954385 \n",
            "408/781 [==============>...............] - ETA: 8:13 - loss: 6.3491 - acc: 0.0045 - LRFinder: val_loss: 5.7851 - lr = 1.56338507 \n",
            "409/781 [==============>...............] - ETA: 8:12 - loss: 6.3477 - acc: 0.0046 - LRFinder: val_loss: 5.7932 - lr = 1.56722629 \n",
            "410/781 [==============>...............] - ETA: 8:11 - loss: 6.3464 - acc: 0.0046 - LRFinder: val_loss: 5.8006 - lr = 1.57106750 \n",
            "411/781 [==============>...............] - ETA: 8:09 - loss: 6.3450 - acc: 0.0045 - LRFinder: val_loss: 5.8094 - lr = 1.57490872 \n",
            "412/781 [==============>...............] - ETA: 8:08 - loss: 6.3437 - acc: 0.0046 - LRFinder: val_loss: 5.8171 - lr = 1.57874994 \n",
            "413/781 [==============>...............] - ETA: 8:06 - loss: 6.3425 - acc: 0.0046 - LRFinder: val_loss: 5.8298 - lr = 1.58259115 \n",
            "414/781 [==============>...............] - ETA: 8:05 - loss: 6.3412 - acc: 0.0046 - LRFinder: val_loss: 5.8385 - lr = 1.58643237 \n",
            "415/781 [==============>...............] - ETA: 8:04 - loss: 6.3400 - acc: 0.0046 - LRFinder: val_loss: 5.8449 - lr = 1.59027359 \n",
            "416/781 [==============>...............] - ETA: 8:02 - loss: 6.3388 - acc: 0.0046 - LRFinder: val_loss: 5.8558 - lr = 1.59411480 \n",
            "417/781 [===============>..............] - ETA: 8:01 - loss: 6.3377 - acc: 0.0046 - LRFinder: val_loss: 5.8633 - lr = 1.59795602 \n",
            "418/781 [===============>..............] - ETA: 7:59 - loss: 6.3365 - acc: 0.0046 - LRFinder: val_loss: 5.8747 - lr = 1.60179723 \n",
            "419/781 [===============>..............] - ETA: 7:58 - loss: 6.3354 - acc: 0.0046 - LRFinder: val_loss: 5.8845 - lr = 1.60563845 \n",
            "420/781 [===============>..............] - ETA: 7:57 - loss: 6.3344 - acc: 0.0046 - LRFinder: val_loss: 5.8915 - lr = 1.60947967 \n",
            "421/781 [===============>..............] - ETA: 7:55 - loss: 6.3333 - acc: 0.0046 - LRFinder: val_loss: 5.9017 - lr = 1.61332088 \n",
            "422/781 [===============>..............] - ETA: 7:54 - loss: 6.3323 - acc: 0.0046 - LRFinder: val_loss: 5.9097 - lr = 1.61716210 \n",
            "423/781 [===============>..............] - ETA: 7:52 - loss: 6.3313 - acc: 0.0046 - LRFinder: val_loss: 5.9207 - lr = 1.62100332 \n",
            "424/781 [===============>..............] - ETA: 7:51 - loss: 6.3303 - acc: 0.0046 - LRFinder: val_loss: 5.9322 - lr = 1.62484453 \n",
            "425/781 [===============>..............] - ETA: 7:50 - loss: 6.3294 - acc: 0.0046 - LRFinder: val_loss: 5.9395 - lr = 1.62868575 \n",
            "426/781 [===============>..............] - ETA: 7:48 - loss: 6.3285 - acc: 0.0046 - LRFinder: val_loss: 5.9500 - lr = 1.63252697 \n",
            "427/781 [===============>..............] - ETA: 7:47 - loss: 6.3276 - acc: 0.0046 - LRFinder: val_loss: 5.9617 - lr = 1.63636818 \n",
            "428/781 [===============>..............] - ETA: 7:46 - loss: 6.3267 - acc: 0.0046 - LRFinder: val_loss: 5.9707 - lr = 1.64020940 \n",
            "429/781 [===============>..............] - ETA: 7:44 - loss: 6.3259 - acc: 0.0046 - LRFinder: val_loss: 5.9791 - lr = 1.64405061 \n",
            "430/781 [===============>..............] - ETA: 7:43 - loss: 6.3251 - acc: 0.0046 - LRFinder: val_loss: 5.9913 - lr = 1.64789183 \n",
            "431/781 [===============>..............] - ETA: 7:42 - loss: 6.3243 - acc: 0.0046 - LRFinder: val_loss: 6.0058 - lr = 1.65173305 \n",
            "432/781 [===============>..............] - ETA: 7:40 - loss: 6.3236 - acc: 0.0046 - LRFinder: val_loss: 6.0153 - lr = 1.65557426 \n",
            "433/781 [===============>..............] - ETA: 7:39 - loss: 6.3229 - acc: 0.0046 - LRFinder: val_loss: 6.0318 - lr = 1.65941548 \n",
            "434/781 [===============>..............] - ETA: 7:37 - loss: 6.3222 - acc: 0.0046 - LRFinder: val_loss: 6.0426 - lr = 1.66325670 \n",
            "435/781 [===============>..............] - ETA: 7:36 - loss: 6.3215 - acc: 0.0046 - LRFinder: val_loss: 6.0477 - lr = 1.66709791 \n",
            "436/781 [===============>..............] - ETA: 7:35 - loss: 6.3209 - acc: 0.0046 - LRFinder: val_loss: 6.0630 - lr = 1.67093913 \n",
            "437/781 [===============>..............] - ETA: 7:33 - loss: 6.3203 - acc: 0.0046 - LRFinder: val_loss: 6.0719 - lr = 1.67478035 \n",
            "438/781 [===============>..............] - ETA: 7:32 - loss: 6.3198 - acc: 0.0046 - LRFinder: val_loss: 6.0848 - lr = 1.67862156 \n",
            "439/781 [===============>..............] - ETA: 7:30 - loss: 6.3192 - acc: 0.0046 - LRFinder: val_loss: 6.0976 - lr = 1.68246278 \n",
            "440/781 [===============>..............] - ETA: 7:29 - loss: 6.3187 - acc: 0.0046 - LRFinder: val_loss: 6.1177 - lr = 1.68630399 \n",
            "441/781 [===============>..............] - ETA: 7:28 - loss: 6.3183 - acc: 0.0046 - LRFinder: val_loss: 6.1292 - lr = 1.69014521 \n",
            "442/781 [===============>..............] - ETA: 7:26 - loss: 6.3178 - acc: 0.0046 - LRFinder: val_loss: 6.1351 - lr = 1.69398643 \n",
            "443/781 [================>.............] - ETA: 7:25 - loss: 6.3174 - acc: 0.0046 - LRFinder: val_loss: 6.1579 - lr = 1.69782764 \n",
            "444/781 [================>.............] - ETA: 7:23 - loss: 6.3171 - acc: 0.0046 - LRFinder: val_loss: 6.1707 - lr = 1.70166886 \n",
            "445/781 [================>.............] - ETA: 7:22 - loss: 6.3167 - acc: 0.0046 - LRFinder: val_loss: 6.1823 - lr = 1.70551008 \n",
            "446/781 [================>.............] - ETA: 7:21 - loss: 6.3164 - acc: 0.0046 - LRFinder: val_loss: 6.1964 - lr = 1.70935129 \n",
            "447/781 [================>.............] - ETA: 7:19 - loss: 6.3162 - acc: 0.0046 - LRFinder: val_loss: 6.2117 - lr = 1.71319251 \n",
            "448/781 [================>.............] - ETA: 7:18 - loss: 6.3159 - acc: 0.0046 - LRFinder: val_loss: 6.2236 - lr = 1.71703373 \n",
            "449/781 [================>.............] - ETA: 7:16 - loss: 6.3157 - acc: 0.0046 - LRFinder: val_loss: 6.2343 - lr = 1.72087494 \n",
            "450/781 [================>.............] - ETA: 7:15 - loss: 6.3155 - acc: 0.0046 - LRFinder: val_loss: 6.2528 - lr = 1.72471616 \n",
            "451/781 [================>.............] - ETA: 7:14 - loss: 6.3154 - acc: 0.0046 - LRFinder: val_loss: 6.2642 - lr = 1.72855738 \n",
            "452/781 [================>.............] - ETA: 7:12 - loss: 6.3153 - acc: 0.0046 - LRFinder: val_loss: 6.2814 - lr = 1.73239859 \n",
            "453/781 [================>.............] - ETA: 7:11 - loss: 6.3152 - acc: 0.0046 - LRFinder: val_loss: 6.2989 - lr = 1.73623981 \n",
            "454/781 [================>.............] - ETA: 7:10 - loss: 6.3152 - acc: 0.0046 - LRFinder: val_loss: 6.3076 - lr = 1.74008102 \n",
            "455/781 [================>.............] - ETA: 7:08 - loss: 6.3152 - acc: 0.0046 - LRFinder: val_loss: 6.3252 - lr = 1.74392224 \n",
            "456/781 [================>.............] - ETA: 7:07 - loss: 6.3152 - acc: 0.0046 - LRFinder: val_loss: 6.3378 - lr = 1.74776346 \n",
            "457/781 [================>.............] - ETA: 7:05 - loss: 6.3152 - acc: 0.0046 - LRFinder: val_loss: 6.3518 - lr = 1.75160467 \n",
            "458/781 [================>.............] - ETA: 7:04 - loss: 6.3153 - acc: 0.0046 - LRFinder: val_loss: 6.3681 - lr = 1.75544589 \n",
            "459/781 [================>.............] - ETA: 7:03 - loss: 6.3154 - acc: 0.0046 - LRFinder: val_loss: 6.3802 - lr = 1.75928711 \n",
            "460/781 [================>.............] - ETA: 7:01 - loss: 6.3156 - acc: 0.0046 - LRFinder: val_loss: 6.4018 - lr = 1.76312832 \n",
            "461/781 [================>.............] - ETA: 7:00 - loss: 6.3158 - acc: 0.0046 - LRFinder: val_loss: 6.4167 - lr = 1.76696954 \n",
            "462/781 [================>.............] - ETA: 6:59 - loss: 6.3160 - acc: 0.0046 - LRFinder: val_loss: 6.4333 - lr = 1.77081076 \n",
            "463/781 [================>.............] - ETA: 6:57 - loss: 6.3162 - acc: 0.0046 - LRFinder: val_loss: 6.4490 - lr = 1.77465197 \n",
            "464/781 [================>.............] - ETA: 6:56 - loss: 6.3165 - acc: 0.0046 - LRFinder: val_loss: 6.4600 - lr = 1.77849319 \n",
            "465/781 [================>.............] - ETA: 6:54 - loss: 6.3168 - acc: 0.0046 - LRFinder: val_loss: 6.4806 - lr = 1.78233440 \n",
            "466/781 [================>.............] - ETA: 6:53 - loss: 6.3172 - acc: 0.0046 - LRFinder: val_loss: 6.4905 - lr = 1.78617562 \n",
            "467/781 [================>.............] - ETA: 6:52 - loss: 6.3175 - acc: 0.0046 - LRFinder: val_loss: 6.5109 - lr = 1.79001684 \n",
            "468/781 [================>.............] - ETA: 6:50 - loss: 6.3180 - acc: 0.0046 - LRFinder: val_loss: 6.5358 - lr = 1.79385805 \n",
            "469/781 [=================>............] - ETA: 6:49 - loss: 6.3184 - acc: 0.0046 - LRFinder: val_loss: 6.5553 - lr = 1.79769927 \n",
            "470/781 [=================>............] - ETA: 6:48 - loss: 6.3189 - acc: 0.0046 - LRFinder: val_loss: 6.5788 - lr = 1.80154049 \n",
            "471/781 [=================>............] - ETA: 6:46 - loss: 6.3195 - acc: 0.0046 - LRFinder: val_loss: 6.5975 - lr = 1.80538170 \n",
            "472/781 [=================>............] - ETA: 6:45 - loss: 6.3201 - acc: 0.0046 - LRFinder: val_loss: 6.6141 - lr = 1.80922292 \n",
            "473/781 [=================>............] - ETA: 6:44 - loss: 6.3207 - acc: 0.0046 - LRFinder: val_loss: 6.6274 - lr = 1.81306414 \n",
            "474/781 [=================>............] - ETA: 6:42 - loss: 6.3213 - acc: 0.0046 - LRFinder: val_loss: 6.6385 - lr = 1.81690535 \n",
            "475/781 [=================>............] - ETA: 6:41 - loss: 6.3220 - acc: 0.0046 - LRFinder: val_loss: 6.6588 - lr = 1.82074657 \n",
            "476/781 [=================>............] - ETA: 6:40 - loss: 6.3227 - acc: 0.0047 - LRFinder: val_loss: 6.6844 - lr = 1.82458778 \n",
            "477/781 [=================>............] - ETA: 6:38 - loss: 6.3235 - acc: 0.0047 - LRFinder: val_loss: 6.6970 - lr = 1.82842900 \n",
            "478/781 [=================>............] - ETA: 6:37 - loss: 6.3243 - acc: 0.0047 - LRFinder: val_loss: 6.7167 - lr = 1.83227022 \n",
            "479/781 [=================>............] - ETA: 6:35 - loss: 6.3251 - acc: 0.0047 - LRFinder: val_loss: 6.7406 - lr = 1.83611143 \n",
            "480/781 [=================>............] - ETA: 6:34 - loss: 6.3259 - acc: 0.0047 - LRFinder: val_loss: 6.7557 - lr = 1.83995265 \n",
            "481/781 [=================>............] - ETA: 6:33 - loss: 6.3268 - acc: 0.0047 - LRFinder: val_loss: 6.7786 - lr = 1.84379387 \n",
            "482/781 [=================>............] - ETA: 6:31 - loss: 6.3278 - acc: 0.0047 - LRFinder: val_loss: 6.7878 - lr = 1.84763508 \n",
            "483/781 [=================>............] - ETA: 6:30 - loss: 6.3287 - acc: 0.0047 - LRFinder: val_loss: 6.8105 - lr = 1.85147630 \n",
            "484/781 [=================>............] - ETA: 6:29 - loss: 6.3297 - acc: 0.0047 - LRFinder: val_loss: 6.8386 - lr = 1.85531752 \n",
            "485/781 [=================>............] - ETA: 6:27 - loss: 6.3308 - acc: 0.0047 - LRFinder: val_loss: 6.8589 - lr = 1.85915873 \n",
            "486/781 [=================>............] - ETA: 6:26 - loss: 6.3319 - acc: 0.0047 - LRFinder: val_loss: 6.8800 - lr = 1.86299995 \n",
            "487/781 [=================>............] - ETA: 6:25 - loss: 6.3330 - acc: 0.0047 - LRFinder: val_loss: 6.8988 - lr = 1.86684117 \n",
            "488/781 [=================>............] - ETA: 6:23 - loss: 6.3341 - acc: 0.0047 - LRFinder: val_loss: 6.9240 - lr = 1.87068238 \n",
            "489/781 [=================>............] - ETA: 6:22 - loss: 6.3353 - acc: 0.0047 - LRFinder: val_loss: 6.9441 - lr = 1.87452360 \n",
            "490/781 [=================>............] - ETA: 6:21 - loss: 6.3366 - acc: 0.0048 - LRFinder: val_loss: 6.9697 - lr = 1.87836481 \n",
            "491/781 [=================>............] - ETA: 6:19 - loss: 6.3379 - acc: 0.0047 - LRFinder: val_loss: 6.9932 - lr = 1.88220603 \n",
            "492/781 [=================>............] - ETA: 6:18 - loss: 6.3392 - acc: 0.0047 - LRFinder: val_loss: 7.0154 - lr = 1.88604725 \n",
            "493/781 [=================>............] - ETA: 6:17 - loss: 6.3406 - acc: 0.0047 - LRFinder: val_loss: 7.0300 - lr = 1.88988846 \n",
            "494/781 [=================>............] - ETA: 6:15 - loss: 6.3420 - acc: 0.0047 - LRFinder: val_loss: 7.0520 - lr = 1.89372968 \n",
            "495/781 [==================>...........] - ETA: 6:14 - loss: 6.3434 - acc: 0.0047 - LRFinder: val_loss: 7.0791 - lr = 1.89757090 \n",
            "496/781 [==================>...........] - ETA: 6:13 - loss: 6.3449 - acc: 0.0047 - LRFinder: val_loss: 7.0976 - lr = 1.90141211 \n",
            "497/781 [==================>...........] - ETA: 6:11 - loss: 6.3464 - acc: 0.0047 - LRFinder: val_loss: 7.1221 - lr = 1.90525333 \n",
            "498/781 [==================>...........] - ETA: 6:10 - loss: 6.3480 - acc: 0.0047 - LRFinder: val_loss: 7.1352 - lr = 1.90909455 \n",
            "499/781 [==================>...........] - ETA: 6:09 - loss: 6.3495 - acc: 0.0047 - LRFinder: val_loss: 7.1698 - lr = 1.91293576 \n",
            "500/781 [==================>...........] - ETA: 6:07 - loss: 6.3512 - acc: 0.0047 - LRFinder: val_loss: 7.1987 - lr = 1.91677698 \n",
            "501/781 [==================>...........] - ETA: 6:06 - loss: 6.3529 - acc: 0.0047 - LRFinder: val_loss: 7.2098 - lr = 1.92061819 \n",
            "502/781 [==================>...........] - ETA: 6:04 - loss: 6.3546 - acc: 0.0047 - LRFinder: val_loss: 7.2432 - lr = 1.92445941 \n",
            "503/781 [==================>...........] - ETA: 6:03 - loss: 6.3563 - acc: 0.0047 - LRFinder: val_loss: 7.2666 - lr = 1.92830063 \n",
            "504/781 [==================>...........] - ETA: 6:02 - loss: 6.3582 - acc: 0.0047 - LRFinder: val_loss: 7.2953 - lr = 1.93214184 \n",
            "505/781 [==================>...........] - ETA: 6:00 - loss: 6.3600 - acc: 0.0047 - LRFinder: val_loss: 7.3216 - lr = 1.93598306 \n",
            "506/781 [==================>...........] - ETA: 5:59 - loss: 6.3619 - acc: 0.0047 - LRFinder: val_loss: 7.3419 - lr = 1.93982428 \n",
            "507/781 [==================>...........] - ETA: 5:58 - loss: 6.3638 - acc: 0.0048 - LRFinder: val_loss: 7.3800 - lr = 1.94366549 \n",
            "508/781 [==================>...........] - ETA: 5:56 - loss: 6.3658 - acc: 0.0048 - LRFinder: val_loss: 7.3905 - lr = 1.94750671 \n",
            "509/781 [==================>...........] - ETA: 5:55 - loss: 6.3679 - acc: 0.0048 - LRFinder: val_loss: 7.4195 - lr = 1.95134793 \n",
            "510/781 [==================>...........] - ETA: 5:54 - loss: 6.3699 - acc: 0.0048 - LRFinder: val_loss: 7.4416 - lr = 1.95518914 \n",
            "511/781 [==================>...........] - ETA: 5:52 - loss: 6.3720 - acc: 0.0048 - LRFinder: val_loss: 7.4712 - lr = 1.95903036 \n",
            "512/781 [==================>...........] - ETA: 5:51 - loss: 6.3742 - acc: 0.0048 - LRFinder: val_loss: 7.5064 - lr = 1.96287157 \n",
            "513/781 [==================>...........] - ETA: 5:50 - loss: 6.3764 - acc: 0.0048 - LRFinder: val_loss: 7.5192 - lr = 1.96671279 \n",
            "514/781 [==================>...........] - ETA: 5:48 - loss: 6.3786 - acc: 0.0048 - LRFinder: val_loss: 7.5475 - lr = 1.97055401 \n",
            "515/781 [==================>...........] - ETA: 5:47 - loss: 6.3809 - acc: 0.0047 - LRFinder: val_loss: 7.5788 - lr = 1.97439522 \n",
            "516/781 [==================>...........] - ETA: 5:46 - loss: 6.3832 - acc: 0.0048 - LRFinder: val_loss: 7.6045 - lr = 1.97823644 \n",
            "517/781 [==================>...........] - ETA: 5:44 - loss: 6.3855 - acc: 0.0047 - LRFinder: val_loss: 7.6244 - lr = 1.98207766 \n",
            "518/781 [==================>...........] - ETA: 5:43 - loss: 6.3879 - acc: 0.0048 - LRFinder: val_loss: 7.6621 - lr = 1.98591887 \n",
            "519/781 [==================>...........] - ETA: 5:42 - loss: 6.3904 - acc: 0.0048 - LRFinder: val_loss: 7.6890 - lr = 1.98976009 \n",
            "520/781 [==================>...........] - ETA: 5:40 - loss: 6.3929 - acc: 0.0047 - LRFinder: val_loss: 7.7317 - lr = 1.99360131 \n",
            "521/781 [===================>..........] - ETA: 5:39 - loss: 6.3955 - acc: 0.0047 - LRFinder: val_loss: 7.7664 - lr = 1.99744252 \n",
            "522/781 [===================>..........] - ETA: 5:37 - loss: 6.3981 - acc: 0.0047 - LRFinder: val_loss: 7.7877 - lr = 2.00128374 \n",
            "523/781 [===================>..........] - ETA: 5:36 - loss: 6.4007 - acc: 0.0048 - LRFinder: val_loss: 7.8337 - lr = 2.00512496 \n",
            "524/781 [===================>..........] - ETA: 5:35 - loss: 6.4035 - acc: 0.0048 - LRFinder: val_loss: 7.8634 - lr = 2.00896617 \n",
            "525/781 [===================>..........] - ETA: 5:33 - loss: 6.4063 - acc: 0.0047 - LRFinder: val_loss: 7.8975 - lr = 2.01280739 \n",
            "526/781 [===================>..........] - ETA: 5:32 - loss: 6.4091 - acc: 0.0048 - LRFinder: val_loss: 7.9229 - lr = 2.01664860 \n",
            "527/781 [===================>..........] - ETA: 5:31 - loss: 6.4120 - acc: 0.0047 - LRFinder: val_loss: 7.9497 - lr = 2.02048982 \n",
            "528/781 [===================>..........] - ETA: 5:29 - loss: 6.4149 - acc: 0.0047 - LRFinder: val_loss: 7.9751 - lr = 2.02433104 \n",
            "529/781 [===================>..........] - ETA: 5:28 - loss: 6.4178 - acc: 0.0047 - LRFinder: val_loss: 7.9994 - lr = 2.02817225 \n",
            "530/781 [===================>..........] - ETA: 5:27 - loss: 6.4208 - acc: 0.0047 - LRFinder: val_loss: 8.0431 - lr = 2.03201347 \n",
            "531/781 [===================>..........] - ETA: 5:25 - loss: 6.4239 - acc: 0.0047 - LRFinder: val_loss: 8.0728 - lr = 2.03585469 \n",
            "532/781 [===================>..........] - ETA: 5:24 - loss: 6.4270 - acc: 0.0047 - LRFinder: val_loss: 8.1088 - lr = 2.03969590 \n",
            "533/781 [===================>..........] - ETA: 5:23 - loss: 6.4301 - acc: 0.0047 - LRFinder: val_loss: 8.1368 - lr = 2.04353712 \n",
            "534/781 [===================>..........] - ETA: 5:21 - loss: 6.4333 - acc: 0.0047 - LRFinder: val_loss: 8.1681 - lr = 2.04737834 \n",
            "535/781 [===================>..........] - ETA: 5:20 - loss: 6.4366 - acc: 0.0047 - LRFinder: val_loss: 8.2059 - lr = 2.05121955 \n",
            "536/781 [===================>..........] - ETA: 5:19 - loss: 6.4399 - acc: 0.0047 - LRFinder: val_loss: 8.2341 - lr = 2.05506077 \n",
            "537/781 [===================>..........] - ETA: 5:17 - loss: 6.4432 - acc: 0.0047 - LRFinder: val_loss: 8.2666 - lr = 2.05890198 \n",
            "538/781 [===================>..........] - ETA: 5:16 - loss: 6.4466 - acc: 0.0047 - LRFinder: val_loss: 8.3053 - lr = 2.06274320 \n",
            "539/781 [===================>..........] - ETA: 5:15 - loss: 6.4500 - acc: 0.0047 - LRFinder: val_loss: 8.3334 - lr = 2.06658442 \n",
            "540/781 [===================>..........] - ETA: 5:13 - loss: 6.4535 - acc: 0.0047 - LRFinder: val_loss: 8.3694 - lr = 2.07042563 \n",
            "541/781 [===================>..........] - ETA: 5:12 - loss: 6.4571 - acc: 0.0047 - LRFinder: val_loss: 8.4178 - lr = 2.07426685 \n",
            "542/781 [===================>..........] - ETA: 5:11 - loss: 6.4607 - acc: 0.0047 - LRFinder: val_loss: 8.4494 - lr = 2.07810807 \n",
            "543/781 [===================>..........] - ETA: 5:09 - loss: 6.4644 - acc: 0.0047 - LRFinder: val_loss: 8.4772 - lr = 2.08194928 \n",
            "544/781 [===================>..........] - ETA: 5:08 - loss: 6.4681 - acc: 0.0048 - LRFinder: val_loss: 8.5329 - lr = 2.08579050 \n",
            "545/781 [===================>..........] - ETA: 5:07 - loss: 6.4718 - acc: 0.0048 - LRFinder: val_loss: 8.5708 - lr = 2.08963172 \n",
            "546/781 [===================>..........] - ETA: 5:05 - loss: 6.4757 - acc: 0.0048 - LRFinder: val_loss: 8.6006 - lr = 2.09347293 \n",
            "547/781 [====================>.........] - ETA: 5:04 - loss: 6.4796 - acc: 0.0047 - LRFinder: val_loss: 8.6489 - lr = 2.09731415 \n",
            "548/781 [====================>.........] - ETA: 5:03 - loss: 6.4835 - acc: 0.0047 - LRFinder: val_loss: 8.6859 - lr = 2.10115536 \n",
            "549/781 [====================>.........] - ETA: 5:01 - loss: 6.4875 - acc: 0.0048 - LRFinder: val_loss: 8.7192 - lr = 2.10499658 \n",
            "550/781 [====================>.........] - ETA: 5:00 - loss: 6.4916 - acc: 0.0048 - LRFinder: val_loss: 8.7457 - lr = 2.10883780 \n",
            "551/781 [====================>.........] - ETA: 4:59 - loss: 6.4957 - acc: 0.0048 - LRFinder: val_loss: 8.7786 - lr = 2.11267901 \n",
            "552/781 [====================>.........] - ETA: 4:57 - loss: 6.4998 - acc: 0.0048 - LRFinder: val_loss: 8.8295 - lr = 2.11652023 \n",
            "553/781 [====================>.........] - ETA: 4:56 - loss: 6.5040 - acc: 0.0048 - LRFinder: val_loss: 8.8664 - lr = 2.12036145 \n",
            "554/781 [====================>.........] - ETA: 4:55 - loss: 6.5083 - acc: 0.0048 - LRFinder: val_loss: 8.8926 - lr = 2.12420266 \n",
            "555/781 [====================>.........] - ETA: 4:54 - loss: 6.5126 - acc: 0.0048 - LRFinder: val_loss: 8.9301 - lr = 2.12804388 \n",
            "556/781 [====================>.........] - ETA: 4:52 - loss: 6.5169 - acc: 0.0048 - LRFinder: val_loss: 8.9815 - lr = 2.13188510 \n",
            "557/781 [====================>.........] - ETA: 4:51 - loss: 6.5214 - acc: 0.0048 - LRFinder: val_loss: 9.0187 - lr = 2.13572631 \n",
            "558/781 [====================>.........] - ETA: 4:50 - loss: 6.5258 - acc: 0.0048 - LRFinder: val_loss: 9.0448 - lr = 2.13956753 \n",
            "559/781 [====================>.........] - ETA: 4:48 - loss: 6.5303 - acc: 0.0048 - LRFinder: val_loss: 9.0999 - lr = 2.14340875 \n",
            "560/781 [====================>.........] - ETA: 4:47 - loss: 6.5349 - acc: 0.0048 - LRFinder: val_loss: 9.1386 - lr = 2.14724996 \n",
            "561/781 [====================>.........] - ETA: 4:46 - loss: 6.5396 - acc: 0.0047 - LRFinder: val_loss: 9.1706 - lr = 2.15109118 \n",
            "562/781 [====================>.........] - ETA: 4:44 - loss: 6.5443 - acc: 0.0048 - LRFinder: val_loss: 9.2370 - lr = 2.15493239 \n",
            "563/781 [====================>.........] - ETA: 4:43 - loss: 6.5490 - acc: 0.0047 - LRFinder: val_loss: 9.2871 - lr = 2.15877361 \n",
            "564/781 [====================>.........] - ETA: 4:42 - loss: 6.5539 - acc: 0.0048 - LRFinder: val_loss: 9.3274 - lr = 2.16261483 \n",
            "565/781 [====================>.........] - ETA: 4:40 - loss: 6.5588 - acc: 0.0048 - LRFinder: val_loss: 9.3866 - lr = 2.16645604 \n",
            "566/781 [====================>.........] - ETA: 4:39 - loss: 6.5638 - acc: 0.0048 - LRFinder: val_loss: 9.4158 - lr = 2.17029726 \n",
            "567/781 [====================>.........] - ETA: 4:38 - loss: 6.5688 - acc: 0.0048 - LRFinder: val_loss: 9.4554 - lr = 2.17413848 \n",
            "568/781 [====================>.........] - ETA: 4:36 - loss: 6.5739 - acc: 0.0048 - LRFinder: val_loss: 9.5062 - lr = 2.17797969 \n",
            "569/781 [====================>.........] - ETA: 4:35 - loss: 6.5791 - acc: 0.0048 - LRFinder: val_loss: 9.5464 - lr = 2.18182091 \n",
            "570/781 [====================>.........] - ETA: 4:34 - loss: 6.5843 - acc: 0.0048 - LRFinder: val_loss: 9.5874 - lr = 2.18566213 \n",
            "571/781 [====================>.........] - ETA: 4:32 - loss: 6.5895 - acc: 0.0047 - LRFinder: val_loss: 9.6424 - lr = 2.18950334 \n",
            "572/781 [====================>.........] - ETA: 4:31 - loss: 6.5949 - acc: 0.0048 - LRFinder: val_loss: 9.6684 - lr = 2.19334456 \n",
            "573/781 [=====================>........] - ETA: 4:30 - loss: 6.6002 - acc: 0.0048 - LRFinder: val_loss: 9.7150 - lr = 2.19718577 \n",
            "574/781 [=====================>........] - ETA: 4:28 - loss: 6.6057 - acc: 0.0048 - LRFinder: val_loss: 9.7646 - lr = 2.20102699 \n",
            "575/781 [=====================>........] - ETA: 4:27 - loss: 6.6112 - acc: 0.0048 - LRFinder: val_loss: 9.8000 - lr = 2.20486821 \n",
            "576/781 [=====================>........] - ETA: 4:26 - loss: 6.6167 - acc: 0.0048 - LRFinder: val_loss: 9.8482 - lr = 2.20870942 \n",
            "577/781 [=====================>........] - ETA: 4:24 - loss: 6.6223 - acc: 0.0048 - LRFinder: val_loss: 9.8861 - lr = 2.21255064 \n",
            "578/781 [=====================>........] - ETA: 4:23 - loss: 6.6279 - acc: 0.0048 - LRFinder: val_loss: 9.9438 - lr = 2.21639186 \n",
            "579/781 [=====================>........] - ETA: 4:22 - loss: 6.6337 - acc: 0.0048 - LRFinder: val_loss: 9.9665 - lr = 2.22023307 \n",
            "580/781 [=====================>........] - ETA: 4:20 - loss: 6.6394 - acc: 0.0048 - LRFinder: val_loss: 10.0240 - lr = 2.22407429 \n",
            "581/781 [=====================>........] - ETA: 4:19 - loss: 6.6452 - acc: 0.0048 - LRFinder: val_loss: 10.0784 - lr = 2.22791551 \n",
            "582/781 [=====================>........] - ETA: 4:18 - loss: 6.6511 - acc: 0.0048 - LRFinder: val_loss: 10.1434 - lr = 2.23175672 \n",
            "583/781 [=====================>........] - ETA: 4:16 - loss: 6.6571 - acc: 0.0048 - LRFinder: val_loss: 10.2024 - lr = 2.23559794 \n",
            "584/781 [=====================>........] - ETA: 4:15 - loss: 6.6632 - acc: 0.0048 - LRFinder: val_loss: 10.2403 - lr = 2.23943915 \n",
            "585/781 [=====================>........] - ETA: 4:14 - loss: 6.6693 - acc: 0.0048 - LRFinder: val_loss: 10.2850 - lr = 2.24328037 \n",
            "586/781 [=====================>........] - ETA: 4:12 - loss: 6.6755 - acc: 0.0048 - LRFinder: val_loss: 10.3510 - lr = 2.24712159 \n",
            "587/781 [=====================>........] - ETA: 4:11 - loss: 6.6817 - acc: 0.0048 - LRFinder: val_loss: 10.4057 - lr = 2.25096280 \n",
            "588/781 [=====================>........] - ETA: 4:10 - loss: 6.6881 - acc: 0.0047 - LRFinder: val_loss: 10.4471 - lr = 2.25480402 \n",
            "589/781 [=====================>........] - ETA: 4:08 - loss: 6.6945 - acc: 0.0047 - LRFinder: val_loss: 10.5019 - lr = 2.25864524 \n",
            "590/781 [=====================>........] - ETA: 4:07 - loss: 6.7009 - acc: 0.0047 - LRFinder: val_loss: 10.5482 - lr = 2.26248645 \n",
            "591/781 [=====================>........] - ETA: 4:06 - loss: 6.7074 - acc: 0.0047 - LRFinder: val_loss: 10.6165 - lr = 2.26632767 \n",
            "592/781 [=====================>........] - ETA: 4:05 - loss: 6.7140 - acc: 0.0047 - LRFinder: val_loss: 10.6462 - lr = 2.27016889 \n",
            "593/781 [=====================>........] - ETA: 4:03 - loss: 6.7207 - acc: 0.0047 - LRFinder: val_loss: 10.6980 - lr = 2.27401010 \n",
            "594/781 [=====================>........] - ETA: 4:02 - loss: 6.7274 - acc: 0.0047 - LRFinder: val_loss: 10.7568 - lr = 2.27785132 \n",
            "595/781 [=====================>........] - ETA: 4:01 - loss: 6.7341 - acc: 0.0047 - LRFinder: val_loss: 10.8282 - lr = 2.28169254 \n",
            "596/781 [=====================>........] - ETA: 3:59 - loss: 6.7410 - acc: 0.0047 - LRFinder: val_loss: 10.8611 - lr = 2.28553375 \n",
            "597/781 [=====================>........] - ETA: 3:58 - loss: 6.7479 - acc: 0.0047 - LRFinder: val_loss: 10.9078 - lr = 2.28937497 \n",
            "598/781 [=====================>........] - ETA: 3:57 - loss: 6.7549 - acc: 0.0047 - LRFinder: val_loss: 10.9659 - lr = 2.29321618 \n",
            "599/781 [======================>.......] - ETA: 3:55 - loss: 6.7619 - acc: 0.0047 - LRFinder: val_loss: 11.0005 - lr = 2.29705740 \n",
            "600/781 [======================>.......] - ETA: 3:54 - loss: 6.7689 - acc: 0.0047 - LRFinder: val_loss: 11.0735 - lr = 2.30089862 \n",
            "601/781 [======================>.......] - ETA: 3:53 - loss: 6.7761 - acc: 0.0047 - LRFinder: val_loss: 11.0999 - lr = 2.30473983 \n",
            "602/781 [======================>.......] - ETA: 3:51 - loss: 6.7833 - acc: 0.0047 - LRFinder: val_loss: 11.1713 - lr = 2.30858105 \n",
            "603/781 [======================>.......] - ETA: 3:50 - loss: 6.7906 - acc: 0.0047 - LRFinder: val_loss: 11.2224 - lr = 2.31242227 \n",
            "604/781 [======================>.......] - ETA: 3:49 - loss: 6.7979 - acc: 0.0047 - LRFinder: val_loss: 11.2780 - lr = 2.31626348 \n",
            "605/781 [======================>.......] - ETA: 3:47 - loss: 6.8053 - acc: 0.0047 - LRFinder: val_loss: 11.3458 - lr = 2.32010470 \n",
            "606/781 [======================>.......] - ETA: 3:46 - loss: 6.8128 - acc: 0.0047 - LRFinder: val_loss: 11.4036 - lr = 2.32394592 \n",
            "607/781 [======================>.......] - ETA: 3:45 - loss: 6.8204 - acc: 0.0047 - LRFinder: val_loss: 11.4654 - lr = 2.32778713 \n",
            "608/781 [======================>.......] - ETA: 3:43 - loss: 6.8280 - acc: 0.0047 - LRFinder: val_loss: 11.5245 - lr = 2.33162835 \n",
            "609/781 [======================>.......] - ETA: 3:42 - loss: 6.8357 - acc: 0.0047 - LRFinder: val_loss: 11.5922 - lr = 2.33546956 \n",
            "610/781 [======================>.......] - ETA: 3:41 - loss: 6.8435 - acc: 0.0047 - LRFinder: val_loss: 11.6449 - lr = 2.33931078 \n",
            "611/781 [======================>.......] - ETA: 3:40 - loss: 6.8514 - acc: 0.0047 - LRFinder: val_loss: 11.7010 - lr = 2.34315200 \n",
            "612/781 [======================>.......] - ETA: 3:38 - loss: 6.8593 - acc: 0.0047 - LRFinder: val_loss: 11.7654 - lr = 2.34699321 \n",
            "613/781 [======================>.......] - ETA: 3:37 - loss: 6.8673 - acc: 0.0047 - LRFinder: val_loss: 11.8246 - lr = 2.35083443 \n",
            "614/781 [======================>.......] - ETA: 3:36 - loss: 6.8754 - acc: 0.0047 - LRFinder: val_loss: 11.8905 - lr = 2.35467565 \n",
            "615/781 [======================>.......] - ETA: 3:34 - loss: 6.8835 - acc: 0.0047 - LRFinder: val_loss: 11.9636 - lr = 2.35851686 \n",
            "616/781 [======================>.......] - ETA: 3:33 - loss: 6.8918 - acc: 0.0047 - LRFinder: val_loss: 12.0177 - lr = 2.36235808 \n",
            "617/781 [======================>.......] - ETA: 3:32 - loss: 6.9001 - acc: 0.0047 - LRFinder: val_loss: 12.0798 - lr = 2.36619930 \n",
            "618/781 [======================>.......] - ETA: 3:30 - loss: 6.9085 - acc: 0.0047 - LRFinder: val_loss: 12.1216 - lr = 2.37004051 \n",
            "619/781 [======================>.......] - ETA: 3:29 - loss: 6.9169 - acc: 0.0047 - LRFinder: val_loss: 12.1999 - lr = 2.37388173 \n",
            "620/781 [======================>.......] - ETA: 3:28 - loss: 6.9254 - acc: 0.0047 - LRFinder: val_loss: 12.2676 - lr = 2.37772294 \n",
            "621/781 [======================>.......] - ETA: 3:27 - loss: 6.9340 - acc: 0.0047 - LRFinder: val_loss: 12.2887 - lr = 2.38156416 \n",
            "622/781 [======================>.......] - ETA: 3:25 - loss: 6.9426 - acc: 0.0047 - LRFinder: val_loss: 12.3647 - lr = 2.38540538 \n",
            "623/781 [======================>.......] - ETA: 3:24 - loss: 6.9513 - acc: 0.0047 - LRFinder: val_loss: 12.4207 - lr = 2.38924659 \n",
            "624/781 [======================>.......] - ETA: 3:23 - loss: 6.9601 - acc: 0.0047 - LRFinder: val_loss: 12.4882 - lr = 2.39308781 \n",
            "625/781 [=======================>......] - ETA: 3:21 - loss: 6.9689 - acc: 0.0047 - LRFinder: val_loss: 12.5707 - lr = 2.39692903 \n",
            "626/781 [=======================>......] - ETA: 3:20 - loss: 6.9779 - acc: 0.0047 - LRFinder: val_loss: 12.5917 - lr = 2.40077024 \n",
            "627/781 [=======================>......] - ETA: 3:19 - loss: 6.9868 - acc: 0.0047 - LRFinder: val_loss: 12.6748 - lr = 2.40461146 \n",
            "628/781 [=======================>......] - ETA: 3:17 - loss: 6.9959 - acc: 0.0048 - LRFinder: val_loss: 12.7259 - lr = 2.40845268 \n",
            "629/781 [=======================>......] - ETA: 3:16 - loss: 7.0050 - acc: 0.0048 - LRFinder: val_loss: 12.8017 - lr = 2.41229389 \n",
            "630/781 [=======================>......] - ETA: 3:15 - loss: 7.0142 - acc: 0.0047 - LRFinder: val_loss: 12.8586 - lr = 2.41613511 \n",
            "631/781 [=======================>......] - ETA: 3:14 - loss: 7.0235 - acc: 0.0047 - LRFinder: val_loss: 12.9189 - lr = 2.41997633 \n",
            "632/781 [=======================>......] - ETA: 3:12 - loss: 7.0328 - acc: 0.0047 - LRFinder: val_loss: 13.0027 - lr = 2.42381754 \n",
            "633/781 [=======================>......] - ETA: 3:11 - loss: 7.0422 - acc: 0.0047 - LRFinder: val_loss: 13.1064 - lr = 2.42765876 \n",
            "634/781 [=======================>......] - ETA: 3:10 - loss: 7.0518 - acc: 0.0047 - LRFinder: val_loss: 13.1663 - lr = 2.43149997 \n",
            "635/781 [=======================>......] - ETA: 3:08 - loss: 7.0614 - acc: 0.0047 - LRFinder: val_loss: 13.1952 - lr = 2.43534119 \n",
            "636/781 [=======================>......] - ETA: 3:07 - loss: 7.0711 - acc: 0.0047 - LRFinder: val_loss: 13.2992 - lr = 2.43918241 \n",
            "637/781 [=======================>......] - ETA: 3:06 - loss: 7.0808 - acc: 0.0047 - LRFinder: val_loss: 13.3611 - lr = 2.44302362 \n",
            "638/781 [=======================>......] - ETA: 3:04 - loss: 7.0907 - acc: 0.0047 - LRFinder: val_loss: 13.4677 - lr = 2.44686484 \n",
            "639/781 [=======================>......] - ETA: 3:03 - loss: 7.1007 - acc: 0.0047 - LRFinder: val_loss: 13.5255 - lr = 2.45070606 \n",
            "640/781 [=======================>......] - ETA: 3:02 - loss: 7.1107 - acc: 0.0047 - LRFinder: val_loss: 13.5978 - lr = 2.45454727 \n",
            "641/781 [=======================>......] - ETA: 3:01 - loss: 7.1208 - acc: 0.0047 - LRFinder: val_loss: 13.6652 - lr = 2.45838849 \n",
            "642/781 [=======================>......] - ETA: 2:59 - loss: 7.1310 - acc: 0.0047 - LRFinder: val_loss: 13.7384 - lr = 2.46222971 \n",
            "643/781 [=======================>......] - ETA: 2:58 - loss: 7.1413 - acc: 0.0047 - LRFinder: val_loss: 13.8185 - lr = 2.46607092 \n",
            "644/781 [=======================>......] - ETA: 2:57 - loss: 7.1517 - acc: 0.0047 - LRFinder: val_loss: 13.8648 - lr = 2.46991214 \n",
            "645/781 [=======================>......] - ETA: 2:55 - loss: 7.1621 - acc: 0.0047 - LRFinder: val_loss: 13.9855 - lr = 2.47375335 \n",
            "646/781 [=======================>......] - ETA: 2:54 - loss: 7.1726 - acc: 0.0047 - LRFinder: val_loss: 14.0350 - lr = 2.47759457 \n",
            "647/781 [=======================>......] - ETA: 2:53 - loss: 7.1832 - acc: 0.0047 - LRFinder: val_loss: 14.1013 - lr = 2.48143579 \n",
            "648/781 [=======================>......] - ETA: 2:51 - loss: 7.1939 - acc: 0.0047 - LRFinder: val_loss: 14.1719 - lr = 2.48527700 \n",
            "649/781 [=======================>......] - ETA: 2:50 - loss: 7.2047 - acc: 0.0047 - LRFinder: val_loss: 14.2567 - lr = 2.48911822 \n",
            "650/781 [=======================>......] - ETA: 2:49 - loss: 7.2155 - acc: 0.0047 - LRFinder: val_loss: 14.3244 - lr = 2.49295944 \n",
            "651/781 [========================>.....] - ETA: 2:47 - loss: 7.2264 - acc: 0.0047 - LRFinder: val_loss: 14.3600 - lr = 2.49680065 \n",
            "652/781 [========================>.....] - ETA: 2:46 - loss: 7.2374 - acc: 0.0047 - LRFinder: val_loss: 14.4298 - lr = 2.50064187 \n",
            "653/781 [========================>.....] - ETA: 2:45 - loss: 7.2484 - acc: 0.0047 - LRFinder: val_loss: 14.5304 - lr = 2.50448309 \n",
            "654/781 [========================>.....] - ETA: 2:44 - loss: 7.2595 - acc: 0.0047 - LRFinder: val_loss: 14.6212 - lr = 2.50832430 \n",
            "655/781 [========================>.....] - ETA: 2:42 - loss: 7.2708 - acc: 0.0047 - LRFinder: val_loss: 14.6648 - lr = 2.51216552 \n",
            "656/781 [========================>.....] - ETA: 2:41 - loss: 7.2820 - acc: 0.0047 - LRFinder: val_loss: 14.7447 - lr = 2.51600673 \n",
            "657/781 [========================>.....] - ETA: 2:40 - loss: 7.2934 - acc: 0.0047 - LRFinder: val_loss: 14.8202 - lr = 2.51984795 \n",
            "658/781 [========================>.....] - ETA: 2:38 - loss: 7.3048 - acc: 0.0047 - LRFinder: val_loss: 14.9041 - lr = 2.52368917 \n",
            "659/781 [========================>.....] - ETA: 2:37 - loss: 7.3164 - acc: 0.0047 - LRFinder: val_loss: 14.9848 - lr = 2.52753038 \n",
            "660/781 [========================>.....] - ETA: 2:36 - loss: 7.3280 - acc: 0.0047 - LRFinder: val_loss: 15.0506 - lr = 2.53137160 \n",
            "661/781 [========================>.....] - ETA: 2:34 - loss: 7.3397 - acc: 0.0047 - LRFinder: val_loss: 15.1423 - lr = 2.53521282 \n",
            "662/781 [========================>.....] - ETA: 2:33 - loss: 7.3515 - acc: 0.0047 - LRFinder: val_loss: 15.2286 - lr = 2.53905403 \n",
            "663/781 [========================>.....] - ETA: 2:32 - loss: 7.3633 - acc: 0.0047 - LRFinder: val_loss: 15.3025 - lr = 2.54289525 \n",
            "664/781 [========================>.....] - ETA: 2:31 - loss: 7.3753 - acc: 0.0047 - LRFinder: val_loss: 15.3721 - lr = 2.54673647 \n",
            "665/781 [========================>.....] - ETA: 2:29 - loss: 7.3873 - acc: 0.0047 - LRFinder: val_loss: 15.4625 - lr = 2.55057768 \n",
            "666/781 [========================>.....] - ETA: 2:28 - loss: 7.3994 - acc: 0.0047 - LRFinder: val_loss: 15.5607 - lr = 2.55441890 \n",
            "667/781 [========================>.....] - ETA: 2:27 - loss: 7.4117 - acc: 0.0047 - LRFinder: val_loss: 15.6321 - lr = 2.55826012 \n",
            "668/781 [========================>.....] - ETA: 2:25 - loss: 7.4240 - acc: 0.0047 - LRFinder: val_loss: 15.6987 - lr = 2.56210133 \n",
            "669/781 [========================>.....] - ETA: 2:24 - loss: 7.4364 - acc: 0.0047 - LRFinder: val_loss: 15.7765 - lr = 2.56594255 \n",
            "670/781 [========================>.....] - ETA: 2:23 - loss: 7.4488 - acc: 0.0047 - LRFinder: val_loss: 15.8664 - lr = 2.56978376 \n",
            "671/781 [========================>.....] - ETA: 2:21 - loss: 7.4613 - acc: 0.0047 - LRFinder: val_loss: 15.9671 - lr = 2.57362498 \n",
            "672/781 [========================>.....] - ETA: 2:20 - loss: 7.4740 - acc: 0.0047 - LRFinder: val_loss: 16.0833 - lr = 2.57746620 \n",
            "673/781 [========================>.....] - ETA: 2:19 - loss: 7.4868 - acc: 0.0047 - LRFinder: val_loss: 16.1527 - lr = 2.58130741 \n",
            "674/781 [========================>.....] - ETA: 2:18 - loss: 7.4997 - acc: 0.0047 - LRFinder: val_loss: 16.2276 - lr = 2.58514863 \n",
            "675/781 [========================>.....] - ETA: 2:16 - loss: 7.5126 - acc: 0.0047 - LRFinder: val_loss: 16.3298 - lr = 2.58898985 \n",
            "676/781 [========================>.....] - ETA: 2:15 - loss: 7.5256 - acc: 0.0047 - LRFinder: val_loss: 16.4196 - lr = 2.59283106 \n",
            "677/781 [=========================>....] - ETA: 2:14 - loss: 7.5388 - acc: 0.0047 - LRFinder: val_loss: 16.5445 - lr = 2.59667228 \n",
            "678/781 [=========================>....] - ETA: 2:12 - loss: 7.5520 - acc: 0.0047 - LRFinder: val_loss: 16.6265 - lr = 2.60051350 \n",
            "679/781 [=========================>....] - ETA: 2:11 - loss: 7.5654 - acc: 0.0047 - LRFinder: val_loss: 16.6685 - lr = 2.60435471 \n",
            "680/781 [=========================>....] - ETA: 2:10 - loss: 7.5788 - acc: 0.0047 - LRFinder: val_loss: 16.7645 - lr = 2.60819593 \n",
            "681/781 [=========================>....] - ETA: 2:08 - loss: 7.5923 - acc: 0.0048 - LRFinder: val_loss: 16.8682 - lr = 2.61203714 \n",
            "682/781 [=========================>....] - ETA: 2:07 - loss: 7.6059 - acc: 0.0048 - LRFinder: val_loss: 16.9404 - lr = 2.61587836 \n",
            "683/781 [=========================>....] - ETA: 2:06 - loss: 7.6196 - acc: 0.0047 - LRFinder: val_loss: 17.0139 - lr = 2.61971958 \n",
            "684/781 [=========================>....] - ETA: 2:05 - loss: 7.6333 - acc: 0.0047 - LRFinder: val_loss: 17.0562 - lr = 2.62356079 \n",
            "685/781 [=========================>....] - ETA: 2:03 - loss: 7.6470 - acc: 0.0047 - LRFinder: val_loss: 17.1500 - lr = 2.62740201 \n",
            "686/781 [=========================>....] - ETA: 2:02 - loss: 7.6609 - acc: 0.0047 - LRFinder: val_loss: 17.2283 - lr = 2.63124323 \n",
            "687/781 [=========================>....] - ETA: 2:01 - loss: 7.6748 - acc: 0.0047 - LRFinder: val_loss: 17.2970 - lr = 2.63508444 \n",
            "688/781 [=========================>....] - ETA: 1:59 - loss: 7.6888 - acc: 0.0047 - LRFinder: val_loss: 17.4061 - lr = 2.63892566 \n",
            "689/781 [=========================>....] - ETA: 1:58 - loss: 7.7029 - acc: 0.0047 - LRFinder: val_loss: 17.4840 - lr = 2.64276688 \n",
            "690/781 [=========================>....] - ETA: 1:57 - loss: 7.7171 - acc: 0.0047 - LRFinder: val_loss: 17.6288 - lr = 2.64660809 \n",
            "691/781 [=========================>....] - ETA: 1:56 - loss: 7.7314 - acc: 0.0047 - LRFinder: val_loss: 17.6791 - lr = 2.65044931 \n",
            "692/781 [=========================>....] - ETA: 1:54 - loss: 7.7458 - acc: 0.0048 - LRFinder: val_loss: 17.7463 - lr = 2.65429052 \n",
            "693/781 [=========================>....] - ETA: 1:53 - loss: 7.7602 - acc: 0.0048 - LRFinder: val_loss: 17.8968 - lr = 2.65813174 \n",
            "694/781 [=========================>....] - ETA: 1:52 - loss: 7.7748 - acc: 0.0048 - LRFinder: val_loss: 17.9551 - lr = 2.66197296 \n",
            "695/781 [=========================>....] - ETA: 1:50 - loss: 7.7895 - acc: 0.0048 - LRFinder: val_loss: 18.0752 - lr = 2.66581417 \n",
            "696/781 [=========================>....] - ETA: 1:49 - loss: 7.8043 - acc: 0.0048 - LRFinder: val_loss: 18.1592 - lr = 2.66965539 \n",
            "697/781 [=========================>....] - ETA: 1:48 - loss: 7.8191 - acc: 0.0048 - LRFinder: val_loss: 18.2645 - lr = 2.67349661 \n",
            "698/781 [=========================>....] - ETA: 1:47 - loss: 7.8341 - acc: 0.0048 - LRFinder: val_loss: 18.3763 - lr = 2.67733782 \n",
            "699/781 [=========================>....] - ETA: 1:45 - loss: 7.8492 - acc: 0.0048 - LRFinder: val_loss: 18.4190 - lr = 2.68117904 \n",
            "700/781 [=========================>....] - ETA: 1:44 - loss: 7.8643 - acc: 0.0048 - LRFinder: val_loss: 18.5300 - lr = 2.68502026 \n",
            "701/781 [=========================>....] - ETA: 1:43 - loss: 7.8795 - acc: 0.0048 - LRFinder: val_loss: 18.6342 - lr = 2.68886147 \n",
            "702/781 [=========================>....] - ETA: 1:41 - loss: 7.8948 - acc: 0.0048 - LRFinder: val_loss: 18.7221 - lr = 2.69270269 \n",
            "703/781 [==========================>...] - ETA: 1:40 - loss: 7.9102 - acc: 0.0048 - LRFinder: val_loss: 18.8184 - lr = 2.69654391 \n",
            "704/781 [==========================>...] - ETA: 1:39 - loss: 7.9257 - acc: 0.0048 - LRFinder: val_loss: 18.9243 - lr = 2.70038512 \n",
            "705/781 [==========================>...] - ETA: 1:37 - loss: 7.9413 - acc: 0.0048 - LRFinder: val_loss: 19.0597 - lr = 2.70422634 \n",
            "706/781 [==========================>...] - ETA: 1:36 - loss: 7.9571 - acc: 0.0048 - LRFinder: val_loss: 19.1369 - lr = 2.70806755 \n",
            "707/781 [==========================>...] - ETA: 1:35 - loss: 7.9729 - acc: 0.0048 - LRFinder: val_loss: 19.2273 - lr = 2.71190877 \n",
            "708/781 [==========================>...] - ETA: 1:34 - loss: 7.9888 - acc: 0.0048 - LRFinder: val_loss: 19.3260 - lr = 2.71574999 \n",
            "709/781 [==========================>...] - ETA: 1:32 - loss: 8.0048 - acc: 0.0048 - LRFinder: val_loss: 19.4073 - lr = 2.71959120 \n",
            "710/781 [==========================>...] - ETA: 1:31 - loss: 8.0208 - acc: 0.0048 - LRFinder: val_loss: 19.4709 - lr = 2.72343242 \n",
            "711/781 [==========================>...] - ETA: 1:30 - loss: 8.0369 - acc: 0.0048 - LRFinder: val_loss: 19.5719 - lr = 2.72727364 \n",
            "712/781 [==========================>...] - ETA: 1:28 - loss: 8.0531 - acc: 0.0048 - LRFinder: val_loss: 19.6825 - lr = 2.73111485 \n",
            "713/781 [==========================>...] - ETA: 1:27 - loss: 8.0694 - acc: 0.0048 - LRFinder: val_loss: 19.7622 - lr = 2.73495607 \n",
            "714/781 [==========================>...] - ETA: 1:26 - loss: 8.0858 - acc: 0.0048 - LRFinder: val_loss: 19.8679 - lr = 2.73879729 \n",
            "715/781 [==========================>...] - ETA: 1:25 - loss: 8.1023 - acc: 0.0048 - LRFinder: val_loss: 19.9392 - lr = 2.74263850 \n",
            "716/781 [==========================>...] - ETA: 1:23 - loss: 8.1188 - acc: 0.0048 - LRFinder: val_loss: 20.0967 - lr = 2.74647972 \n",
            "717/781 [==========================>...] - ETA: 1:22 - loss: 8.1355 - acc: 0.0048 - LRFinder: val_loss: 20.1512 - lr = 2.75032093 \n",
            "718/781 [==========================>...] - ETA: 1:21 - loss: 8.1523 - acc: 0.0048 - LRFinder: val_loss: 20.2605 - lr = 2.75416215 \n",
            "719/781 [==========================>...] - ETA: 1:19 - loss: 8.1691 - acc: 0.0048 - LRFinder: val_loss: 20.3378 - lr = 2.75800337 \n",
            "720/781 [==========================>...] - ETA: 1:18 - loss: 8.1860 - acc: 0.0048 - LRFinder: val_loss: 20.4366 - lr = 2.76184458 \n",
            "721/781 [==========================>...] - ETA: 1:17 - loss: 8.2030 - acc: 0.0048 - LRFinder: val_loss: 20.5372 - lr = 2.76568580 \n",
            "722/781 [==========================>...] - ETA: 1:15 - loss: 8.2201 - acc: 0.0048 - LRFinder: val_loss: 20.6142 - lr = 2.76952702 \n",
            "723/781 [==========================>...] - ETA: 1:14 - loss: 8.2372 - acc: 0.0048 - LRFinder: val_loss: 20.7948 - lr = 2.77336823 \n",
            "724/781 [==========================>...] - ETA: 1:13 - loss: 8.2546 - acc: 0.0048 - LRFinder: val_loss: 20.8550 - lr = 2.77720945 \n",
            "725/781 [==========================>...] - ETA: 1:12 - loss: 8.2719 - acc: 0.0048 - LRFinder: val_loss: 21.0011 - lr = 2.78105067 \n",
            "726/781 [==========================>...] - ETA: 1:10 - loss: 8.2895 - acc: 0.0048 - LRFinder: val_loss: 21.1027 - lr = 2.78489188 \n",
            "727/781 [==========================>...] - ETA: 1:09 - loss: 8.3071 - acc: 0.0048 - LRFinder: val_loss: 21.2645 - lr = 2.78873310 \n",
            "728/781 [==========================>...] - ETA: 1:08 - loss: 8.3249 - acc: 0.0048 - LRFinder: val_loss: 21.2980 - lr = 2.79257431 \n",
            "729/781 [===========================>..] - ETA: 1:06 - loss: 8.3427 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "730/781 [===========================>..] - ETA: 1:05 - loss: 8.3606 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "731/781 [===========================>..] - ETA: 1:04 - loss: 8.3786 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "732/781 [===========================>..] - ETA: 1:03 - loss: 8.3964 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "733/781 [===========================>..] - ETA: 1:01 - loss: 8.4142 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "734/781 [===========================>..] - ETA: 1:00 - loss: 8.4319 - acc: 0.0048 - LRFinder: val_loss: 21.3533 - lr = 2.81562161 \n",
            "735/781 [===========================>..] - ETA: 59s - loss: 8.4495 - acc: 0.0048  - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "736/781 [===========================>..] - ETA: 57s - loss: 8.4674 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "737/781 [===========================>..] - ETA: 56s - loss: 8.4853 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "738/781 [===========================>..] - ETA: 55s - loss: 8.5033 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "739/781 [===========================>..] - ETA: 54s - loss: 8.5213 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "740/781 [===========================>..] - ETA: 52s - loss: 8.5392 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "741/781 [===========================>..] - ETA: 51s - loss: 8.5571 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "742/781 [===========================>..] - ETA: 50s - loss: 8.5748 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "743/781 [===========================>..] - ETA: 48s - loss: 8.5924 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "744/781 [===========================>..] - ETA: 47s - loss: 8.6098 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "745/781 [===========================>..] - ETA: 46s - loss: 8.6271 - acc: 0.0048 - LRFinder: val_loss: 21.3228 - lr = 2.85787499 \n",
            "746/781 [===========================>..] - ETA: 45s - loss: 8.6441 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "747/781 [===========================>..] - ETA: 43s - loss: 8.6620 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "748/781 [===========================>..] - ETA: 42s - loss: 8.6799 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "749/781 [===========================>..] - ETA: 41s - loss: 8.6980 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "750/781 [===========================>..] - ETA: 39s - loss: 8.7162 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "751/781 [===========================>..] - ETA: 38s - loss: 8.7344 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "752/781 [===========================>..] - ETA: 37s - loss: 8.7527 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "753/781 [===========================>..] - ETA: 36s - loss: 8.7708 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "754/781 [===========================>..] - ETA: 34s - loss: 8.7890 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "755/781 [============================>.] - ETA: 33s - loss: 8.8069 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "756/781 [============================>.] - ETA: 32s - loss: 8.8248 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "757/781 [============================>.] - ETA: 30s - loss: 8.8425 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "758/781 [============================>.] - ETA: 29s - loss: 8.8600 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "759/781 [============================>.] - ETA: 28s - loss: 8.8772 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "760/781 [============================>.] - ETA: 26s - loss: 8.8943 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "761/781 [============================>.] - ETA: 25s - loss: 8.9112 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "762/781 [============================>.] - ETA: 24s - loss: 8.9278 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "763/781 [============================>.] - ETA: 23s - loss: 8.9442 - acc: 0.0048 - LRFinder: val_loss: 21.2903 - lr = 2.92701689 \n",
            "764/781 [============================>.] - ETA: 21s - loss: 8.9603 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "765/781 [============================>.] - ETA: 20s - loss: 8.9778 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "766/781 [============================>.] - ETA: 19s - loss: 8.9955 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "767/781 [============================>.] - ETA: 17s - loss: 9.0138 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "768/781 [============================>.] - ETA: 16s - loss: 9.0324 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "769/781 [============================>.] - ETA: 15s - loss: 9.0511 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "770/781 [============================>.] - ETA: 14s - loss: 9.0701 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "771/781 [============================>.] - ETA: 12s - loss: 9.0892 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "772/781 [============================>.] - ETA: 11s - loss: 9.1083 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "773/781 [============================>.] - ETA: 10s - loss: 9.1274 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "774/781 [============================>.] - ETA: 8s - loss: 9.1465 - acc: 0.0049  - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "775/781 [============================>.] - ETA: 7s - loss: 9.1655 - acc: 0.0048 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "776/781 [============================>.] - ETA: 6s - loss: 9.1843 - acc: 0.0049 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "777/781 [============================>.] - ETA: 5s - loss: 9.2030 - acc: 0.0049 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "778/781 [============================>.] - ETA: 3s - loss: 9.2216 - acc: 0.0049 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "779/781 [============================>.] - ETA: 2s - loss: 9.2399 - acc: 0.0049 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "780/781 [============================>.] - ETA: 1s - loss: 9.2582 - acc: 0.0049 - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            " - LRFinder: Skipping iteration since loss is 4 times as large as best loss (5.3435)\n",
            "781/781 [==============================] - 1021s 1s/step - loss: 9.2760 - acc: 0.0049 - val_loss: 23.2620 - val_acc: 0.0050\n",
            "\tLR Finder : Saved the losses and learning rate values in path : {/content/gdrive/My Drive/tinyimagenet-model/lr/}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07f345f0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRUOP58IHPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jduw8tcO7Dzi",
        "colab_type": "code",
        "outputId": "d039a834-6a49-4c5e-b924-3eed7b50e840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "lr_finder.plot_schedule(clip_beginning=25, clip_endding=500)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAESCAYAAAAYMKWkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPyYhL4QQfuz/AejPOiMcjiRYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvXI3jWRII4G",
        "colab_type": "code",
        "outputId": "439ffc29-f3f0-4d68-99e4-1dcd46611a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4477
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 256\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64))\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_1_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.2, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "print(model.summary())\n",
        "model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 1 3456        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 1 512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None, 1 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 1 147456      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 1 512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, None, 1 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 1 147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 1 512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, None, 1 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 1 147456      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 1 147456      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 1 512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 1 147456      activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 1 512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 1 147456      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 2 0           conv2d_7[0][0]                   \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 2 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 2 1024        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, None, None, 2 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 2 589824      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 2 1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, None, None, 2 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 2 589824      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, None, 2 1024        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, None, None, 2 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 2 589824      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, None, None, 2 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 2 589824      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, None, 5 0           conv2d_11[0][0]                  \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 5 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, None, 5 2048        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, None, None, 5 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 5 2359296     activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, None, 5 2048        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, None, None, 5 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 5 2359296     activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, None, 5 2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, None, None, 5 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, None, None, 5 2359296     activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, None, None, 5 2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, None, None, 5 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, None, None, 5 2359296     activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, None, 1 0           conv2d_15[0][0]                  \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, None, 1 4096        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, None, None, 1 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, None, None, 2 204800      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 200)          0           conv2d_16[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 12,908,928\n",
            "Trainable params: 12,899,200\n",
            "Non-trainable params: 9,728\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "--->params {'epochs': 25, 'steps': 390, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/25\n",
            "390/390 [==============================] - 323s 828ms/step - loss: 4.9929 - acc: 0.0336 - val_loss: 4.7464 - val_acc: 0.0442\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.04417, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 2/25\n",
            "390/390 [==============================] - 314s 805ms/step - loss: 4.7383 - acc: 0.0565 - val_loss: 4.5483 - val_acc: 0.0671\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00002: val_acc improved from 0.04417 to 0.06706, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 3/25\n",
            "390/390 [==============================] - 314s 804ms/step - loss: 4.5722 - acc: 0.0745 - val_loss: 4.4432 - val_acc: 0.0900\n",
            " - lr: 0.00037 \n",
            "\n",
            "Epoch 00003: val_acc improved from 0.06706 to 0.08995, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 4/25\n",
            "390/390 [==============================] - 316s 809ms/step - loss: 4.4508 - acc: 0.0891 - val_loss: 4.4434 - val_acc: 0.0906\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00004: val_acc improved from 0.08995 to 0.09056, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 5/25\n",
            "390/390 [==============================] - 315s 808ms/step - loss: 4.3466 - acc: 0.1040 - val_loss: 4.2337 - val_acc: 0.1205\n",
            " - lr: 0.00055 \n",
            "\n",
            "Epoch 00005: val_acc improved from 0.09056 to 0.12054, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 6/25\n",
            "390/390 [==============================] - 314s 806ms/step - loss: 4.2669 - acc: 0.1163 - val_loss: 3.9829 - val_acc: 0.1505\n",
            " - lr: 0.00064 \n",
            "\n",
            "Epoch 00006: val_acc improved from 0.12054 to 0.15053, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 7/25\n",
            "390/390 [==============================] - 315s 808ms/step - loss: 4.2020 - acc: 0.1256 - val_loss: 4.0631 - val_acc: 0.1473\n",
            " - lr: 0.00073 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.15053\n",
            "Epoch 8/25\n",
            "390/390 [==============================] - 315s 808ms/step - loss: 4.1325 - acc: 0.1368 - val_loss: 3.8239 - val_acc: 0.1747\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00008: val_acc improved from 0.15053 to 0.17474, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 9/25\n",
            "390/390 [==============================] - 315s 807ms/step - loss: 4.0857 - acc: 0.1458 - val_loss: 3.8408 - val_acc: 0.1807\n",
            " - lr: 0.00091 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.17474 to 0.18071, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 10/25\n",
            "390/390 [==============================] - 315s 809ms/step - loss: 4.0383 - acc: 0.1526 - val_loss: 3.5923 - val_acc: 0.2178\n",
            " - lr: 0.00100 \n",
            "\n",
            "Epoch 00010: val_acc improved from 0.18071 to 0.21779, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 11/25\n",
            "390/390 [==============================] - 316s 811ms/step - loss: 3.9705 - acc: 0.1647 - val_loss: 3.5132 - val_acc: 0.2468\n",
            " - lr: 0.00091 \n",
            "\n",
            "Epoch 00011: val_acc improved from 0.21779 to 0.24676, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 12/25\n",
            "390/390 [==============================] - 315s 809ms/step - loss: 3.8831 - acc: 0.1792 - val_loss: 3.3636 - val_acc: 0.2716\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.24676 to 0.27158, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 13/25\n",
            "390/390 [==============================] - 315s 807ms/step - loss: 3.8164 - acc: 0.1911 - val_loss: 3.2658 - val_acc: 0.2984\n",
            " - lr: 0.00073 \n",
            "\n",
            "Epoch 00013: val_acc improved from 0.27158 to 0.29842, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 14/25\n",
            "390/390 [==============================] - 313s 803ms/step - loss: 3.7524 - acc: 0.2028 - val_loss: 3.2278 - val_acc: 0.3006\n",
            " - lr: 0.00064 \n",
            "\n",
            "Epoch 00014: val_acc improved from 0.29842 to 0.30065, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 15/25\n",
            "390/390 [==============================] - 313s 802ms/step - loss: 3.6893 - acc: 0.2147 - val_loss: 3.1259 - val_acc: 0.3355\n",
            " - lr: 0.00055 \n",
            "\n",
            "Epoch 00015: val_acc improved from 0.30065 to 0.33549, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 16/25\n",
            "390/390 [==============================] - 312s 800ms/step - loss: 3.6280 - acc: 0.2255 - val_loss: 3.0545 - val_acc: 0.3459\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.33549 to 0.34593, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 17/25\n",
            "390/390 [==============================] - 313s 802ms/step - loss: 3.5782 - acc: 0.2348 - val_loss: 2.9595 - val_acc: 0.3718\n",
            " - lr: 0.00037 \n",
            "\n",
            "Epoch 00017: val_acc improved from 0.34593 to 0.37176, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 18/25\n",
            "390/390 [==============================] - 314s 804ms/step - loss: 3.5278 - acc: 0.2444 - val_loss: 2.8779 - val_acc: 0.3897\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.37176 to 0.38969, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 19/25\n",
            "390/390 [==============================] - 314s 804ms/step - loss: 3.4841 - acc: 0.2528 - val_loss: 2.8398 - val_acc: 0.4031\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00019: val_acc improved from 0.38969 to 0.40306, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 20/25\n",
            "390/390 [==============================] - 314s 805ms/step - loss: 3.4416 - acc: 0.2610 - val_loss: 2.7968 - val_acc: 0.4092\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00020: val_acc improved from 0.40306 to 0.40924, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 21/25\n",
            "390/390 [==============================] - 314s 806ms/step - loss: 3.4119 - acc: 0.2668 - val_loss: 2.7724 - val_acc: 0.4239\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00021: val_acc improved from 0.40924 to 0.42393, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 22/25\n",
            "390/390 [==============================] - 315s 807ms/step - loss: 3.3967 - acc: 0.2701 - val_loss: 2.7493 - val_acc: 0.4279\n",
            " - lr: 0.00006 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.42393 to 0.42788, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 23/25\n",
            "390/390 [==============================] - 314s 804ms/step - loss: 3.3770 - acc: 0.2711 - val_loss: 2.7261 - val_acc: 0.4247\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.42788\n",
            "Epoch 24/25\n",
            "390/390 [==============================] - 315s 807ms/step - loss: 3.3711 - acc: 0.2746 - val_loss: 2.7445 - val_acc: 0.4285\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00024: val_acc improved from 0.42788 to 0.42848, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5\n",
            "Epoch 25/25\n",
            "390/390 [==============================] - 315s 809ms/step - loss: 3.3607 - acc: 0.2770 - val_loss: 2.7308 - val_acc: 0.4276\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.42848\n",
            "LR Range :  1.0507692e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEWCAYAAAC9qEq5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8FPX9x/HXJwnhJlwhAQImQDiS\nQFAj4n2gEi9QEcRWa3/V2kNbWxRFvG9RQa3Vqq1aW1slICgegAfeVTAogSRc4b5Cwn0nJPn8/pjB\nxjQJS8ju7PF5Ph77YHZ25rufyQY+zLHzFlXFGGOMCYQorwswxhgTOazpGGOMCRhrOsYYYwLGmo4x\nxpiAsaZjjDEmYKzpGGOMCRhrOsYYYwLGmo4xASYiPxGRXBHZIyKbRGSmiJwqIveKyGt1rLNaRPa7\n6xSLyN9FpFWgazfmaFnTMSaARGQM8BTwMJAAdAeeA4b7sPrFqtoKGAgcC9zurzqN8RdrOsYEiIjE\nAfcDN6jqNFXdq6oHVfUdVR3r6ziqWgzMxmk+xoQUazrGBM5JQDNg+tEMIiJJwPlAUWMUZUwgWdMx\nJnA6AFtUtaKB678lIruBdUAJcE+jVWZMgFjTMSZwtgIdRSSmgetfoqqtgTOBvkDHxirMmECxpmNM\n4HwNlAGXHM0gqvoZ8HfgiUaoyZiAauj/uIwxR0hVd4rI3cCzIlIBfAAcBM4BzgL2AVEi0uzHq2lZ\nLcM9BawWkUxVzfN37cY0FtvTMSaAVHUiMAa4EyjFOT9zI/CWu8iVwP5qjxV1jFMK/AO4288lG9Oo\nxELcjDHGBIrt6RhjjAkYazrGGGMCxpqOMcaYgLGmY4wxJmAi+pLpjh07anJystdlGGNMSJk/f/4W\nVY1vyLoR3XSSk5PJzc31ugxjjAkpIrKmoeva4TVjjDEBY03HGGNMwFjTMcYYEzDWdIwxxgSMNR1j\njDEB49emIyLZIrJURIpEZFwtrzcVkcnu63NFJLnaa7e785eKyNBq818WkRIRya8xVnsR+VBElrt/\ntvPnthljjDlyfms6IhINPIsTq5sGXCkiaTUWuxbYrqq9gCeBCe66acBoIB3IBp5zxwMnRyS7lrcc\nB3ysqqnAx+5zY4wxQcSfezqDgCJVXamq5cAbwPAaywwHXnWnpwJDRETc+W+oapmqrsLJgh8EoKqf\nA9tqeb/qY73KUQZlGTMrv5g1W/d6XYYxYcWfTacrTlbIIevdebUu4+bG78TJkfdl3ZoSVHWTO10M\nJNS2kIhcLyK5IpJbWlrqy3aYCPTJkhJ+/dp8Lnj6C/aXV3pdjjFhIywvJFAnJKjWoCBVfVFVs1Q1\nKz6+QXdxMGFux75ybntzIQB7yyuZMGuJxxUZEz782XQ2AN2qPU9y59W6jIjEAHHAVh/XrWmziHR2\nx+oMlDS4chPR7nq7gG17y3n3d6fy85OT+ft/VvNV0RavyzImLPiz6XwLpIpIiojE4lwYMKPGMjOA\na9zpy4E57l7KDGC0e3VbCpAKzDvM+1Uf6xrg7UbYBhNh3l24kXfyNnLTkFQyusZxW3ZfesS3ZOyU\nPHYdOOh1ecaEPL81HfcczY3AbGAxkKOqBSJyv4gMcxd7CeggIkU4ufHj3HULgBygEJgF3KCqlQAi\n8jrwNdBHRNaLyLXuWI8C54rIcuAc97kxPivZdYA738ons1tbfnNmTwCax0YzadRANu8u474ZhR5X\naEzoE2fHIjJlZWWp3WXaAKgq176ay1dFW3jv96fRq1OrH70+8YOlPDOniBeuPp6h6YkeVWlMcBCR\n+aqa1ZB1w/JCAmOOVE7uOuYsKeG27L7/03AAfnd2Kuld2jB+2iK27CnzoEJjwoM1HRPx1m3bx/3v\nFHJSjw78/OTkWpeJjYli0qiB7D5QwR3TFxHJRwiMORrWdExEq6pSbpmSh4jw+MgBREVJncv2SWzN\nzef1ZnbBZqZ/f7iLKY0xtbGmYyLaK/9ZzdxV27j7ojSS2rU47PLXndaDE5Lbcc+MAjbu2B+ACo0J\nL9Z0TMQqKtnDY7OWMKRvJ0ZmJfm0TnSU8MTITCqrlFunLqSqyg6zGXMkrOmYiFRRWcXNOQtoERvN\nIyP649zyzzfHdGjJ+Av68WXRFl6b2+CoeGMikjUdE5Ge+3QFeet38uAl/enUutkRr//TE7tzeu94\nHn5/Mau22E1BjfGVNR0TcfI37ORPHy9nWGYXLhzQuUFjiAiPjRhAbHQUN+csoNIOsxnjE2s6JqIc\nOFjJmJwFtG8Zy/3D049qrMS4ZjxwSQbfrd3BC5+vaKQKjQlv1nRMRHnyw2Us27yHCZcPoG2L2KMe\nb1hmFy7on8iTHy6jcOOuRqjQmPBmTcdEjG9Xb+PFL1Zy5aDunNWnU6OMKSI8eEl/4prHMiZnAWUV\nlr1jTH2s6ZiIsLesgptz8khq15w7LuzXqGO3bxnLhBH9WVK8m6c/Wt6oYxsTbqzpmIjw8PuLWbd9\nHxNHDqRV05hGH39IvwRGZSXx/GcrmL9me6OPb0y4sKZjwt5ny0r519y1XHdqCoNS2vvtfe66KI3O\ncc25OWcB+8or/PY+xoQyazomrO3cd5Bbp+aR2qkVN5/Xx6/v1bpZE54Ymcnqrft4dKZFXBtTG2s6\nJqzdMyOfrXvKmTRqIM2aRPv9/U7q2YFfnJLCP75ew5fLLeLamJqs6Ziw9f6iTby1YCM3nt2L/klx\nAXvfW7P70DO+JWOn5rFzv0VcG1OdNR0Tlkp2H+CO6Yvo3zWOG87qFdD3btbEibgu2V3Gfe8UBPS9\njQl21nRM2FFVxk9bxN7ySiaNyqRJdOB/zTO7teWGM3sy7bsNzMovDvj7GxOsrOmYsDNl/no+WlzC\nrUP7kJrQ2rM6bjw7lYyubbhjukVcG3OINR0TVtZvd6KnB6W05xenpHhayw8R12UVjJ9mEdfGgDUd\nE0aqqpSxUxaiqkwcmVlv9HSg9E5ozS3n9eaDws1M+84iro2xpmPCxqtfr+brlVu586I0urU/fPR0\noFx7ag8GJbfn3hkFbLCIaxPhrOmYsLCidA+PzlzCmX3iGX1CN6/L+ZEfIq5VuXVqnkVcm4hmTceE\nvIrKKsbk5NGsSTQTRgw4oujpQOneoQV3XpjGV0Vb+ec3FnFtIpc1HRPynv9sBXnrdvDAJRkktDny\n6OlAuXJQN87sE88jMxezsnSP1+UY4wlrOiakFWzcydMfL+fCAZ0ZltnF63LqJSJMGDGApjHR3Dwl\nj4rKKq9LMibgrOmYkFVWUcmYyXm0bRHLg8MzvC7HJwltnIjr79fu4IXPV3pdjjEBZ03HhKwnP1zO\n0s27mTCiP+1aHn30dKAMy+zChQM689RHFnFtIo81HROS5q/Zxoufr2D0Cd04u2+C1+UcsQeHZ9C2\nhUVcm8hjTceEnH3lFYzJyaNL2+bceVGa1+U0SLtqEddPWcS1iSB+bToiki0iS0WkSETG1fJ6UxGZ\n7L4+V0SSq712uzt/qYgMPdyYIjJERL4TkQUi8qWIBPbWwiZgHnl/CWu37eOJkZl+iZ4OlLP7JnBF\nVjde+GwF89ds87ocYwLCb01HRKKBZ4HzgTTgShGp+d/Sa4HtqtoLeBKY4K6bBowG0oFs4DkRiT7M\nmH8BfqqqA4F/A3f6a9uMd75YXso/v1nDL05JYXCPDl6Xc9TuvKgfXdo2Z0xOnkVcm4jgzz2dQUCR\nqq5U1XLgDWB4jWWGA6+601OBIeJ8s2848IaqlqnqKqDIHa++MRVo407HARv9tF3GIzv3H2TslIX0\n6tSKsUP9Gz0dKIcirtdu28cj71vEtQl//mw6XYF11Z6vd+fVuoyqVgA7gQ71rFvfmNcB74vIeuBq\n4NHaihKR60UkV0RyS0tLG7BZxiv3zSigdE8Zk0ZlBiR6OlAG93Airv/5zRq+WG6/kya8hdOFBH8E\nLlDVJOAVYFJtC6nqi6qapapZ8fHxAS3QNNys/GKmfb+BG87qxYCktl6X0+jGDu3j7MFNWcjOfRZx\nbcKXP5vOBqD6nReT3Hm1LiMiMTiHxbbWs26t80UkHshU1bnu/MnAyY2zGcZrW/aUccf0RWR0bcPv\nzg7P60OciOtMSveUca9FXJsw5s+m8y2QKiIpIhKLc2HAjBrLzACucacvB+aok3Q1AxjtXt2WAqQC\n8+oZczsQJyK93bHOBRb7cdtMgByKnt5dVsGkUQM9iZ4OlAFJbbnxrF5M/34Ds/I3eV2OMX7ht+tN\nVbVCRG4EZgPRwMuqWiAi9wO5qjoDeAn4p4gUAdtwmgjucjlAIVAB3KCqlQC1jenO/yXwpohU4TSh\nX/hr20zgTPtuAx8Ubmb8BX3p7WH0dKDceHYv5iwpYfz0fI4/pj3xrZt6XZIxjUoiOUI3KytLc3Nz\nvS7D1GHjjv0MffJz+nVuw+vXDyY6CJJAA2H55t1c+MyXnNE7nhevPj4ooxpMZBOR+aqa1ZB1w/dY\nhQlpVVXKrVMXUqnKEyMzI6bhAKQmtObWoX34sHAzU+ev97ocYxqVNR0TlF6bu4Yvi7Zwx4X96N4h\neKKnA+UXp6QwKKU9979TaBHXJqxY0zFBZ9WWvTz8/mLO6B3PTwZ197ocT0RFCRNHZlKlytgpFnFt\nwoc1HRNUKquUm3MWEBsdFbTR04HSrX0L7rwojf+s2Mo/vl7tdTnGNAprOiaovPD5Cr5b60RPJ8YF\nb/R0oIw+wYm4fnTWElZYxLUJA9Z0TNBYvGkXT364jAv6JwZ99HSgiAiPjRhAsybRjMmxiGsT+qzp\nmKBQXlHFmJw84prH8uAl/SP6sFpNndo044HhGeSt28Hzn63wuhxjjoo1HRMUnv54GYs37eLRy/rT\nPoSipwPl4swuXDSgM09/vJyCjTu9LseYBrOmYzz33drt/OXTFYw8Polz0kIvejpQHjgUcT05zyKu\nTciypmM8tb+8klty8ugc15y7Lw7N6OlAadcylsdGDGDp5t1M+nCZ1+UY0yDWdIynJsxawsote3l8\n5ABaN2vidTlB76y+nbhyUDde/Hwluast4tqEHms6xjNfFW3h7/9Zzc9PTubknh29Lidk3HFhGknt\nmnPzlDz2llnEtQkt1nSMJ3YdOMjYKXn06NiS27L7el1OSGnVNIYnLncjrmdagocJLdZ0jCfuf6eQ\n4l0HmDgqk+ax4RM9HSgn9ujAdaem8No3a/lsmUVcm9BhTccE3AcFxUydv57fntmLY7u387qckHXz\neX1I7dSKW6fmWcS1CRnWdExAbd1Txvjpi0jr3IbfD0n1upyQ5kRcD2TrnnLumZHvdTnG+MSajgkY\nVeWO6fns2l/BpCsyiY2xX7+j1T8pjhvP7sVbCzYyc5FFXJvgZ3/rTcC8tWADswqK+eO5vemb2Mbr\ncsLGDWf1on/XOMZPX0TJ7gNel2NMvazpmIDYtHM/d79dwPHHtOP603t4XU5YaRIdxZNXZLK3vJLx\n0xYRyRH0JvhZ0zF+p+pET1dUKhMjLHo6UHp1ciKuP1pcwhSLuDZBzKemIyKnisj/udPxIpLi37JM\nOHlt7lq+WL6F8Rf0JbljS6/LCVu/OCWFE92I6/Xb93ldjjG1OmzTEZF7gNuA291ZTYDX/FmUCR+r\nt+zl4fcWc1pqR64afIzX5YS1qCjhiZGZqCpjpyy0iGsTlHzZ07kUGAbsBVDVjUBrfxZlwkNllXLL\nlDxiooXHLo/s6OlA6da+BXdfnMbXK7fy9/+s9rocY/6HL02nXJ0zkwogInZ8xPjkr1+sJHfNdu4b\nlk7nuOZelxMxRmV14+y+nZgwawlFJRZxbYKLL00nR0ReANqKyC+Bj4C/+bcsE+qWFO9i0gfLGJqe\nwKXHdvW6nIgiIjx6WX+ax0Zz8xSLuDbB5bBNR1WfAKYCbwJ9gLtV9U/+LsyErvKKKsZMzqN1sxge\nvtSip73QqU0zHrzEibj+y6cWcW2Chy8XEkxQ1Q9Vdayq3qKqH4rIhEAUZ0LTM3OWU7hpFw9f1p8O\nrZp6XU7EumhAF4ZlduHpj5eTv8Eirk1w8OXw2rm1zDu/sQsx4WHBuh089+kKLjuuK0PTE70uJ+Ld\nPzyd9i1jGZOzgAMHLeLaeK/OpiMivxGRRUAfEVlY7bEKWBi4Ek2oOHCwkjE5C0ho3ZR7Lk73uhwD\ntG0Ry4TLB7Bs8x6etIhrEwRi6nnt38BM4BFgXLX5u1XVcnLN/5gwawkrS/fyr+tOJK65RU8Hi7P6\ndOLKQd158YuVnJOWwAnJ7b0uyUSwOvd0VHWnqq5W1StVdQ2wH+ey6VYi0t2XwUUkW0SWikiRiIyr\n5fWmIjLZfX2uiCRXe+12d/5SERl6uDHF8ZCILBORxSLye59+AqZR/GfFFl75ajXXnHQMp/Sy6Olg\nc8eF/ZyI6xyLuDbe8uVCgotFZDmwCvgMWI2zB3S49aKBZ3HO/6QBV4pIWo3FrgW2q2ov4Elggrtu\nGjAaSAeygedEJPowY/4c6Ab0VdV+wBuHq9E0jt0HDjJ2ykJSOrZk3Pn9vC7H1KJV0xgmjhzIuu37\neOh9i7g23vHlQoIHgcHAMlVNAYYA3/iw3iCgSFVXqmo5ThMYXmOZ4cCr7vRUYIg419cOB95Q1TJV\nXQUUuePVN+ZvgPtVtQpAVUt8qNE0ggfeLWTTzv0WPR3kBqW055en9eDfc9fy6VL762G84UvTOaiq\nW4EoEYlS1U+ALB/W6wqsq/Z8vTuv1mVUtQLYCXSoZ936xuwJXCEiuSIyU0RqjaUUkevdZXJLSy1b\n/mh9VLiZnNz1/PqMnhxn0dNBb8y5vemd0Irb3lxoEdfGE740nR0i0gr4HPiXiDyNex+2INMUOKCq\nWcBfgZdrW0hVX1TVLFXNio+PD2iB4Wbb3nLGTVtE38TW3HSORU+HguoR13dbxLXxgC9NZziwD/gj\nMAtYAVzsw3obcM6xHJLkzqt1GRGJAeKArfWsW9+Y64Fp7vR0YIAPNZoGUlXueiufnfvLefKKgTSN\nscNqoSKjaxy/H5LK2ws28t5Ci7g2geXLbXD2qmqVqlao6qvAn3FO7h/Ot0CqiKSISCzOhQEzaiwz\nA7jGnb4cmOPeXHQGMNq9ui0FSAXmHWbMt4Cz3OkzAPtSgh/NyNvIe4s28YdzetOvs0VPh5rfntmT\nzKQ47nzLIq5NYNX35dA27mXLfxaR89xLkm8EVgKjDjewe47mRmA2sBjIUdUCEblfRIa5i70EdBCR\nImAM7veBVLUAyAEKcfaublDVyrrGdMd6FBjhfqH1EeC6I/tRGF9t3nWAu98u4NjubfmVRU+HpJjo\nKCaOGsi+8kpuf9Mirk3gSF2/bCLyNrAd+BrnirVOgAA3qeqCgFXoR1lZWZqbm+t1GSFFVfn5K98y\nd9VWZt50OimWBBrSXvpyFQ+8W8hjIwYw6oRuh1/BGEBE5rvnz49YfXck6KGq/d03+BuwCeiuqrYv\nHsFen7eOz5aVct+wdGs4YeD/Tk7mw8Ji7n+3kJN6dqBb+xZel2TCXH3ndH64nlJVK4H11nAi29qt\n+3jwvUJO6dWBqy16OiwcirgGuGVKnkVcG7+rr+lkisgu97EbGHBoWkR2BapAExwORU9Hi/D45ZlE\nRVlGTrhIateCuy9KY+6qbbxiEdfGz+q791q0qrZxH61VNabatF2uFGFe/nIV81Zv455h6XRpa9HT\n4WZkVhJD+nbiMYu4Nn7my/d0TIRbtnk3j3+wlHPTEhhxnEVPhyMR4ZER/WkRG83NOQss4tr4jTUd\nU6+DlVWMyVlAq6YxPHKZRU+Hs06tm/HQpf3JW7+T5yzi2viJNR1Trz/PKSJ/wy4evjSDjhY9HfYu\n6N+Z4QO78KePl7NovUVcm8ZnTcfUaeH6Hfz5kyIuPbYr2RmdvS7HBMj9wzLo0Moiro1/+JKns7va\nVWyHHutEZLqI2NfRw5QTPZ1HfKum3DvMoqcjSVyLJkwYMYDlJXuYZBHXppH5sqfzFDAWJ0IgCbgF\nJ8r6Deq4k7MJfU/MXkpRyR4eu3yARU9HoDP7dOKnJ3bnr1+sZN4qS6c3jceXpjNMVV9Q1d2quktV\nXwSGqupkwAJUwtA3K7fy0leruGpwd07vbfEPkWr8Bf3o1q4FN09ZwB6LuDaNxJems09ERolIlPsY\nBRy6M4F9fTnM7Cmr4JYpeXRv34LxF1j0dCRr2TSGiaMyWb99Pw+9ZxHXpnH40nR+ClwNlACb3emr\nRKQ5zh2fTRh56L1CNuzYz8SRmbSIre/WfCYSnJDcnutP68Hr89byiUVcm0bgS57OSlW9WFU7qmq8\nO12kqvtV9ctAFGkC45MlJbw+bx3Xn96DrOT2XpdjgsQfz+1Nn4TW3DZ1ITv2lXtdjglxvly9Fi8i\n40XkRRF5+dAjEMWZwNm+t5zb3lxIn4TWjDm3t9flmCDSrEk0E0dlsm1vOXe/XXD4FYyphy+H197G\niZH+CHiv2sOEkbvezmfb3nImjsq06GnzPzK6xnHTkFRm5G3k3YUbvS7HhDBfDtq3UNXb/F6J8cw7\neRt5d+Embj63Nxld47wuxwSp35zZk4+WlHDnW/kMSm5PpzbNvC7JhCBf9nTeFZEL/F6J8UTJrgPc\n9XY+md3a8psze3pdjgliMdFRTByZyf7ySsZNs4hr0zC+NJ2bcBrPfsvTCS+qym1vLmR/eSUTR2YS\nE213RTL169WpFePO78ucJSXk5K7zuhwTgny5eq21qkapanPL0wkvk79dxydLS7ktuy+9OrXyuhwT\nIq45KZmTenTg/ncKWbdtn9flmBBTZ9MRkb7un8fV9ghcicYf1m3bxwPvFnJSjw78/ORkr8sxISQq\nSnh85ABExCKuzRGr70KCMcD1wMRaXlPgbL9UZPyuyo2eFnH+8bDoaXOkktq14O6L07h16kJe/moV\n151m9/41vqmz6ajq9e6fZwWuHBMIL3+1irmrtvHYiAEktWvhdTkmRI08PokPCjbz2OylnNE7ntSE\n1l6XZEKAT2eOReRkEfmJiPzs0MPfhRn/KCrZzWOzlzKkbydGZiV5XY4JYSLCI5f1p1XTGMbk5HHQ\nIq6ND3y5I8E/gSeAU4ET3EeWn+syfuBET+fRMjaaR0ZY9LQ5evGtm/LQJRks2rCTZz8p8rocEwJ8\n+XJoFpCmdlF+yHvukxUsXL+TZ39yHJ1a2xf7TOM4v39nLhnYhT/PKWJI3wT6J9kXjE3dfDm8lg8k\n+rsQ41+L1u/kmTnLGZbZhQsHWPS0aVz3DcugY6umFnFtDsuXptMRKBSR2SIy49DD34WZxuNETy+g\nfctY7h9u0dOm8cW1aMJjlzsR1xM/WOp1OSaI+XJ47V5/F2H8a9KHy1hesoe//98JtG0R63U5Jkyd\n3jueqwZ3529frmJIvwQG9+jgdUkmCNW7pyMi0cC9qvpZzUeA6jNHad6qbfz1i5X85MTunNmnk9fl\nmDA3/oJ+dG/fglum5FnEtalVvU1HVSuBKhGxM4MhaK8bPd2tXQvusOhpEwAtYmOYODKTDTv289B7\nhV6XY4KQL+d09gCLROQlEfnToYcvg4tItogsFZEiERlXy+tNRWSy+/pcEUmu9trt7vylIjL0CMb8\nk4js8aW+cPfQ+4tZt30fT4zMpGVTi542gZGV3J5fnd6T1+et45MlFnFtfsyXpjMNuAv4HJhf7VEv\n99Dcs8D5QBpwpYik1VjsWmC7qvYCngQmuOumAaOBdCAbeE5Eog83pohkAe182Kaw9+nSEv49dy2/\nPK0Hg1IsetoE1h/PTaVvYmtufXMh2/daxLX5L1/uMv1qbQ8fxh4EFKnqSlUtB94AhtdYZjhwaKyp\nwBBxvrE4HHhDVctUdRVQ5I5X55huQ3ocuNWH2sLazn0Hue3NhfROaGXR08YTTWOciOsd+8q56+18\nr8sxQcSXOxKkishUESkUkZWHHj6M3RWoHrix3p1X6zKqWgHsBDrUs259Y94IzFDVTYfZnutFJFdE\ncktLS33YjNBz94x8tu4pZ9KogTRrYtHTxhvpXZyI63cXbuKdPIu4Ng5fDq+9AvwFqADOAv4BvObP\noo6UiHQBRgLPHG5ZVX1RVbNUNSs+Pt7/xQXY+4s28faCjfzu7FSLnjae+/UZPRnYrS13vZ1Pya4D\nXpdjgoAvTae5qn4MiKquUdV7gQt9WG8D0K3a8yR3Xq3LiEgMEAdsrWfduuYfC/QCikRkNdBCRCLu\nRlAluw9wx/RFDEiK47dnWfS08V5MdBSTRmVy4GAlt7250CKujU9Np0xEooDlInKjiFwK+BIz+S2Q\nKiIpIhKLc2FAzTsZzACucacvB+a493ibAYx2r25LAVKBeXWNqarvqWqiqiarajKwz704IWKoKuOn\nLWJveSWTRmXSxKKnTZDoEd+Kcdl9+WRpKW98axHXkc6Xf5luAloAvweOB67iv42iTu45mhuB2cBi\nIEdVC0TkfhEZ5i72EtDB3SsZA4xz1y0AcoBCYBZwg6pW1jWmrxsbzqbMX89Hi0u4dWgfenWyXBMT\nXH52UjIn9+zAg+9axHWkE193d0WkhaqG1W9LVlaW5ubmel3GUVu/fR/ZT31Bepc2vP7LwZYEaoLS\nhh37yX7yc/p1acMb9nsa0kRkvqo2KOLGl6vXThKRQmCJ+zxTRJ5ryJuZxldVpYyd4hwrf2Jkpv1F\nNkGra9vm3DMsnXmrtvHyV6u8Lsd4xJfDa08BQ3FO8KOqecDp/izK+O7Vr1fz9cqt3HVRGt3aW/S0\nCW4jjuvKuWkJPDZ7Kcs27/a6HOMBn842q2rNs38WmBEEVpTu4dGZSzi7byeuOKHb4VcwxmM/jrhe\nYBHXEciXprNORE4GVESaiMgtOCfxjYcq3Ojp5rHRPHqZRU+b0NGxVVMevjSD/A27+POciPtmQ8Tz\npen8GrgB55v/G4CBwG/9WZQ5vOc/W0Heuh08MDyDTm0setqEluyMzlx2bFf+/EkRC9fv8LocE0C+\n3Htti6r+VFUTVLWTql4F/CwAtZk6FGzcydMfL+eiAZ25OLOL1+UY0yD3DEunU+umjMnJs4jrCNLQ\nbxCOadQqjM/KKioZMzmPti1ieWB4htflGNNgcc2diOuikj08PtsiriNFQ5uOnUDwyJMfLmfp5t08\nNmIA7Vpa9LQJbaelxnP14GNU/zePAAAWKUlEQVR4+atVfLNyq9flmABoaNOxGyh5YP6abbz4+QpG\nn9CNs/pa9LQJD7df0JdjLOI6YtTZdERkt4jsquWxG7ATCQG2r7yCMTl5dGnbnDsvqpmFZ0zoahEb\nw8RRA9m4Yz8PvmsR1+Guzqajqq1VtU0tj9aqatnHAfbI+0tYu82Jnm5l0dMmzBx/TDt+dUZP3vh2\nHXOWbPa6HONHdiviEPDF8lL++c0afnFKCoN7dPC6HGP84g/nOBHXt725yCKuw5g1nSC3c/9Bxk5Z\nSK9OrRg7tI/X5RjjN01jopk0aiA79pVzp0Vchy1rOkHuvhkFlO4pY9KoTIueNmEvrUsb/nBOb95b\nuIkZFnEdlqzpBLFZ+ZuY9v0GbjirFwOS2npdjjEB8avTe3Bs97bc9VY+my3iOuxY0wlSpbvLGD89\nn4yubfjd2REVgmoinBNxPZCyikpunWoR1+HGmk4QUlXGT1/EnrIKJo0aaNHTJuKkdGzJ7ef347Nl\npbw+zyKuw4n9axaE3vxuAx8WbuaW83rTO8Gip01kunrwMZzaqyMPvlfI2q1hFVoc0azpBJkNO/Zz\n34wCBiW359pTe3hdjjGeiYoSHrt8ANFRwi1T8qisssNs4cCaThCpqlJunZpHpRs9HW3R0ybCdWnb\nnHsvTmfe6m289OVKr8sxjcCaThD55zdr+KpoK3dc2I/uHSx62hiAy47rynlpCTwxe5lFXIcBazpB\nYmXpHh6ZuZgzesfzk0HdvS7HmKAhIjx8WX9aN7OI63BgTScIVFRWcfOUPGKjo5gwYoBFTxtTQ8dW\nTXn4sv7kb9jFMxZxHdKs6QSBFz5fyfdrd/DAJRkkxln0tDG1GZqeyGXHdeXZT4rIW2cR16HKmo7H\nCjfu4qmPlnFB/0SGWfS0MfW65+JDEdcLLOI6RFnT8VBZRSVjchYQ1zyWBy/pb4fVjDmMuOZNePzy\nTFaU7uWxWRZxHYqs6Xjo6Y+Ws6R4N49e1p/2Fj1tjE9OTe3INSc5Eddfr7CI61BjTccj89ds5/nP\nVjAqK4lz0hK8LseYkDLu/H6kdGzJLVPy2H3goNflmCNgTccD+8oruGVKHp3jmnOXRU8bc8Sax0Yz\ncVQmm3bu5wGLuA4p1nQ8MGHmElZt2cvjIwfQulkTr8sxJiQd170dvz6jJzm56/mo0CKuQ4Vfm46I\nZIvIUhEpEpFxtbzeVEQmu6/PFZHkaq/d7s5fKiJDDzemiPzLnZ8vIi+LSFD+a/5V0RZe/XoN/3dK\nMif37Oh1OcaEtD+c05t+ndswbtoitlnEdUjwW9MRkWjgWeB8IA24UkRqHku6Ftiuqr2AJ4EJ7rpp\nwGggHcgGnhOR6MOM+S+gL9AfaA5c569ta6hdBw4ydkoePeJbclt2X6/LMSbkxcZEMWlUJjv3l3Pn\nW4sseycE+HNPZxBQpKorVbUceAMYXmOZ4cCr7vRUYIg41w0PB95Q1TJVXQUUuePVOaaqvq8uYB6Q\n5Mdta5D7ZhSyeXcZk0YNtOhpYxpJv85t+OO5vXl/UbFFXIcAfzadrkD19KX17rxal1HVCmAn0KGe\ndQ87pntY7Wpg1lFvQSP6oKCYN79bz2/P7MnAbhY9bUxj+tXpPTnOjbgu3mkR18EsHC8keA74XFW/\nqO1FEbleRHJFJLe0tDQgBW3dU8b46YtI79KG352dGpD3NCaSREcJE0cN5GClctubFnEdzPzZdDYA\n3ao9T3Ln1bqMiMQAccDWetatd0wRuQeIB8bUVZSqvqiqWaqaFR8ff4SbdORUlTum57NrvxM9HRsT\njn3eGO+ldGzJ+Av68tmyUv49b63X5Zg6+PNfwG+BVBFJEZFYnAsDZtRYZgZwjTt9OTDHPSczAxjt\nXt2WAqTinKepc0wRuQ4YClypqkFz7/O3FmxgVkExY87rTZ9Ei542xp+uGnwMp6V25KH3FrNm616v\nyzG18FvTcc/R3AjMBhYDOapaICL3i8gwd7GXgA4iUoSzdzLOXbcAyAEKcc7N3KCqlXWN6Y71PJAA\nfC0iC0Tkbn9tm6827dzP3W8XkHVMO355mkVPG+NvIv+NuL45xyKug5FE8rHPrKwszc3N9cvYqsrP\nXp5H7urtzLzpNJI7tvTL+xhj/tf079fzx8l53H5+X351Rk+vywk7IjJfVbMasq6dYPCT1+au5Yvl\nWxh/YT9rOMYE2CUDu5KdnsjED5axtNgiroOJNR0/WL1lLw+/t5jTUjty1YkWPW1MoIkID12aQZvm\nTsR1eUXQnOaNeNZ0GllllXLLlDxiop1jy5aRY4w3OrRqysOX9qdg4y6embPc63KMy5pOI/vrFyvJ\nXbOd+4en0zmuudflGBPRzktPZMRxSTz36QoWWMR1ULCm04iWFO9i0gfLyE5P5JKBNW++YIzxwj3D\n0kiwiOugYU2nkZRXVDFmch5tmsfw0KUZdljNmCDRplkTnhiZycrSvUyYtcTrciKeNZ1G8syc5RRu\n2sXDl/anQ6umXpdjjKnm5F4d+fnJybzy1Wr+s2KL1+VENGs6jWDBuh089+kKRhyXxHnpiV6XY4yp\nxW3ZfenRsSVjpyxkl0Vce8aazlE6cLCSMTkLSGjdlHuGWfS0McHqRxHX71jEtVes6RylCbOWsLJ0\nL4+PzKSNRU8bE9SO7d6O357ZiynzLeLaK9Z0jsJ/Vmzhla9Wc81Jx3BKL4ueNiYU/H5IKmmd2zBu\n2kK27inzupyIY02ngXYfOMjYKQtJ6diScef387ocY4yPYmOimHRFJrv2V3DnW/mWvRNg1nQa6IF3\nC9m0cz8TR2XSPNaip40JJX0T2zDmvN7MzC/m7QUWcR1I1nQa4KPCzeTkrufXZ/TkuO7tvC7HGNMA\nvzytB1nHtOPut/NZUrzL63IihjWdBvjblyvpm9iam86x6GljQpUTcZ1JlUL2U19w3pOfMemDpRRu\n3GWH3PzI8nQakKdz4GAlW/aUkdSuhR+qMsYE0uZdB5i5aBOzCoqZt2obVQrd27cgOyOR7IxEBia1\nJSrK7jBS3dHk6VjT8VOImzEm9GzZU8ZHhZuZVVDMV0VbOFipJLZpxtD0BIZmJDIouT0x0XaAyJpO\nA1nTMcbUZef+g3yypISZ+Zv4bFkpBw5W0b5lLOf2SyA7I5GTe3WgaUxkXkRkTaeBrOkYY3yxr7yC\nz5eVMjO/mDmLS9hdVkHrpjGc3a8T2emJnNEnnhaxMV6XGTBH03Qi56dkjDEN1CI2huyMzmRndKas\nopL/rNjKrEXFfFDoXHLdrEkUZ/SOJzsjkbP7JhDX3O5OUhfb07E9HWNMA1VUVjFv9TZm5xczq6CY\nzbvKaBItnNyzI+dnJHJuWkJY3nXeDq81kDUdY0xjqapSFqzfwez8YmbmF7N22z6iBE5Ibs/5GYkM\nzUgMmzRhazoNZE3HGOMPqsriTbuZVVDMrPxNLNu8B4CB3do6l2KnJ5LcsaXHVTacNZ0GsqZjjAmE\nFaV7mF1QzKz8Yhau3wlA38TWP3wXqE9C65BKG7am00DWdIwxgbZ++z5mF2xmdn4x367ZhiqkdGzJ\n0HSnAWUmxQV9A7Km00DWdIwxXirdXcYHhc4e0NcrtlJRpXSJa8Z56Ymcn5FIVnJ7ooPwbgjWdBrI\nmo4xJljs2FfOx4tLmFVQzOfLSimrqKJDy1jOS08gO6MzJ/XoQGxMcNwNwZpOA1nTMcYEo71lFXy6\ntJRZBcXMWbyZveWVtGkWwzn9nNvxnJ4a72mkijWdBrKmY4wJdgcOVvJV0RZm5Rfz4eLN7Nh3kOZN\nojmrbzxD0xM5u28nWjcL7JdR7Y4ExhgTppo1iWZIvwSG9EvgYGUV81ZtY2b+JmYXbOb9RcXERkdx\nampHstMTOSctgfYtY70uuV62p2N7OsaYEFRVpXy/bjszFzl3Q1i/fT/RUcKJKe3JzkhkaHoiCW2a\n+eW9g/bwmohkA08D0cDfVPXRGq83Bf4BHA9sBa5Q1dXua7cD1wKVwO9VdXZ9Y4pICvAG0AGYD1yt\nquX11WdNxxgTDlSVgo27mOXejqeoxPky6nHd23J+RmeGpifSvUPj5X8FZdMRkWhgGXAusB74FrhS\nVQurLfNbYICq/lpERgOXquoVIpIGvA4MAroAHwG93dVqHVNEcoBpqvqGiDwP5KnqX+qr0ZqOMSYc\nFZXs/qEB5W9worjTOrfhfPfLqL06tTqq7wIFa9M5CbhXVYe6z28HUNVHqi0z213maxGJAYqBeGBc\n9WUPLeeu9j9jAo8CpUCiqlbUfO+6WNMxxoS7ddv2/XA3hPlrt6MKPeJb8vxVx9M7oXWDxgzWCwm6\nAuuqPV8PnFjXMm6z2IlzeKwr8E2Ndbu607WN2QHYoaoVtSz/IyJyPXA9QPfu3Y9si4wxJsR0a9+C\n607rwXWn9aBk1wFmF27mo8LNdG3rzc1HI+7qNVV9EXgRnD0dj8sxxpiA6dSmGVcPPoarBx/jWQ3+\n/HrrBqBbtedJ7rxal3EPr8XhXFBQ17p1zd8KtHXHqOu9jDHGeMyfTedbIFVEUkQkFhgNzKixzAzg\nGnf6cmCOOieZZgCjRaSpe1VaKjCvrjHddT5xx8Ad820/bpsxxpgG8NvhNfcczY3AbJzLm19W1QIR\nuR/IVdUZwEvAP0WkCNiG00Rwl8sBCoEK4AZVrQSobUz3LW8D3hCRB4Hv3bGNMcYEEftyqF29Zowx\nR+Rorl4LjluWGmOMiQjWdIwxxgSMNR1jjDEBY03HGGNMwET0hQQiUgqsaeDqHYEtjVhOqIjU7YbI\n3fZI3W6I3G0/3HYfo6rxDRk4opvO0RCR3IZevRHKInW7IXK3PVK3GyJ32/253XZ4zRhjTMBY0zHG\nGBMw1nQa7kWvC/BIpG43RO62R+p2Q+Ruu9+2287pGGOMCRjb0zHGGBMw1nSMMcYEjDWdBhCRbBFZ\nKiJFIjLO63qOloh0E5FPRKRQRApE5CZ3fnsR+VBElrt/tnPni4j8yd3+hSJyXLWxrnGXXy4i19T1\nnsFERKJF5HsRedd9niIic93tm+zGaOBGbUx2588VkeRqY9zuzl8qIvXGpAcLEWkrIlNFZImILBaR\nkyLhMxeRP7q/5/ki8rqINAvXz1xEXhaREhHJrzav0T5jETleRBa56/xJROSwRamqPY7ggROpsALo\nAcQCeUCa13Ud5TZ1Bo5zp1sDy4A04DFgnDt/HDDBnb4AmAkIMBiY685vD6x0/2znTrfzevt82P4x\nwL+Bd93nOcBod/p54Dfu9G+B593p0cBkdzrN/T1oCqS4vx/RXm+XD9v9KnCdOx0LtA33zxwnxn4V\n0LzaZ/3zcP3MgdOB44D8avMa7TPGyTkb7K4zEzj/sDV5/UMJtQdwEjC72vPbgdu9rquRt/Ft4Fxg\nKdDZndcZWOpOvwBcWW35pe7rVwIvVJv/o+WC8YGTMvsxcDbwrvuXZwsQU/PzxslxOsmdjnGXk5q/\nA9WXC9YHTkrvKtyLiWp+luH6mbtNZ537D2iM+5kPDefPHEiu0XQa5TN2X1tSbf6PlqvrYYfXjtyh\nX9pD1rvzwoJ7+OBYYC6QoKqb3JeKgQR3uq6fQSj+bJ4CbgWq3OcdgB2qWuE+r74NP2yf+/pOd/lQ\n3O4UoBR4xT20+DcRaUmYf+aqugF4AlgLbML5DOcTGZ/5IY31GXd1p2vOr5c1HfMDEWkFvAn8QVV3\nVX9Nnf/KhNX19SJyEVCiqvO9rsUDMTiHXf6iqscCe3EOtfwgTD/zdsBwnKbbBWgJZHtalIe8+Iyt\n6Ry5DUC3as+T3HkhTUSa4DScf6nqNHf2ZhHp7L7eGShx59f1Mwi1n80pwDARWQ28gXOI7WmgrYgc\ninKvvg0/bJ/7ehywldDbbnD+V7peVee6z6fiNKFw/8zPAVapaqmqHgSm4fweRMJnfkhjfcYb3Oma\n8+tlTefIfQukule7xOKcXJzhcU1Hxb3i5CVgsapOqvbSDODQlSrX4JzrOTT/Z+7VLoOBne7u+mzg\nPBFp5/6P8jx3XlBS1dtVNUlVk3E+xzmq+lPgE+Byd7Ga233o53G5u7y680e7VzqlAKk4J1iDlqoW\nA+tEpI87awhQSJh/5jiH1QaLSAv39/7Qdof9Z15No3zG7mu7RGSw+7P8WbWx6ub1Sa5QfOBc5bEM\n54qVO7yupxG251ScXeyFwAL3cQHOseuPgeXAR0B7d3kBnnW3fxGQVW2sXwBF7uP/vN62I/gZnMl/\nr17rgfMPSBEwBWjqzm/mPi9yX+9Rbf073J/HUny4gicYHsBAINf93N/CuTIp7D9z4D5gCZAP/BPn\nCrSw/MyB13HOXR3E2bu9tjE/YyDL/TmuAP5MjQtTanvYbXCMMcYEjB1eM8YYEzDWdIwxxgSMNR1j\njDEBY03HGGNMwFjTMcYYEzDWdExYE5EOIrLAfRSLyIZqz2N9HOOVat9nqWuZG0Tkp41U85ciMlBE\noqSR72IuIr8QkcRqzw+7bcY0Jrtk2kQMEbkX2KOqT9SYLzh/F6pqXTHARORL4Eac7z9sUdW2R7h+\ntKpW1je2qi44+kqNOXK2p2Mikoj0Eic/6F9AAdBZRF4UkVxxslburrbsoT2PGBHZISKPikieiHwt\nIp3cZR4UkT9UW/5REZknTtbKye78liLypvu+U933GlhPmY8Crd29sn+4Y1zjjrtARJ5z94YO1fWU\niCwEBonIfSLyrTiZMc+73zK/AucLoZMP7ekd2jZ37KvEyUbJF5GH3Xn1bfNod9k8EfmkkT8iE6as\n6ZhI1hd4UlXT1Ln78DhVzQIygXNFJK2WdeKAz1Q1E/ga55vatRFVHQSMBQ41sN8BxaqaBjyAczfv\n+owDdqvqQFX9mYhkAJcCJ6vqQJybdo6uVtfnqjpAVb8GnlbVE4D+7mvZqjoZ524TV7hjlv9QrEgS\n8CBwllvXKeLcELW+bb4HGOLOv/Qw22IMYE3HRLYVqppb7fmVIvId8B3QDyeoq6b9qjrTnZ6Pk1VS\nm2m1LHMqzo1FUdU8nD2sI3EOcAKQKyILgDOAnu5r5cD0assOEZF5OEFjZwDphxn7RJz7im1R50aY\n/8YJAIO6t/kr4B8ich32b4nxUczhFzEmbO09NCEiqcBNwCBV3SEir+Hcd6um8mrTldT9d6jMh2WO\nlAAvq+pdP5rp3P14vx66gZZIC5z7YB2nqhtE5EFq3xZf1bXNv8RpVhcB34nIsaq6/Sjex0QA+9+J\nMY42wG6cu+Z2xkmTbGxfAaMARKQ/te9J/UDdUDH57y33PwJGiUhHd34HEeley6rNcULptohIa2BE\ntdd240SS1zQXOMsd89Bhu88Osz09VPUb4C5gO6ETYmY8ZHs6xji+w7nF/RJgDU6DaGzP4ByOKnTf\nqxAnibI+LwELRSTXPa9zH/CRiETh3Dn418DG6iuo6lYRedUdfxNOQznkFeBvIrIfGFRtnfUichfw\nKc4e1Tuq+l61hlebJ8W5rb8AH6hq/mG2xRi7ZNqYQHH/AY9R1QPu4bwPgFT9b0yyMWHP9nSMCZxW\nwMdu8xHgV9ZwTKSxPR1jjDEBYxcSGGOMCRhrOsYYYwLGmo4xxpiAsaZjjDEmYKzpGGOMCZj/B2AO\nGlFaoj2+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGJSxlYItjjE",
        "colab_type": "code",
        "outputId": "b2719de6-c868-41b2-80f3-634eb94d8659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2488
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=48)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_2_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.2, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_1_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "--->params {'epochs': 25, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/25\n",
            "781/781 [==============================] - 463s 592ms/step - loss: 3.0615 - acc: 0.3445 - val_loss: 2.7818 - val_acc: 0.4083\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.40825, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 456s 584ms/step - loss: 3.0606 - acc: 0.3419 - val_loss: 2.7929 - val_acc: 0.4011\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.40825\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 456s 583ms/step - loss: 3.0724 - acc: 0.3360 - val_loss: 2.8310 - val_acc: 0.3903\n",
            " - lr: 0.00037 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.40825\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 455s 582ms/step - loss: 3.0960 - acc: 0.3288 - val_loss: 2.9351 - val_acc: 0.3596\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.40825\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 454s 581ms/step - loss: 3.1152 - acc: 0.3271 - val_loss: 2.9709 - val_acc: 0.3587\n",
            " - lr: 0.00055 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.40825\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 456s 584ms/step - loss: 3.1414 - acc: 0.3210 - val_loss: 2.8628 - val_acc: 0.3824\n",
            " - lr: 0.00064 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.40825\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 455s 583ms/step - loss: 3.1543 - acc: 0.3202 - val_loss: 3.0217 - val_acc: 0.3445\n",
            " - lr: 0.00073 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.40825\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 455s 582ms/step - loss: 3.1791 - acc: 0.3183 - val_loss: 3.0938 - val_acc: 0.3404\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.40825\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 455s 583ms/step - loss: 3.2028 - acc: 0.3160 - val_loss: 3.0365 - val_acc: 0.3530\n",
            " - lr: 0.00091 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.40825\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 454s 582ms/step - loss: 3.2161 - acc: 0.3157 - val_loss: 3.0972 - val_acc: 0.3375\n",
            " - lr: 0.00100 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.40825\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 455s 583ms/step - loss: 3.2028 - acc: 0.3230 - val_loss: 3.0381 - val_acc: 0.3694\n",
            " - lr: 0.00091 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.40825\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 455s 582ms/step - loss: 3.1453 - acc: 0.3385 - val_loss: 2.9029 - val_acc: 0.3962\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.40825\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 455s 582ms/step - loss: 3.0976 - acc: 0.3529 - val_loss: 2.9777 - val_acc: 0.3910\n",
            " - lr: 0.00073 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.40825\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 454s 582ms/step - loss: 3.0415 - acc: 0.3704 - val_loss: 2.9411 - val_acc: 0.3857\n",
            " - lr: 0.00064 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.40825\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 456s 584ms/step - loss: 2.9880 - acc: 0.3831 - val_loss: 2.8150 - val_acc: 0.4375\n",
            " - lr: 0.00055 \n",
            "\n",
            "Epoch 00015: val_acc improved from 0.40825 to 0.43750, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 454s 582ms/step - loss: 2.9453 - acc: 0.3957 - val_loss: 2.6646 - val_acc: 0.4730\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.43750 to 0.47295, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 17/25\n",
            "781/781 [==============================] - 455s 583ms/step - loss: 2.8947 - acc: 0.4089 - val_loss: 2.6473 - val_acc: 0.4786\n",
            " - lr: 0.00037 \n",
            "\n",
            "Epoch 00017: val_acc improved from 0.47295 to 0.47863, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 18/25\n",
            "781/781 [==============================] - 454s 581ms/step - loss: 2.8477 - acc: 0.4216 - val_loss: 2.5472 - val_acc: 0.5011\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.47863 to 0.50111, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 19/25\n",
            "781/781 [==============================] - 454s 582ms/step - loss: 2.7976 - acc: 0.4340 - val_loss: 2.5181 - val_acc: 0.5231\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00019: val_acc improved from 0.50111 to 0.52310, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 20/25\n",
            "781/781 [==============================] - 455s 582ms/step - loss: 2.7495 - acc: 0.4479 - val_loss: 2.5231 - val_acc: 0.5143\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.52310\n",
            "Epoch 21/25\n",
            "781/781 [==============================] - 455s 582ms/step - loss: 2.7245 - acc: 0.4553 - val_loss: 2.4905 - val_acc: 0.5266\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00021: val_acc improved from 0.52310 to 0.52664, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 22/25\n",
            "781/781 [==============================] - 454s 582ms/step - loss: 2.7117 - acc: 0.4593 - val_loss: 2.4677 - val_acc: 0.5270\n",
            " - lr: 0.00006 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.52664 to 0.52705, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 23/25\n",
            "781/781 [==============================] - 455s 583ms/step - loss: 2.6922 - acc: 0.4620 - val_loss: 2.4502 - val_acc: 0.5329\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00023: val_acc improved from 0.52705 to 0.53292, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "Epoch 24/25\n",
            "781/781 [==============================] - 456s 584ms/step - loss: 2.6875 - acc: 0.4644 - val_loss: 2.4678 - val_acc: 0.5305\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.53292\n",
            "Epoch 25/25\n",
            "781/781 [==============================] - 455s 583ms/step - loss: 2.6685 - acc: 0.4699 - val_loss: 2.4615 - val_acc: 0.5339\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00025: val_acc improved from 0.53292 to 0.53393, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5\n",
            "LR Range :  1.0253522e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mswPgTsAhcb",
        "colab_type": "code",
        "outputId": "71c26880-a52e-4cfb-dd9a-bbcd0141e514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2488
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=48)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_3_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.2, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_2_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "--->params {'epochs': 25, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/25\n",
            "781/781 [==============================] - 472s 604ms/step - loss: 2.7115 - acc: 0.4575 - val_loss: 2.5415 - val_acc: 0.5161\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.51613, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 2.7445 - acc: 0.4506 - val_loss: 2.5424 - val_acc: 0.5129\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.51613\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.7812 - acc: 0.4439 - val_loss: 2.6142 - val_acc: 0.4968\n",
            " - lr: 0.00037 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.51613\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.8121 - acc: 0.4380 - val_loss: 2.7121 - val_acc: 0.4719\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.51613\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.8610 - acc: 0.4286 - val_loss: 2.8354 - val_acc: 0.4519\n",
            " - lr: 0.00055 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.51613\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 465s 596ms/step - loss: 2.9043 - acc: 0.4222 - val_loss: 2.7307 - val_acc: 0.4777\n",
            " - lr: 0.00064 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.51613\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.9498 - acc: 0.4156 - val_loss: 2.9010 - val_acc: 0.4369\n",
            " - lr: 0.00073 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.51613\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 3.0081 - acc: 0.4069 - val_loss: 2.9502 - val_acc: 0.4315\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.51613\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 3.0695 - acc: 0.4000 - val_loss: 2.9417 - val_acc: 0.4398\n",
            " - lr: 0.00091 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.51613\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 3.1155 - acc: 0.3949 - val_loss: 3.0823 - val_acc: 0.4177\n",
            " - lr: 0.00100 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.51613\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.1387 - acc: 0.3972 - val_loss: 3.0911 - val_acc: 0.4198\n",
            " - lr: 0.00091 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.51613\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.1230 - acc: 0.4069 - val_loss: 2.9400 - val_acc: 0.4612\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.51613\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.0930 - acc: 0.4186 - val_loss: 2.9552 - val_acc: 0.4683\n",
            " - lr: 0.00073 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.51613\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.0599 - acc: 0.4344 - val_loss: 2.9019 - val_acc: 0.4743\n",
            " - lr: 0.00064 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.51613\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.0260 - acc: 0.4455 - val_loss: 2.9191 - val_acc: 0.4908\n",
            " - lr: 0.00055 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.51613\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 465s 596ms/step - loss: 2.9895 - acc: 0.4578 - val_loss: 2.8183 - val_acc: 0.5174\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.51613 to 0.51742, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 17/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.9524 - acc: 0.4688 - val_loss: 2.8275 - val_acc: 0.5189\n",
            " - lr: 0.00037 \n",
            "\n",
            "Epoch 00017: val_acc improved from 0.51742 to 0.51894, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 18/25\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 2.9122 - acc: 0.4813 - val_loss: 2.7133 - val_acc: 0.5432\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.51894 to 0.54315, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 19/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.8766 - acc: 0.4920 - val_loss: 2.7271 - val_acc: 0.5442\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00019: val_acc improved from 0.54315 to 0.54417, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 20/25\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 2.8341 - acc: 0.5032 - val_loss: 2.7159 - val_acc: 0.5525\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00020: val_acc improved from 0.54417 to 0.55247, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 21/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.8161 - acc: 0.5109 - val_loss: 2.7026 - val_acc: 0.5544\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00021: val_acc improved from 0.55247 to 0.55440, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 22/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.8021 - acc: 0.5133 - val_loss: 2.6760 - val_acc: 0.5645\n",
            " - lr: 0.00006 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.55440 to 0.56453, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 23/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.7858 - acc: 0.5175 - val_loss: 2.6636 - val_acc: 0.5665\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00023: val_acc improved from 0.56453 to 0.56645, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "Epoch 24/25\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 2.7707 - acc: 0.5209 - val_loss: 2.6767 - val_acc: 0.5651\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.56645\n",
            "Epoch 25/25\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 2.7669 - acc: 0.5243 - val_loss: 2.6761 - val_acc: 0.5673\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00025: val_acc improved from 0.56645 to 0.56726, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5\n",
            "LR Range :  1.0253522e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wH9TZZyL8X9",
        "colab_type": "code",
        "outputId": "30c962b5-1620-42d1-9a26-e8e51e298872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3035
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=32)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_4_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.25, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_3_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 30, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 402s 515ms/step - loss: 3.5051 - acc: 0.3598 - val_loss: 2.7589 - val_acc: 0.5438\n",
            " - lr: 0.00018 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.54377, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 385s 493ms/step - loss: 3.4924 - acc: 0.3603 - val_loss: 2.7955 - val_acc: 0.5378\n",
            " - lr: 0.00026 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.54377\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 382s 490ms/step - loss: 3.5279 - acc: 0.3550 - val_loss: 2.8761 - val_acc: 0.5189\n",
            " - lr: 0.00034 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.54377\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 384s 491ms/step - loss: 3.5679 - acc: 0.3491 - val_loss: 2.9164 - val_acc: 0.5175\n",
            " - lr: 0.00042 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.54377\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 382s 490ms/step - loss: 3.6154 - acc: 0.3425 - val_loss: 2.9519 - val_acc: 0.5023\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.54377\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 381s 488ms/step - loss: 3.6694 - acc: 0.3358 - val_loss: 3.1311 - val_acc: 0.4693\n",
            " - lr: 0.00058 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.54377\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 378s 484ms/step - loss: 3.7145 - acc: 0.3303 - val_loss: 3.1452 - val_acc: 0.4609\n",
            " - lr: 0.00066 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.54377\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 383s 490ms/step - loss: 3.7894 - acc: 0.3219 - val_loss: 3.1993 - val_acc: 0.4650\n",
            " - lr: 0.00074 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.54377\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 384s 491ms/step - loss: 3.8413 - acc: 0.3186 - val_loss: 3.3191 - val_acc: 0.4443\n",
            " - lr: 0.00082 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.54377\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 381s 488ms/step - loss: 3.9068 - acc: 0.3100 - val_loss: 3.3174 - val_acc: 0.4468\n",
            " - lr: 0.00090 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.54377\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 380s 487ms/step - loss: 3.9789 - acc: 0.3065 - val_loss: 3.3535 - val_acc: 0.4518\n",
            " - lr: 0.00098 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.54377\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 385s 492ms/step - loss: 4.0300 - acc: 0.3032 - val_loss: 3.5071 - val_acc: 0.4325\n",
            " - lr: 0.00094 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.54377\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 385s 494ms/step - loss: 4.0507 - acc: 0.3076 - val_loss: 3.4358 - val_acc: 0.4636\n",
            " - lr: 0.00086 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.54377\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 381s 488ms/step - loss: 4.0456 - acc: 0.3165 - val_loss: 3.4421 - val_acc: 0.4652\n",
            " - lr: 0.00078 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.54377\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 380s 486ms/step - loss: 4.0401 - acc: 0.3257 - val_loss: 3.3829 - val_acc: 0.4889\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.54377\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 383s 490ms/step - loss: 4.0310 - acc: 0.3333 - val_loss: 3.3840 - val_acc: 0.4981\n",
            " - lr: 0.00062 \n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.54377\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 382s 489ms/step - loss: 4.0202 - acc: 0.3413 - val_loss: 3.3818 - val_acc: 0.5038\n",
            " - lr: 0.00054 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.54377\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 380s 486ms/step - loss: 4.0124 - acc: 0.3478 - val_loss: 3.3368 - val_acc: 0.5250\n",
            " - lr: 0.00046 \n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.54377\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 379s 485ms/step - loss: 3.9993 - acc: 0.3548 - val_loss: 3.3505 - val_acc: 0.5302\n",
            " - lr: 0.00038 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.54377\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 378s 484ms/step - loss: 3.9693 - acc: 0.3659 - val_loss: 3.3630 - val_acc: 0.5276\n",
            " - lr: 0.00030 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.54377\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 376s 482ms/step - loss: 3.9679 - acc: 0.3683 - val_loss: 3.3249 - val_acc: 0.5376\n",
            " - lr: 0.00022 \n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.54377\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 378s 484ms/step - loss: 3.9285 - acc: 0.3803 - val_loss: 3.2966 - val_acc: 0.5474\n",
            " - lr: 0.00014 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.54377 to 0.54741, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 379s 485ms/step - loss: 3.9032 - acc: 0.3837 - val_loss: 3.2763 - val_acc: 0.5529\n",
            " - lr: 0.00009 \n",
            "\n",
            "Epoch 00023: val_acc improved from 0.54741 to 0.55288, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 379s 485ms/step - loss: 3.8950 - acc: 0.3876 - val_loss: 3.2920 - val_acc: 0.5559\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00024: val_acc improved from 0.55288 to 0.55592, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 380s 487ms/step - loss: 3.8858 - acc: 0.3906 - val_loss: 3.2912 - val_acc: 0.5534\n",
            " - lr: 0.00007 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.55592\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 379s 485ms/step - loss: 3.8851 - acc: 0.3919 - val_loss: 3.2632 - val_acc: 0.5633\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00026: val_acc improved from 0.55592 to 0.56331, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 380s 487ms/step - loss: 3.8735 - acc: 0.3945 - val_loss: 3.2769 - val_acc: 0.5594\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.56331\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 379s 486ms/step - loss: 3.8761 - acc: 0.3957 - val_loss: 3.2602 - val_acc: 0.5646\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00028: val_acc improved from 0.56331 to 0.56463, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 378s 484ms/step - loss: 3.8569 - acc: 0.3983 - val_loss: 3.2861 - val_acc: 0.5583\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.56463\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 380s 486ms/step - loss: 3.8636 - acc: 0.3973 - val_loss: 3.2490 - val_acc: 0.5653\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00030: val_acc improved from 0.56463 to 0.56534, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5\n",
            "LR Range :  1.0169e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhi/+H/oVhfD8YAoPgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEBtl2RwsvXM",
        "colab_type": "code",
        "outputId": "fdfe63db-1b1f-40b4-9b79-bbc520533848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2913
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=48)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_5_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.3, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_4_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "--->params {'epochs': 30, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 467s 597ms/step - loss: 3.3799 - acc: 0.5255 - val_loss: 3.2855 - val_acc: 0.5602\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.56020, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 460s 589ms/step - loss: 3.3760 - acc: 0.5221 - val_loss: 3.3185 - val_acc: 0.5493\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.56020\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.3878 - acc: 0.5179 - val_loss: 3.3840 - val_acc: 0.5283\n",
            " - lr: 0.00036 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.56020\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.4023 - acc: 0.5162 - val_loss: 3.3671 - val_acc: 0.5285\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.56020\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.4273 - acc: 0.5089 - val_loss: 3.5075 - val_acc: 0.5076\n",
            " - lr: 0.00053 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.56020\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.4665 - acc: 0.5005 - val_loss: 3.4753 - val_acc: 0.5172\n",
            " - lr: 0.00061 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.56020\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.5077 - acc: 0.4939 - val_loss: 3.5346 - val_acc: 0.5021\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.56020\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 459s 587ms/step - loss: 3.5479 - acc: 0.4881 - val_loss: 3.5667 - val_acc: 0.5006\n",
            " - lr: 0.00079 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.56020\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.5948 - acc: 0.4810 - val_loss: 3.6190 - val_acc: 0.4944\n",
            " - lr: 0.00087 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.56020\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.6489 - acc: 0.4750 - val_loss: 3.7112 - val_acc: 0.4800\n",
            " - lr: 0.00096 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.56020\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 457s 586ms/step - loss: 3.7074 - acc: 0.4666 - val_loss: 3.7731 - val_acc: 0.4726\n",
            " - lr: 0.00096 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.56020\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 459s 587ms/step - loss: 3.7180 - acc: 0.4708 - val_loss: 3.6315 - val_acc: 0.5128\n",
            " - lr: 0.00087 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.56020\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 458s 586ms/step - loss: 3.7017 - acc: 0.4802 - val_loss: 3.8347 - val_acc: 0.4756\n",
            " - lr: 0.00079 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.56020\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.6855 - acc: 0.4918 - val_loss: 3.6472 - val_acc: 0.5198\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.56020\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 458s 586ms/step - loss: 3.6515 - acc: 0.5045 - val_loss: 3.6361 - val_acc: 0.5325\n",
            " - lr: 0.00061 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.56020\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.6403 - acc: 0.5116 - val_loss: 3.6688 - val_acc: 0.5264\n",
            " - lr: 0.00053 \n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.56020\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 458s 586ms/step - loss: 3.6056 - acc: 0.5234 - val_loss: 3.6263 - val_acc: 0.5437\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.56020\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.5812 - acc: 0.5348 - val_loss: 3.5917 - val_acc: 0.5423\n",
            " - lr: 0.00036 \n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.56020\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.5476 - acc: 0.5456 - val_loss: 3.5494 - val_acc: 0.5716\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00019: val_acc improved from 0.56020 to 0.57162, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 460s 589ms/step - loss: 3.5169 - acc: 0.5535 - val_loss: 3.5559 - val_acc: 0.5660\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.57162\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 462s 592ms/step - loss: 3.4890 - acc: 0.5630 - val_loss: 3.5117 - val_acc: 0.5767\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00021: val_acc improved from 0.57162 to 0.57668, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 462s 591ms/step - loss: 3.4594 - acc: 0.5705 - val_loss: 3.4944 - val_acc: 0.5851\n",
            " - lr: 0.00009 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.57668 to 0.58509, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 461s 590ms/step - loss: 3.4526 - acc: 0.5725 - val_loss: 3.4987 - val_acc: 0.5801\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.58509\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 460s 590ms/step - loss: 3.4459 - acc: 0.5765 - val_loss: 3.5073 - val_acc: 0.5785\n",
            " - lr: 0.00007 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.58509\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 458s 586ms/step - loss: 3.4380 - acc: 0.5795 - val_loss: 3.5048 - val_acc: 0.5809\n",
            " - lr: 0.00006 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.58509\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.4329 - acc: 0.5814 - val_loss: 3.4841 - val_acc: 0.5862\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00026: val_acc improved from 0.58509 to 0.58620, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 460s 589ms/step - loss: 3.4266 - acc: 0.5830 - val_loss: 3.4991 - val_acc: 0.5843\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.58620\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.4140 - acc: 0.5845 - val_loss: 3.4777 - val_acc: 0.5882\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00028: val_acc improved from 0.58620 to 0.58823, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 458s 587ms/step - loss: 3.4162 - acc: 0.5850 - val_loss: 3.5098 - val_acc: 0.5818\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.58823\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 459s 588ms/step - loss: 3.4145 - acc: 0.5843 - val_loss: 3.4609 - val_acc: 0.5868\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.58823\n",
            "LR Range :  1.0140825e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqU18Q31PGh1",
        "colab_type": "code",
        "outputId": "40a51f48-39aa-4f53-9a04-872348aaf948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3035
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=48)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_6_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.4, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_5_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 30, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 479s 614ms/step - loss: 3.4432 - acc: 0.5770 - val_loss: 3.5801 - val_acc: 0.5661\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.56611, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 466s 596ms/step - loss: 3.4782 - acc: 0.5692 - val_loss: 3.5947 - val_acc: 0.5624\n",
            " - lr: 0.00030 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.56611\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.5145 - acc: 0.5620 - val_loss: 3.6724 - val_acc: 0.5425\n",
            " - lr: 0.00040 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.56611\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 3.5584 - acc: 0.5517 - val_loss: 3.6556 - val_acc: 0.5432\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.56611\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.6126 - acc: 0.5415 - val_loss: 3.7211 - val_acc: 0.5392\n",
            " - lr: 0.00060 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.56611\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6738 - acc: 0.5297 - val_loss: 3.7875 - val_acc: 0.5288\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.56611\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 3.7360 - acc: 0.5190 - val_loss: 3.8558 - val_acc: 0.5088\n",
            " - lr: 0.00080 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.56611\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 463s 592ms/step - loss: 3.8043 - acc: 0.5099 - val_loss: 3.9206 - val_acc: 0.5064\n",
            " - lr: 0.00090 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.56611\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 462s 591ms/step - loss: 3.8854 - acc: 0.4965 - val_loss: 3.9169 - val_acc: 0.5136\n",
            " - lr: 0.00100 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.56611\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 462s 592ms/step - loss: 3.9228 - acc: 0.4976 - val_loss: 4.3296 - val_acc: 0.4324\n",
            " - lr: 0.00090 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.56611\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.9322 - acc: 0.5002 - val_loss: 4.2320 - val_acc: 0.4560\n",
            " - lr: 0.00080 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.56611\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.8909 - acc: 0.5188 - val_loss: 3.9181 - val_acc: 0.5364\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.56611\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 464s 595ms/step - loss: 3.8755 - acc: 0.5284 - val_loss: 3.9642 - val_acc: 0.5337\n",
            " - lr: 0.00060 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.56611\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.8473 - acc: 0.5377 - val_loss: 3.9351 - val_acc: 0.5498\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.56611\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.8136 - acc: 0.5532 - val_loss: 3.8974 - val_acc: 0.5552\n",
            " - lr: 0.00040 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.56611\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.7774 - acc: 0.5651 - val_loss: 3.8773 - val_acc: 0.5663\n",
            " - lr: 0.00030 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.56611 to 0.56635, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 462s 592ms/step - loss: 3.7427 - acc: 0.5764 - val_loss: 3.8541 - val_acc: 0.5738\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00017: val_acc improved from 0.56635 to 0.57385, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 463s 592ms/step - loss: 3.7143 - acc: 0.5866 - val_loss: 3.7889 - val_acc: 0.5837\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.57385 to 0.58367, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 462s 591ms/step - loss: 3.6917 - acc: 0.5921 - val_loss: 3.8159 - val_acc: 0.5861\n",
            " - lr: 0.00009 \n",
            "\n",
            "Epoch 00019: val_acc improved from 0.58367 to 0.58610, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6859 - acc: 0.5966 - val_loss: 3.8371 - val_acc: 0.5758\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.58610\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.6869 - acc: 0.5967 - val_loss: 3.8202 - val_acc: 0.5797\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.58610\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.6777 - acc: 0.5993 - val_loss: 3.7994 - val_acc: 0.5869\n",
            " - lr: 0.00007 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.58610 to 0.58691, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.6681 - acc: 0.6024 - val_loss: 3.8113 - val_acc: 0.5788\n",
            " - lr: 0.00006 \n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.58691\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6676 - acc: 0.6042 - val_loss: 3.8192 - val_acc: 0.5812\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.58691\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6603 - acc: 0.6051 - val_loss: 3.8170 - val_acc: 0.5817\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.58691\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 465s 595ms/step - loss: 3.6627 - acc: 0.6064 - val_loss: 3.7965 - val_acc: 0.5885\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00026: val_acc improved from 0.58691 to 0.58853, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6587 - acc: 0.6071 - val_loss: 3.8087 - val_acc: 0.5823\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.58853\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.6521 - acc: 0.6077 - val_loss: 3.7991 - val_acc: 0.5906\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00028: val_acc improved from 0.58853 to 0.59056, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6462 - acc: 0.6113 - val_loss: 3.8233 - val_acc: 0.5805\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.59056\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.6449 - acc: 0.6117 - val_loss: 3.7739 - val_acc: 0.5892\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.59056\n",
            "LR Range :  1.0105633e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfbAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLZ-0Xi8lv94",
        "colab_type": "code",
        "outputId": "d9bce1a6-291b-406e-c8c7-ada42da82020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2913
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=48)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_7_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.3, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_6_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "--->params {'epochs': 30, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 470s 602ms/step - loss: 3.6791 - acc: 0.6015 - val_loss: 3.8449 - val_acc: 0.5781\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.57812, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_7_0002.hdf5\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.7148 - acc: 0.5926 - val_loss: 3.8818 - val_acc: 0.5743\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.57812\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 462s 592ms/step - loss: 3.7436 - acc: 0.5876 - val_loss: 3.9083 - val_acc: 0.5731\n",
            " - lr: 0.00036 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.57812\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.7926 - acc: 0.5768 - val_loss: 4.0096 - val_acc: 0.5475\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.57812\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.8338 - acc: 0.5690 - val_loss: 3.9885 - val_acc: 0.5533\n",
            " - lr: 0.00053 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.57812\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 3.8854 - acc: 0.5598 - val_loss: 4.0841 - val_acc: 0.5363\n",
            " - lr: 0.00061 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.57812\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.9397 - acc: 0.5515 - val_loss: 4.1682 - val_acc: 0.5200\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.57812\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.0126 - acc: 0.5379 - val_loss: 4.1664 - val_acc: 0.5326\n",
            " - lr: 0.00079 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.57812\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.0780 - acc: 0.5301 - val_loss: 4.2196 - val_acc: 0.5289\n",
            " - lr: 0.00087 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.57812\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 464s 594ms/step - loss: 4.1541 - acc: 0.5179 - val_loss: 4.5838 - val_acc: 0.4451\n",
            " - lr: 0.00096 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.57812\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.2219 - acc: 0.5098 - val_loss: 4.3313 - val_acc: 0.5129\n",
            " - lr: 0.00096 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.57812\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.2420 - acc: 0.5126 - val_loss: 4.2910 - val_acc: 0.5267\n",
            " - lr: 0.00087 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.57812\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.2358 - acc: 0.5218 - val_loss: 4.3181 - val_acc: 0.5329\n",
            " - lr: 0.00079 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.57812\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.2325 - acc: 0.5314 - val_loss: 4.3946 - val_acc: 0.5182\n",
            " - lr: 0.00070 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.57812\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.2072 - acc: 0.5414 - val_loss: 4.3269 - val_acc: 0.5403\n",
            " - lr: 0.00061 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.57812\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.1793 - acc: 0.5563 - val_loss: 4.2931 - val_acc: 0.5557\n",
            " - lr: 0.00053 \n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.57812\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 462s 592ms/step - loss: 4.1616 - acc: 0.5632 - val_loss: 4.3318 - val_acc: 0.5490\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.57812\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.1271 - acc: 0.5763 - val_loss: 4.2525 - val_acc: 0.5695\n",
            " - lr: 0.00036 \n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.57812\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.1019 - acc: 0.5851 - val_loss: 4.2433 - val_acc: 0.5749\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.57812\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 463s 592ms/step - loss: 4.0711 - acc: 0.5984 - val_loss: 4.2738 - val_acc: 0.5669\n",
            " - lr: 0.00019 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.57812\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 462s 592ms/step - loss: 4.0446 - acc: 0.6056 - val_loss: 4.2193 - val_acc: 0.5796\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00021: val_acc improved from 0.57812 to 0.57962, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_7_0002.hdf5\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.0200 - acc: 0.6152 - val_loss: 4.2037 - val_acc: 0.5941\n",
            " - lr: 0.00009 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.57962 to 0.59410, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_7_0002.hdf5\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.0149 - acc: 0.6162 - val_loss: 4.2108 - val_acc: 0.5854\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.59410\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 463s 592ms/step - loss: 4.0069 - acc: 0.6198 - val_loss: 4.2276 - val_acc: 0.5851\n",
            " - lr: 0.00007 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.59410\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 4.0084 - acc: 0.6186 - val_loss: 4.2190 - val_acc: 0.5871\n",
            " - lr: 0.00006 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.59410\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.9916 - acc: 0.6239 - val_loss: 4.2016 - val_acc: 0.5916\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.59410\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.9891 - acc: 0.6266 - val_loss: 4.2092 - val_acc: 0.5885\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.59410\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.9893 - acc: 0.6254 - val_loss: 4.2048 - val_acc: 0.5926\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.59410\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 463s 592ms/step - loss: 3.9828 - acc: 0.6287 - val_loss: 4.2248 - val_acc: 0.5855\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.59410\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 463s 593ms/step - loss: 3.9859 - acc: 0.6263 - val_loss: 4.1734 - val_acc: 0.5974\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00030: val_acc improved from 0.59410 to 0.59745, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_7_0002.hdf5\n",
            "LR Range :  1.0140825e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhRz4iJlG3l3",
        "colab_type": "code",
        "outputId": "05eefaf9-0f24-4ce8-94ba-0d79f3d0c9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2185
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_8_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.3, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_7_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 20, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "781/781 [==============================] - 796s 1s/step - loss: 4.0149 - acc: 0.6503 - val_loss: 4.2367 - val_acc: 0.5831\n",
            " - lr: 0.00023 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58313, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 786s 1s/step - loss: 4.0397 - acc: 0.6415 - val_loss: 4.3363 - val_acc: 0.5595\n",
            " - lr: 0.00036 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58313\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 785s 1s/step - loss: 4.0723 - acc: 0.6314 - val_loss: 4.3163 - val_acc: 0.5600\n",
            " - lr: 0.00049 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.58313\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.1132 - acc: 0.6172 - val_loss: 4.3554 - val_acc: 0.5538\n",
            " - lr: 0.00061 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.58313\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.1667 - acc: 0.6051 - val_loss: 4.4636 - val_acc: 0.5289\n",
            " - lr: 0.00074 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.58313\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.2325 - acc: 0.5917 - val_loss: 4.7349 - val_acc: 0.4822\n",
            " - lr: 0.00087 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.58313\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.3028 - acc: 0.5754 - val_loss: 4.5937 - val_acc: 0.5035\n",
            " - lr: 0.00100 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.58313\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.3409 - acc: 0.5693 - val_loss: 4.6053 - val_acc: 0.5130\n",
            " - lr: 0.00087 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.58313\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 4.3104 - acc: 0.5860 - val_loss: 4.5335 - val_acc: 0.5315\n",
            " - lr: 0.00074 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.58313\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 4.2661 - acc: 0.6005 - val_loss: 4.5381 - val_acc: 0.5373\n",
            " - lr: 0.00061 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.58313\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 4.2316 - acc: 0.6123 - val_loss: 4.4918 - val_acc: 0.5539\n",
            " - lr: 0.00049 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.58313\n",
            "Epoch 12/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 4.1769 - acc: 0.6295 - val_loss: 4.3970 - val_acc: 0.5769\n",
            " - lr: 0.00036 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.58313\n",
            "Epoch 13/20\n",
            "781/781 [==============================] - 782s 1s/step - loss: 4.1413 - acc: 0.6421 - val_loss: 4.3610 - val_acc: 0.5852\n",
            " - lr: 0.00023 \n",
            "\n",
            "Epoch 00013: val_acc improved from 0.58313 to 0.58519, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5\n",
            "Epoch 14/20\n",
            "781/781 [==============================] - 781s 1s/step - loss: 4.0949 - acc: 0.6590 - val_loss: 4.3520 - val_acc: 0.5918\n",
            " - lr: 0.00010 \n",
            "\n",
            "Epoch 00014: val_acc improved from 0.58519 to 0.59177, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5\n",
            "Epoch 15/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.0694 - acc: 0.6665 - val_loss: 4.3191 - val_acc: 0.5993\n",
            " - lr: 0.00008 \n",
            "\n",
            "Epoch 00015: val_acc improved from 0.59177 to 0.59927, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5\n",
            "Epoch 16/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 4.0577 - acc: 0.6698 - val_loss: 4.3156 - val_acc: 0.6011\n",
            " - lr: 0.00007 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.59927 to 0.60109, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5\n",
            "Epoch 17/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 4.0534 - acc: 0.6735 - val_loss: 4.3393 - val_acc: 0.5953\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60109\n",
            "Epoch 18/20\n",
            "781/781 [==============================] - 785s 1s/step - loss: 4.0516 - acc: 0.6738 - val_loss: 4.2863 - val_acc: 0.6035\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60109 to 0.60353, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5\n",
            "Epoch 19/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.0486 - acc: 0.6753 - val_loss: 4.3145 - val_acc: 0.6022\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60353\n",
            "Epoch 20/20\n",
            "781/781 [==============================] - 784s 1s/step - loss: 4.0422 - acc: 0.6764 - val_loss: 4.3391 - val_acc: 0.5943\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60353\n",
            "LR Range :  1.0211268e-06 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H+OYydZNX501AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cj5dvbm8mD6",
        "colab_type": "code",
        "outputId": "54ce4b0b-8041-4e68-e787-35d442119b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2063
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_9_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.2, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_8_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 images belonging to 200 classes.\n",
            "--->params {'epochs': 20, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "781/781 [==============================] - 775s 992ms/step - loss: 4.0614 - acc: 0.6724 - val_loss: 4.3391 - val_acc: 0.5928\n",
            " - lr: 0.00011 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.59285, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_9_0002.hdf5\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.0779 - acc: 0.6660 - val_loss: 4.3443 - val_acc: 0.5931\n",
            " - lr: 0.00016 \n",
            "\n",
            "Epoch 00002: val_acc improved from 0.59285 to 0.59309, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_9_0002.hdf5\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.0899 - acc: 0.6628 - val_loss: 4.3633 - val_acc: 0.5908\n",
            " - lr: 0.00022 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.59309\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.1007 - acc: 0.6593 - val_loss: 4.3827 - val_acc: 0.5847\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.59309\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.1177 - acc: 0.6564 - val_loss: 4.4167 - val_acc: 0.5752\n",
            " - lr: 0.00033 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.59309\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.1404 - acc: 0.6499 - val_loss: 4.4429 - val_acc: 0.5724\n",
            " - lr: 0.00039 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59309\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.1637 - acc: 0.6433 - val_loss: 4.4407 - val_acc: 0.5717\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59309\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.1904 - acc: 0.6373 - val_loss: 4.5350 - val_acc: 0.5537\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59309\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 768s 984ms/step - loss: 4.2036 - acc: 0.6368 - val_loss: 4.5175 - val_acc: 0.5637\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.59309\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.1936 - acc: 0.6429 - val_loss: 4.4768 - val_acc: 0.5720\n",
            " - lr: 0.00039 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.59309\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.1780 - acc: 0.6491 - val_loss: 4.4394 - val_acc: 0.5796\n",
            " - lr: 0.00033 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.59309\n",
            "Epoch 12/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.1559 - acc: 0.6566 - val_loss: 4.4526 - val_acc: 0.5825\n",
            " - lr: 0.00028 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.59309\n",
            "Epoch 13/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.1399 - acc: 0.6625 - val_loss: 4.4117 - val_acc: 0.5847\n",
            " - lr: 0.00022 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59309\n",
            "Epoch 14/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.1212 - acc: 0.6720 - val_loss: 4.4412 - val_acc: 0.5879\n",
            " - lr: 0.00016 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59309\n",
            "Epoch 15/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.0995 - acc: 0.6806 - val_loss: 4.4103 - val_acc: 0.5961\n",
            " - lr: 0.00011 \n",
            "\n",
            "Epoch 00015: val_acc improved from 0.59309 to 0.59613, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_9_0002.hdf5\n",
            "Epoch 16/20\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.0821 - acc: 0.6858 - val_loss: 4.3897 - val_acc: 0.6018\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.59613 to 0.60180, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_9_0002.hdf5\n",
            "Epoch 17/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.0683 - acc: 0.6890 - val_loss: 4.4044 - val_acc: 0.5993\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60180\n",
            "Epoch 18/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.0624 - acc: 0.6911 - val_loss: 4.3574 - val_acc: 0.6047\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60180 to 0.60474, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_9_0002.hdf5\n",
            "Epoch 19/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.0595 - acc: 0.6960 - val_loss: 4.3881 - val_acc: 0.6034\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60474\n",
            "Epoch 20/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.0566 - acc: 0.6954 - val_loss: 4.4144 - val_acc: 0.5968\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60474\n",
            "LR Range :  5.1584504e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuUdSmEw0WJ5",
        "colab_type": "code",
        "outputId": "3ad3a49d-ea3a-4536-9904-1b0011711861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2482
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_10_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.3, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_9_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "--->params {'epochs': 30, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 772s 989ms/step - loss: 4.0682 - acc: 0.6907 - val_loss: 4.4010 - val_acc: 0.5973\n",
            " - lr: 0.00009 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.59726, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_10_0002.hdf5\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 768s 983ms/step - loss: 4.0777 - acc: 0.6880 - val_loss: 4.4103 - val_acc: 0.5965\n",
            " - lr: 0.00014 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.59726\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.0850 - acc: 0.6836 - val_loss: 4.4308 - val_acc: 0.5923\n",
            " - lr: 0.00018 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.59726\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 768s 984ms/step - loss: 4.1052 - acc: 0.6796 - val_loss: 4.4627 - val_acc: 0.5855\n",
            " - lr: 0.00022 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.59726\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.1182 - acc: 0.6764 - val_loss: 4.4565 - val_acc: 0.5902\n",
            " - lr: 0.00026 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.59726\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.1333 - acc: 0.6731 - val_loss: 4.4895 - val_acc: 0.5814\n",
            " - lr: 0.00031 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59726\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 762s 975ms/step - loss: 4.1584 - acc: 0.6681 - val_loss: 4.5130 - val_acc: 0.5762\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59726\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 754s 965ms/step - loss: 4.1810 - acc: 0.6637 - val_loss: 4.5526 - val_acc: 0.5720\n",
            " - lr: 0.00039 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59726\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 751s 961ms/step - loss: 4.2009 - acc: 0.6591 - val_loss: 4.5915 - val_acc: 0.5614\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.59726\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 751s 961ms/step - loss: 4.2324 - acc: 0.6511 - val_loss: 4.6238 - val_acc: 0.5569\n",
            " - lr: 0.00048 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.59726\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 751s 962ms/step - loss: 4.2554 - acc: 0.6493 - val_loss: 4.5945 - val_acc: 0.5679\n",
            " - lr: 0.00048 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.59726\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 751s 962ms/step - loss: 4.2567 - acc: 0.6502 - val_loss: 4.6069 - val_acc: 0.5682\n",
            " - lr: 0.00044 \n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.59726\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 752s 963ms/step - loss: 4.2554 - acc: 0.6527 - val_loss: 4.5840 - val_acc: 0.5703\n",
            " - lr: 0.00039 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59726\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 751s 962ms/step - loss: 4.2476 - acc: 0.6620 - val_loss: 4.6260 - val_acc: 0.5636\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59726\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 751s 962ms/step - loss: 4.2362 - acc: 0.6657 - val_loss: 4.5576 - val_acc: 0.5870\n",
            " - lr: 0.00031 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.59726\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 752s 963ms/step - loss: 4.2223 - acc: 0.6724 - val_loss: 4.5627 - val_acc: 0.5891\n",
            " - lr: 0.00026 \n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.59726\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 753s 965ms/step - loss: 4.2047 - acc: 0.6790 - val_loss: 4.5702 - val_acc: 0.5877\n",
            " - lr: 0.00022 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.59726\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 751s 962ms/step - loss: 4.1913 - acc: 0.6844 - val_loss: 4.5214 - val_acc: 0.5945\n",
            " - lr: 0.00018 \n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.59726\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 768s 983ms/step - loss: 4.1761 - acc: 0.6913 - val_loss: 4.5354 - val_acc: 0.5963\n",
            " - lr: 0.00014 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.59726\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 767s 982ms/step - loss: 4.1606 - acc: 0.6969 - val_loss: 4.5657 - val_acc: 0.5910\n",
            " - lr: 0.00009 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.59726\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 767s 983ms/step - loss: 4.1463 - acc: 0.7022 - val_loss: 4.5189 - val_acc: 0.5999\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00021: val_acc improved from 0.59726 to 0.59988, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_10_0002.hdf5\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 767s 982ms/step - loss: 4.1367 - acc: 0.7074 - val_loss: 4.5029 - val_acc: 0.6030\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00022: val_acc improved from 0.59988 to 0.60302, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_10_0002.hdf5\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 768s 984ms/step - loss: 4.1327 - acc: 0.7082 - val_loss: 4.5099 - val_acc: 0.5989\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.60302\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.1322 - acc: 0.7076 - val_loss: 4.5227 - val_acc: 0.5979\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.60302\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 768s 983ms/step - loss: 4.1241 - acc: 0.7124 - val_loss: 4.5195 - val_acc: 0.6011\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.60302\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 769s 984ms/step - loss: 4.1253 - acc: 0.7116 - val_loss: 4.4948 - val_acc: 0.6045\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00026: val_acc improved from 0.60302 to 0.60454, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_10_0002.hdf5\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 769s 985ms/step - loss: 4.1282 - acc: 0.7097 - val_loss: 4.5198 - val_acc: 0.5983\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.60454\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 768s 984ms/step - loss: 4.1198 - acc: 0.7116 - val_loss: 4.4971 - val_acc: 0.6073\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00028: val_acc improved from 0.60454 to 0.60727, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_10_0002.hdf5\n",
            "Epoch 29/30\n",
            "617/781 [======================>.......] - ETA: 2:35 - loss: 4.1176 - acc: 0.7138"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e82eq8cRE48h",
        "colab_type": "code",
        "outputId": "f7b06864-1651-45de-be88-4771a380827e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2185
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_11_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.7, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_10_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 20, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 4.1222 - acc: 0.7101 - val_loss: 4.5519 - val_acc: 0.5922\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.59225, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 759s 972ms/step - loss: 4.1684 - acc: 0.6945 - val_loss: 4.6768 - val_acc: 0.5682\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.59225\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 757s 970ms/step - loss: 4.2218 - acc: 0.6782 - val_loss: 4.7510 - val_acc: 0.5489\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.59225\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 757s 969ms/step - loss: 4.2467 - acc: 0.6741 - val_loss: 4.6419 - val_acc: 0.5822\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.59225\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 757s 970ms/step - loss: 4.1948 - acc: 0.6934 - val_loss: 4.5854 - val_acc: 0.5947\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00005: val_acc improved from 0.59225 to 0.59471, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1451 - acc: 0.7116 - val_loss: 4.5777 - val_acc: 0.5957\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00006: val_acc improved from 0.59471 to 0.59573, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 757s 970ms/step - loss: 4.1142 - acc: 0.7241 - val_loss: 4.5560 - val_acc: 0.5999\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00007: val_acc improved from 0.59573 to 0.59988, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1047 - acc: 0.7271 - val_loss: 4.5634 - val_acc: 0.6030\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00008: val_acc improved from 0.59988 to 0.60302, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1001 - acc: 0.7286 - val_loss: 4.5496 - val_acc: 0.6032\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.60302 to 0.60322, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.0989 - acc: 0.7299 - val_loss: 4.5681 - val_acc: 0.5978\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60322\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1018 - acc: 0.7279 - val_loss: 4.5492 - val_acc: 0.6036\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00011: val_acc improved from 0.60322 to 0.60363, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 12/20\n",
            "781/781 [==============================] - 757s 970ms/step - loss: 4.0975 - acc: 0.7281 - val_loss: 4.5478 - val_acc: 0.6073\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.60363 to 0.60727, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 13/20\n",
            "781/781 [==============================] - 757s 970ms/step - loss: 4.1115 - acc: 0.7251 - val_loss: 4.5547 - val_acc: 0.5991\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.60727\n",
            "Epoch 14/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1187 - acc: 0.7257 - val_loss: 4.5696 - val_acc: 0.6003\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.60727\n",
            "Epoch 15/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1277 - acc: 0.7216 - val_loss: 4.5460 - val_acc: 0.6045\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.60727\n",
            "Epoch 16/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1257 - acc: 0.7222 - val_loss: 4.5437 - val_acc: 0.6074\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.60727 to 0.60737, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 17/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1294 - acc: 0.7226 - val_loss: 4.5689 - val_acc: 0.5995\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60737\n",
            "Epoch 18/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1326 - acc: 0.7202 - val_loss: 4.5177 - val_acc: 0.6096\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60737 to 0.60960, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5\n",
            "Epoch 19/20\n",
            "781/781 [==============================] - 759s 971ms/step - loss: 4.1405 - acc: 0.7200 - val_loss: 4.5549 - val_acc: 0.6054\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60960\n",
            "Epoch 20/20\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 4.1359 - acc: 0.7203 - val_loss: 4.5804 - val_acc: 0.5992\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60960\n",
            "LR Range :  5.045272e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkz58dGXrRxX",
        "colab_type": "code",
        "outputId": "6293be58-1759-4cc6-a0c7-073d1cba175c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2063
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 256\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True, random_crop_size=32)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_12_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.7, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "# op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_11_0002.hdf5')\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "--->params {'epochs': 20, 'steps': 390, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "390/390 [==============================] - 265s 679ms/step - loss: 5.3301 - acc: 0.4387 - val_loss: 4.6579 - val_acc: 0.5879\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58794, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5\n",
            "Epoch 2/20\n",
            "390/390 [==============================] - 258s 661ms/step - loss: 5.2020 - acc: 0.4449 - val_loss: 4.7032 - val_acc: 0.5709\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58794\n",
            "Epoch 3/20\n",
            "390/390 [==============================] - 256s 658ms/step - loss: 5.2022 - acc: 0.4423 - val_loss: 4.7472 - val_acc: 0.5633\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.58794\n",
            "Epoch 4/20\n",
            "390/390 [==============================] - 257s 658ms/step - loss: 5.2089 - acc: 0.4398 - val_loss: 4.7450 - val_acc: 0.5620\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.58794\n",
            "Epoch 5/20\n",
            "390/390 [==============================] - 256s 657ms/step - loss: 5.1567 - acc: 0.4474 - val_loss: 4.6995 - val_acc: 0.5755\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.58794\n",
            "Epoch 6/20\n",
            "390/390 [==============================] - 256s 657ms/step - loss: 5.1201 - acc: 0.4569 - val_loss: 4.6638 - val_acc: 0.5840\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.58794\n",
            "Epoch 7/20\n",
            "390/390 [==============================] - 255s 655ms/step - loss: 5.0793 - acc: 0.4673 - val_loss: 4.6559 - val_acc: 0.5906\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00007: val_acc improved from 0.58794 to 0.59056, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5\n",
            "Epoch 8/20\n",
            "390/390 [==============================] - 255s 654ms/step - loss: 5.0660 - acc: 0.4696 - val_loss: 4.6540 - val_acc: 0.5895\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59056\n",
            "Epoch 9/20\n",
            "390/390 [==============================] - 256s 655ms/step - loss: 5.0638 - acc: 0.4709 - val_loss: 4.6345 - val_acc: 0.5906\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.59056\n",
            "Epoch 10/20\n",
            "390/390 [==============================] - 256s 655ms/step - loss: 5.0575 - acc: 0.4710 - val_loss: 4.6583 - val_acc: 0.5872\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.59056\n",
            "Epoch 11/20\n",
            "390/390 [==============================] - 256s 655ms/step - loss: 5.0533 - acc: 0.4710 - val_loss: 4.6318 - val_acc: 0.5923\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00011: val_acc improved from 0.59056 to 0.59228, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5\n",
            "Epoch 12/20\n",
            "390/390 [==============================] - 255s 655ms/step - loss: 5.0428 - acc: 0.4745 - val_loss: 4.6348 - val_acc: 0.5955\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.59228 to 0.59552, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5\n",
            "Epoch 13/20\n",
            "390/390 [==============================] - 255s 655ms/step - loss: 5.0376 - acc: 0.4761 - val_loss: 4.6387 - val_acc: 0.5919\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59552\n",
            "Epoch 14/20\n",
            "390/390 [==============================] - 256s 656ms/step - loss: 5.0524 - acc: 0.4737 - val_loss: 4.6508 - val_acc: 0.5913\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59552\n",
            "Epoch 15/20\n",
            "390/390 [==============================] - 256s 655ms/step - loss: 5.0420 - acc: 0.4759 - val_loss: 4.6382 - val_acc: 0.5935\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.59552\n",
            "Epoch 16/20\n",
            "390/390 [==============================] - 255s 653ms/step - loss: 5.0294 - acc: 0.4776 - val_loss: 4.6281 - val_acc: 0.5975\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.59552 to 0.59755, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5\n",
            "Epoch 17/20\n",
            "390/390 [==============================] - 254s 650ms/step - loss: 5.0230 - acc: 0.4789 - val_loss: 4.6627 - val_acc: 0.5917\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.59755\n",
            "Epoch 18/20\n",
            "390/390 [==============================] - 256s 656ms/step - loss: 5.0380 - acc: 0.4758 - val_loss: 4.6061 - val_acc: 0.5976\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.59755 to 0.59765, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5\n",
            "Epoch 19/20\n",
            "390/390 [==============================] - 256s 657ms/step - loss: 5.0230 - acc: 0.4797 - val_loss: 4.6354 - val_acc: 0.5965\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.59765\n",
            "Epoch 20/20\n",
            "390/390 [==============================] - 256s 656ms/step - loss: 5.0323 - acc: 0.4782 - val_loss: 4.6689 - val_acc: 0.5908\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.59765\n",
            "LR Range :  5.0906596e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEWCAYAAAC0Q+rDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJRkREYnM/w8VVb6MiXK4/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_AMyM3LvyQw",
        "colab_type": "code",
        "outputId": "48ca95dc-5e72-46c4-9c2e-0622ff70ba81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2984
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_13_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.7, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "# model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model = load_model('/content/gdrive/My Drive/tinyimagenet-model/model_12_0002.hdf5')\n",
        "\n",
        "\n",
        "wts = get_wts(model, factor=5)\n",
        "print(wts)\n",
        "loss = weighted_categorical_crossentropy(wts)\n",
        "model.compile(optimizer=op, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "100/100 [==============================] - 29s 285ms/step\n",
            "[1.9999999999999998, 1.65, 3.1500000000000004, 3.5, 3.1999999999999997, 3.3, 2.45, 3.0500000000000003, 2.15, 2.95, 3.0, 2.8, 2.4000000000000004, 2.5, 2.65, 3.1, 3.75, 2.25, 3.1, 2.5500000000000003, 2.65, 2.05, 2.25, 2.2, 3.8000000000000003, 2.35, 3.1500000000000004, 4.0, 2.8, 4.1, 3.1500000000000004, 2.8, 3.9000000000000004, 3.0, 2.75, 3.1, 2.4000000000000004, 2.6999999999999997, 2.45, 3.75, 3.75, 3.1999999999999997, 3.5, 3.25, 1.4, 1.85, 2.9, 2.85, 3.95, 3.8000000000000003, 2.5999999999999996, 2.85, 2.5500000000000003, 2.9, 2.5, 2.8, 3.0, 2.6999999999999997, 1.9000000000000004, 2.8, 2.5500000000000003, 3.1999999999999997, 4.3, 3.3, 4.6, 3.6, 2.95, 3.45, 2.6999999999999997, 3.25, 3.3, 2.4000000000000004, 3.25, 3.0, 2.85, 3.3, 3.0500000000000003, 4.7, 2.45, 3.75, 4.5, 1.8000000000000003, 2.5500000000000003, 3.3499999999999996, 3.45, 3.4, 3.1500000000000004, 3.4, 3.55, 3.4, 2.8, 2.85, 2.9, 2.5, 3.3499999999999996, 2.85, 3.1999999999999997, 3.3499999999999996, 3.0500000000000003, 3.8000000000000003, 4.05, 2.45, 3.25, 1.9999999999999998, 3.3, 3.8500000000000005, 3.4, 2.6999999999999997, 2.45, 2.85, 2.4000000000000004, 2.5, 3.8500000000000005, 3.6500000000000004, 3.45, 1.9000000000000004, 3.25, 3.1, 2.65, 3.75, 4.0, 2.5999999999999996, 3.6500000000000004, 3.95, 2.45, 3.0, 2.9, 3.4, 3.4, 2.65, 3.3, 4.95, 4.6, 1.9000000000000004, 3.8000000000000003, 4.15, 3.25, 3.8000000000000003, 4.2, 4.2, 3.3499999999999996, 2.9, 3.6500000000000004, 1.9499999999999997, 3.6, 1.65, 2.0999999999999996, 3.55, 2.95, 2.5500000000000003, 3.0500000000000003, 3.45, 3.1500000000000004, 3.4, 2.9, 3.0500000000000003, 3.1999999999999997, 3.1999999999999997, 4.35, 4.55, 3.6, 2.5500000000000003, 3.1, 2.95, 2.95, 1.75, 1.75, 3.3499999999999996, 4.7, 2.9, 2.45, 2.5, 4.95, 2.0999999999999996, 2.9, 4.75, 2.25, 3.6, 2.15, 3.8000000000000003, 3.9000000000000004, 2.6999999999999997, 3.45, 2.35, 2.15, 2.8, 2.5999999999999996, 2.4000000000000004, 3.1, 1.65, 3.45, 2.45, 2.9, 2.5999999999999996, 2.45, 2.95, 2.8, 3.75, 3.55, 3.1]\n",
            "--->params {'epochs': 30, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 806s 1s/step - loss: 7.8522 - acc: 0.6942 - val_loss: 8.9414 - val_acc: 0.5932\n",
            " - lr: 0.00015 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.59325, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 802s 1s/step - loss: 7.7416 - acc: 0.6972 - val_loss: 9.0853 - val_acc: 0.5814\n",
            " - lr: 0.00025 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.59325\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 7.8064 - acc: 0.6898 - val_loss: 9.1748 - val_acc: 0.5795\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.59325\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 7.9212 - acc: 0.6773 - val_loss: 9.4559 - val_acc: 0.5540\n",
            " - lr: 0.00045 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.59325\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 8.0596 - acc: 0.6644 - val_loss: 9.3728 - val_acc: 0.5683\n",
            " - lr: 0.00045 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.59325\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 799s 1s/step - loss: 8.0234 - acc: 0.6693 - val_loss: 9.2381 - val_acc: 0.5782\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59325\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 799s 1s/step - loss: 7.9070 - acc: 0.6799 - val_loss: 9.2690 - val_acc: 0.5792\n",
            " - lr: 0.00025 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59325\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 798s 1s/step - loss: 7.7939 - acc: 0.6938 - val_loss: 9.1151 - val_acc: 0.5920\n",
            " - lr: 0.00015 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59325\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 802s 1s/step - loss: 7.6806 - acc: 0.7091 - val_loss: 8.9918 - val_acc: 0.6043\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.59325 to 0.60434, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.6085 - acc: 0.7155 - val_loss: 9.0391 - val_acc: 0.5994\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60434\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.5878 - acc: 0.7166 - val_loss: 8.9932 - val_acc: 0.6011\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.60434\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.5770 - acc: 0.7185 - val_loss: 8.9759 - val_acc: 0.6059\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.60434 to 0.60585, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 804s 1s/step - loss: 7.5761 - acc: 0.7200 - val_loss: 9.0159 - val_acc: 0.5993\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.60585\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 802s 1s/step - loss: 7.5521 - acc: 0.7201 - val_loss: 9.0441 - val_acc: 0.6021\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.60585\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.5512 - acc: 0.7219 - val_loss: 8.9892 - val_acc: 0.6061\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00015: val_acc improved from 0.60585 to 0.60606, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.5343 - acc: 0.7228 - val_loss: 8.9819 - val_acc: 0.6051\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.60606\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.5269 - acc: 0.7234 - val_loss: 9.0626 - val_acc: 0.5968\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60606\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 802s 1s/step - loss: 7.5434 - acc: 0.7225 - val_loss: 8.8825 - val_acc: 0.6075\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60606 to 0.60748, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 801s 1s/step - loss: 7.5385 - acc: 0.7225 - val_loss: 9.0014 - val_acc: 0.6041\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60748\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 7.5078 - acc: 0.7258 - val_loss: 9.1239 - val_acc: 0.5941\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60748\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 799s 1s/step - loss: 7.5236 - acc: 0.7248 - val_loss: 9.0137 - val_acc: 0.6013\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.60748\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 799s 1s/step - loss: 7.5260 - acc: 0.7248 - val_loss: 8.9563 - val_acc: 0.6051\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.60748\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 7.5183 - acc: 0.7251 - val_loss: 9.0172 - val_acc: 0.6002\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.60748\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 7.5147 - acc: 0.7264 - val_loss: 9.0372 - val_acc: 0.5991\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.60748\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 800s 1s/step - loss: 7.5066 - acc: 0.7272 - val_loss: 9.0549 - val_acc: 0.6037\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.60748\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 802s 1s/step - loss: 7.4973 - acc: 0.7288 - val_loss: 8.9326 - val_acc: 0.6043\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.60748\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 801s 1s/step - loss: 7.5005 - acc: 0.7277 - val_loss: 9.0084 - val_acc: 0.5993\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.60748\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 802s 1s/step - loss: 7.4912 - acc: 0.7287 - val_loss: 8.9794 - val_acc: 0.6058\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.60748\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.4968 - acc: 0.7292 - val_loss: 9.0487 - val_acc: 0.5984\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.60748\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 803s 1s/step - loss: 7.4881 - acc: 0.7288 - val_loss: 8.8780 - val_acc: 0.6077\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00030: val_acc improved from 0.60748 to 0.60768, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5\n",
            "LR Range :  5.0301793e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Bs7MnCMI_h",
        "colab_type": "code",
        "outputId": "7fc23b5e-3a24-4597-e7c8-e7b56d51aace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4435
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_14_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.7, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.load_weights('/content/gdrive/My Drive/tinyimagenet-model/model_13_0002.hdf5')\n",
        "\n",
        "\n",
        "wts = get_wts(model, factor=5)\n",
        "print(wts)\n",
        "loss = weighted_categorical_crossentropy(wts)\n",
        "model.compile(optimizer=op, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "100/100 [==============================] - 49s 492ms/step\n",
            "[1.9000000000000004, 1.7000000000000002, 2.85, 3.4, 3.25, 3.3499999999999996, 2.15, 2.5999999999999996, 1.9000000000000004, 3.0500000000000003, 2.5999999999999996, 2.6999999999999997, 2.4000000000000004, 2.4000000000000004, 2.65, 3.1500000000000004, 3.8000000000000003, 2.15, 3.0500000000000003, 2.5, 2.45, 2.25, 1.9499999999999997, 2.05, 3.55, 1.9499999999999997, 3.1, 3.9000000000000004, 2.65, 3.9000000000000004, 3.4, 2.65, 3.75, 3.3, 2.65, 2.75, 1.9999999999999998, 2.85, 2.65, 3.1, 3.5, 3.3, 3.3, 3.3499999999999996, 1.4499999999999997, 1.85, 2.85, 2.75, 3.6500000000000004, 3.75, 2.45, 2.85, 1.9999999999999998, 2.75, 2.45, 2.75, 2.6999999999999997, 2.6999999999999997, 1.9999999999999998, 3.0, 2.4000000000000004, 3.3499999999999996, 4.1, 3.1, 4.6, 4.15, 3.0500000000000003, 3.55, 2.3, 3.1500000000000004, 3.1, 2.75, 3.1999999999999997, 3.1, 2.5500000000000003, 3.25, 3.0500000000000003, 4.2, 2.0999999999999996, 3.75, 4.6, 1.85, 2.5500000000000003, 3.1999999999999997, 3.4, 3.1, 3.0500000000000003, 3.45, 3.7, 3.1999999999999997, 2.25, 2.8, 2.75, 2.5, 3.75, 3.0, 3.1500000000000004, 3.25, 2.75, 3.8500000000000005, 3.95, 2.65, 2.9, 1.9000000000000004, 3.1500000000000004, 3.45, 3.4, 2.65, 2.05, 2.9, 2.75, 2.65, 3.9000000000000004, 3.45, 3.3, 1.6, 3.1, 2.85, 2.5, 3.3499999999999996, 4.0, 2.4000000000000004, 4.0, 4.05, 2.2, 3.0500000000000003, 2.8, 3.3499999999999996, 3.5, 2.75, 3.25, 4.65, 4.449999999999999, 1.7000000000000002, 3.55, 4.1, 3.3, 3.5, 3.9000000000000004, 4.05, 3.1, 3.25, 3.45, 1.65, 3.55, 1.8000000000000003, 2.05, 3.6, 2.85, 2.8, 2.65, 3.5, 2.95, 3.0500000000000003, 2.75, 2.8, 3.1999999999999997, 3.45, 4.0, 4.55, 3.7, 2.5500000000000003, 2.85, 2.9, 2.85, 1.65, 1.5499999999999998, 3.5, 4.6, 3.0, 2.25, 2.5500000000000003, 3.9000000000000004, 2.0999999999999996, 2.85, 5.1000000000000005, 2.5, 3.6500000000000004, 2.25, 3.8500000000000005, 4.0, 2.45, 3.3499999999999996, 2.5, 1.9499999999999997, 2.65, 2.45, 2.3, 3.3, 1.6, 3.4, 2.3, 2.8, 2.25, 2.65, 3.1999999999999997, 2.8, 3.6500000000000004, 3.5, 2.85]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 20, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "781/781 [==============================] - 1431s 2s/step - loss: 7.3266 - acc: 0.7297 - val_loss: 9.0285 - val_acc: 0.5892\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58924, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 1448s 2s/step - loss: 7.3891 - acc: 0.7184 - val_loss: 9.8532 - val_acc: 0.5376\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58924\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 1455s 2s/step - loss: 7.4790 - acc: 0.7073 - val_loss: 9.9435 - val_acc: 0.5326\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.58924\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 1433s 2s/step - loss: 7.5432 - acc: 0.7008 - val_loss: 9.2628 - val_acc: 0.5790\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.58924\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 1427s 2s/step - loss: 7.4715 - acc: 0.7141 - val_loss: 9.0754 - val_acc: 0.5890\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.58924\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 1421s 2s/step - loss: 7.4076 - acc: 0.7261 - val_loss: 9.0408 - val_acc: 0.5974\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00006: val_acc improved from 0.58924 to 0.59745, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 1420s 2s/step - loss: 7.3503 - acc: 0.7355 - val_loss: 8.9956 - val_acc: 0.5995\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00007: val_acc improved from 0.59745 to 0.59947, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 1420s 2s/step - loss: 7.3313 - acc: 0.7377 - val_loss: 9.0283 - val_acc: 0.6010\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00008: val_acc improved from 0.59947 to 0.60099, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 1419s 2s/step - loss: 7.3199 - acc: 0.7395 - val_loss: 8.9578 - val_acc: 0.6023\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.60099 to 0.60231, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 1419s 2s/step - loss: 7.3201 - acc: 0.7401 - val_loss: 9.0210 - val_acc: 0.5998\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60231\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 1419s 2s/step - loss: 7.3267 - acc: 0.7404 - val_loss: 8.9588 - val_acc: 0.6052\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00011: val_acc improved from 0.60231 to 0.60525, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 12/20\n",
            "564/781 [====================>.........] - ETA: 6:22 - loss: 7.3175 - acc: 0.7384Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "100/100 [==============================] - 49s 492ms/step\n",
            "[1.9000000000000004, 1.7000000000000002, 2.85, 3.4, 3.25, 3.3499999999999996, 2.15, 2.5999999999999996, 1.9000000000000004, 3.0500000000000003, 2.5999999999999996, 2.6999999999999997, 2.4000000000000004, 2.4000000000000004, 2.65, 3.1500000000000004, 3.8000000000000003, 2.15, 3.0500000000000003, 2.5, 2.45, 2.25, 1.9499999999999997, 2.05, 3.55, 1.9499999999999997, 3.1, 3.9000000000000004, 2.65, 3.9000000000000004, 3.4, 2.65, 3.75, 3.3, 2.65, 2.75, 1.9999999999999998, 2.85, 2.65, 3.1, 3.5, 3.3, 3.3, 3.3499999999999996, 1.4499999999999997, 1.85, 2.85, 2.75, 3.6500000000000004, 3.75, 2.45, 2.85, 1.9999999999999998, 2.75, 2.45, 2.75, 2.6999999999999997, 2.6999999999999997, 1.9999999999999998, 3.0, 2.4000000000000004, 3.3499999999999996, 4.1, 3.1, 4.6, 4.15, 3.0500000000000003, 3.55, 2.3, 3.1500000000000004, 3.1, 2.75, 3.1999999999999997, 3.1, 2.5500000000000003, 3.25, 3.0500000000000003, 4.2, 2.0999999999999996, 3.75, 4.6, 1.85, 2.5500000000000003, 3.1999999999999997, 3.4, 3.1, 3.0500000000000003, 3.45, 3.7, 3.1999999999999997, 2.25, 2.8, 2.75, 2.5, 3.75, 3.0, 3.1500000000000004, 3.25, 2.75, 3.8500000000000005, 3.95, 2.65, 2.9, 1.9000000000000004, 3.1500000000000004, 3.45, 3.4, 2.65, 2.05, 2.9, 2.75, 2.65, 3.9000000000000004, 3.45, 3.3, 1.6, 3.1, 2.85, 2.5, 3.3499999999999996, 4.0, 2.4000000000000004, 4.0, 4.05, 2.2, 3.0500000000000003, 2.8, 3.3499999999999996, 3.5, 2.75, 3.25, 4.65, 4.449999999999999, 1.7000000000000002, 3.55, 4.1, 3.3, 3.5, 3.9000000000000004, 4.05, 3.1, 3.25, 3.45, 1.65, 3.55, 1.8000000000000003, 2.05, 3.6, 2.85, 2.8, 2.65, 3.5, 2.95, 3.0500000000000003, 2.75, 2.8, 3.1999999999999997, 3.45, 4.0, 4.55, 3.7, 2.5500000000000003, 2.85, 2.9, 2.85, 1.65, 1.5499999999999998, 3.5, 4.6, 3.0, 2.25, 2.5500000000000003, 3.9000000000000004, 2.0999999999999996, 2.85, 5.1000000000000005, 2.5, 3.6500000000000004, 2.25, 3.8500000000000005, 4.0, 2.45, 3.3499999999999996, 2.5, 1.9499999999999997, 2.65, 2.45, 2.3, 3.3, 1.6, 3.4, 2.3, 2.8, 2.25, 2.65, 3.1999999999999997, 2.8, 3.6500000000000004, 3.5, 2.85]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 20, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "781/781 [==============================] - 1431s 2s/step - loss: 7.3266 - acc: 0.7297 - val_loss: 9.0285 - val_acc: 0.5892\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58924, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 1448s 2s/step - loss: 7.3891 - acc: 0.7184 - val_loss: 9.8532 - val_acc: 0.5376\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58924\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 1455s 2s/step - loss: 7.4790 - acc: 0.7073 - val_loss: 9.9435 - val_acc: 0.5326\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.58924\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 1433s 2s/step - loss: 7.5432 - acc: 0.7008 - val_loss: 9.2628 - val_acc: 0.5790\n",
            " - lr: 0.00035 \n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.58924\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 1427s 2s/step - loss: 7.4715 - acc: 0.7141 - val_loss: 9.0754 - val_acc: 0.5890\n",
            " - lr: 0.00020 \n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.58924\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 1421s 2s/step - loss: 7.4076 - acc: 0.7261 - val_loss: 9.0408 - val_acc: 0.5974\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00006: val_acc improved from 0.58924 to 0.59745, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 1420s 2s/step - loss: 7.3503 - acc: 0.7355 - val_loss: 8.9956 - val_acc: 0.5995\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00007: val_acc improved from 0.59745 to 0.59947, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 1420s 2s/step - loss: 7.3313 - acc: 0.7377 - val_loss: 9.0283 - val_acc: 0.6010\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00008: val_acc improved from 0.59947 to 0.60099, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 1419s 2s/step - loss: 7.3199 - acc: 0.7395 - val_loss: 8.9578 - val_acc: 0.6023\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.60099 to 0.60231, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 1419s 2s/step - loss: 7.3201 - acc: 0.7401 - val_loss: 9.0210 - val_acc: 0.5998\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60231\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 1419s 2s/step - loss: 7.3267 - acc: 0.7404 - val_loss: 8.9588 - val_acc: 0.6052\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00011: val_acc improved from 0.60231 to 0.60525, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 12/20\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.3151 - acc: 0.7382 - val_loss: 8.9601 - val_acc: 0.6064\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.3151 - acc: 0.7382 - val_loss: 8.9601 - val_acc: 0.6064\n",
            " - lr: 0.00003  - lr: 0.00003 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.60525 to 0.60636, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.60525 to 0.60636, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 13/20\n",
            "Epoch 13/20\n",
            "781/781 [==============================] - 1417s 2s/step - loss: 7.3556 - acc: 0.7371 - val_loss: 8.9848 - val_acc: 0.6007\n",
            "781/781 [==============================] - 1417s 2s/step - loss: 7.3556 - acc: 0.7371 - val_loss: 8.9848 - val_acc: 0.6007\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.60636\n",
            "Epoch 14/20\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.60636\n",
            "Epoch 14/20\n",
            "781/781 [==============================] - 1416s 2s/step - loss: 7.3736 - acc: 0.7367 - val_loss: 9.0245 - val_acc: 0.6012\n",
            "781/781 [==============================] - 1416s 2s/step - loss: 7.3736 - acc: 0.7367 - val_loss: 9.0245 - val_acc: 0.6012\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.60636\n",
            "Epoch 15/20\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.60636\n",
            "Epoch 15/20\n",
            "781/781 [==============================] - 1412s 2s/step - loss: 7.3931 - acc: 0.7340 - val_loss: 8.9696 - val_acc: 0.6060\n",
            "781/781 [==============================] - 1412s 2s/step - loss: 7.3931 - acc: 0.7340 - val_loss: 8.9696 - val_acc: 0.6060\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.60636\n",
            "Epoch 16/20\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.60636\n",
            "Epoch 16/20\n",
            "781/781 [==============================] - 1416s 2s/step - loss: 7.3902 - acc: 0.7338 - val_loss: 8.9399 - val_acc: 0.6073\n",
            "781/781 [==============================] - 1416s 2s/step - loss: 7.3902 - acc: 0.7338 - val_loss: 8.9399 - val_acc: 0.6073\n",
            " - lr: 0.00001  - lr: 0.00001 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.60636 to 0.60727, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.60636 to 0.60727, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 17/20\n",
            "Epoch 17/20\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.3980 - acc: 0.7345 - val_loss: 9.0495 - val_acc: 0.5989\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.3980 - acc: 0.7345 - val_loss: 9.0495 - val_acc: 0.5989\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60727\n",
            "Epoch 18/20\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60727\n",
            "Epoch 18/20\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.4125 - acc: 0.7325 - val_loss: 8.8505 - val_acc: 0.6082\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.4125 - acc: 0.7325 - val_loss: 8.8505 - val_acc: 0.6082\n",
            " - lr: 0.00001  - lr: 0.00001 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60727 to 0.60818, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60727 to 0.60818, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5\n",
            "Epoch 19/20\n",
            "Epoch 19/20\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.4342 - acc: 0.7316 - val_loss: 8.9604 - val_acc: 0.6063\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.4342 - acc: 0.7316 - val_loss: 8.9604 - val_acc: 0.6063\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60818\n",
            "Epoch 20/20\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60818\n",
            "Epoch 20/20\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.4267 - acc: 0.7328 - val_loss: 9.0844 - val_acc: 0.5990\n",
            "781/781 [==============================] - 1418s 2s/step - loss: 7.4267 - acc: 0.7328 - val_loss: 9.0844 - val_acc: 0.5990\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60818\n",
            "LR Range :  5.045272e-07 0.0005\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60818\n",
            "LR Range :  5.045272e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl0HOd55/vvgx0kQYBoQKS4mURL\nlkJZ1mJSIhDfiddIziRWcsaOpWyaxI4niZR44htPpMmN4vhac6PEc+w4lmPreIm8hdJonISTKFbs\n2GNPDHCBrIUiJVpocBclAg0SC4kdz/2jqqkm1AAaQFc30P37nNMH1dVvvf10kegHVe9m7o6IiEjU\nygodgIiIlAYlHBERyQslHBERyQslHBERyQslHBERyQslHBERyQslHBERyQslHJE8M7NfMrNOMxsy\ns9Nm9s9m9mYz+6iZfW2GY46a2XB4zMtm9jdmtirfsYsshhKOSB6Z2YeBTwH/DVgLbAY+C9yWxeE/\n5+6rgOuBG4B7o4pTJApKOCJ5Ymb1wMeAu9z9m+5+3t3H3f1/uftHsq3H3V8GniBIPCLLhhKOSP60\nAjXA3y2mEjPbCLwL6MpFUCL5ooQjkj8xoNfdJxZ4/N+b2SBwAjgD/EnOIhPJAyUckfxJAk1mVrHA\n43/e3euAtwBXA025CkwkH5RwRPKnAxgFfn4xlbj794G/AT6Rg5hE8mahf2mJyDy5e7+Z3Qc8aGYT\nwL8A48A7gLcCF4AyM6u59DAfzVDdp4CjZnaduz8TdewiuaArHJE8cvf/DnwY+H+AHoL2mLuBvw+L\n3AEMpz0SM9TTA3wFuC/ikEVyxrQAm4iI5IOucEREJC+UcEREJC+UcEREJC+UcEREJC9Kult0U1OT\nb9mypdBhiIgsK08++WSvuzfP97iSTjhbtmyhs7Oz0GGIiCwrZnZsIcfplpqIiOSFEo6IiOSFEo6I\niOSFEo6IiOSFEo6IiORFpAnHzG41s8Nm1mVm92R4vdrMHglf32tmW9Jeuzfcf9jMbpmrTjP7GzM7\nYmZPhw8tvysisoRE1i3azMqBB4F3AieB/Wa2290PpRV7P3DW3a8ws9uBB4D3mdk24HbgGmA98B0z\ne314zGx1fsTdH4vqM4mIyMJFeYVzE9Dl7t3uPgbsAm6bVuY24OFw+zHg7WZm4f5d7j7q7kcI1m6/\nKcs6ZQb/8PQpeocyLa0iIhK9KBPOBoK1PlJOhvsylgnXee8nWPd9pmPnqvN+M3vWzD5pZtWZgjKz\nD5pZp5l19vT0zP9TLVNHes/zoV1P83t/+1ShQxGRElVMnQbuJVjnfQfQCPxhpkLu/pC7b3f37c3N\n856ZYdnqSCQBOHCyv8CRiEipijLhnAI2pT3fGO7LWMbMKoB6IDnLsTPW6e6nPTAKfJng9puE2hO9\nAAyNTdB/YbzA0YhIKYoy4ewHrjSzrWZWRdAJYPe0MruBO8Pt9wDf9WAJ0t3A7WEvtq3AlcC+2eo0\ns8vDnwb8PPBchJ9tWXF39nQn2dBQizvsPZIsdEgiUoIiSzhhm8zdwBPA88Cj7n7QzD5mZu8Oi30R\niJlZF8E67/eExx4EHgUOAd8C7nL3yZnqDOv6upkdAA4ATcDHo/psy82LZ4boHRrjt98Sp6ayjPaE\nEo6I5F+ks0W7++PA49P23Ze2PQK8d4Zj7wfuz6bOcP/bFhtvsWrvCm6n/dTrm3niYOPF9hwRkXwq\npk4DMoOO7iSbGmvZ1LiC1niMw68Mqnu0iOSdEk6Rm5xy9nT30doSA6At3gTAnm5d5YhIfinhFLnn\nTw/QPzx+MdG8Yf1qVlVXqB1HRPJOCafIpdprWuPBFU5FeRk3b1U7jojknxJOkWtP9NLSvJK1q2su\n7muNxzjSe57T/cMFjExESo0SThEbn5xi35E+2sKrm5TU7TVd5YhIPinhFLEDp/o5PzZ5McGkXL2u\njjUrKtWOIyJ5pYRTxFJXMDtbLr3CKSszdrbE6EgkCSZ2EBGJnhJOEWtP9HL1ujoaV1a95rW2eIxT\n54Y53nehAJGJSClSwilSoxOTdB49e7F32nSt4W023VYTkXxRwilSTx0/x+jE1Gvab1LizSu5rK5a\nHQdEJG+UcIpURyJJmcFNWxszvm5mtMVjtKsdR0TyRAmnSHUkkrxhQz31tZUzlmmNx+gdGqXrzFAe\nIxORUqWEU4SGxyZ56sTM7TcpbWrHEZE8UsIpQp3H+hif9Bnbb1I2Na5g45rai6uBiohESQmnCLUn\nklSUGdtft2bOsm3xGHu6+5iaUjuOiERLCacItSeSXL+pgZXVc6+v1xZvon94nEOnB/IQmYiUMiWc\nIjMwMs6Bk+fmbL9JSZVT92gRiZoSTpHZf6SPKSfrhLN2dQ0tzSvVjiMikVPCKTLtiSRVFWXcuHnu\n9puUtniMfUf6GJ+cijAyESl1SjhFpiOR5E2b11BTWZ71MW3xJs6PTXLgVH+EkYlIqVPCKSJnz49x\n6PTAa9a/mUtqNmm144hIlJRwisjeI0HCaLtifgmncWUVV6+rUzuOiERKCaeItCeSrKgq540bG+Z9\nbFu8ic6jZxkZn4wgMhERJZyi0p5IsmNLI5Xl8/9nbYvHGJ2Y4qnj5yKITERECadonBkcoevM0Lzb\nb1JuammkzKCjW+04IhINJZwikWrwz3b8zXSrayq5dmMDHWrHEZGIKOEUiY5EkrqaCq5ZX7/gOlpb\nYjx1/BwXxiZyGJmISEAJp0h0dCe5eWuM8jJbcB1t8RgTU87+o2dzGJmISEAJpwicOjfMseSFBbff\npGzfsobKclP3aBGJhBJOEUi138x3/M10K6oquGHTGvZoAKiIRCDShGNmt5rZYTPrMrN7MrxebWaP\nhK/vNbMtaa/dG+4/bGa3zKPOT5tZSa2Z3J7opXFlFa+/rG7RdbXGYxw41U//8HgOIhMReVVkCcfM\nyoEHgXcB24A7zGzbtGLvB866+xXAJ4EHwmO3AbcD1wC3Ap81s/K56jSz7UD2s1YWAXenI5GktSVG\n2SLab1Ja4zGmHPYd6ctBdCIir4ryCucmoMvdu919DNgF3DatzG3Aw+H2Y8DbzczC/bvcfdTdjwBd\nYX0z1hkmo78A/kuEn2nJOZa8wOn+kQV3h57uhs0NVFeUqR1HRHIuyoSzATiR9vxkuC9jGXefAPqB\n2CzHzlbn3cBudz89W1Bm9kEz6zSzzp6ennl9oKWofZHjb6arrihnx5ZGTeQpIjlXFJ0GzGw98F7g\nr+Yq6+4Puft2d9/e3NwcfXARa0/0snZ1NS1NK3NWZ2s8xgsvD5IcGs1ZnSIiUSacU8CmtOcbw30Z\ny5hZBVAPJGc5dqb9NwBXAF1mdhRYYWZdufogS5W7s6c7SVu8ieBOZG6kulfv6VY7jojkTpQJZz9w\npZltNbMqgk4Au6eV2Q3cGW6/B/iuu3u4//awF9tW4Epg30x1uvs/ufs6d9/i7luAC2FHhKL24pkh\neofGaG3Jze20lGs31LOqukLtOCKSUxVRVezuE2Z2N/AEUA58yd0PmtnHgE533w18EfhqeDXSR5BA\nCMs9ChwCJoC73H0SIFOdUX2Gpa69K0gIuWq/SakoL+OmrWrHEZHciizhALj748Dj0/bdl7Y9QtD2\nkunY+4H7s6kzQ5lVC4l3uenoTrJxTS2bGlfkvO62eIzvvnCGl/tHWFdfk/P6RaT0FEWngVI0NeXs\n6e5b9HQ2M0ldNXV067aaiOSGEs4ydej0AP3D47TFmyKp/yfWraZhRSXtXbqtJiK5oYSzTC12/Zu5\nlJUZO7fGaE8kCfpxiIgsjhLOMtWe6KWleSVrV0fXvtJ2RYxT54Y50Tcc2XuISOlQwlmGxien2Hck\nuvablFT96h4tIrmghLMMHTjVz/mxSVpbomm/SYk3r6K5rpqObrXjiMjiKeEsQ6n2m50tjZG+j5nR\nFlc7jojkhhLOMtSRSHL1ujpiq6ojf6/Wlhg9g6MkekpqiSERiYASzjIzOjHJ/qN9kfVOmy7V7bpd\nsw6IyCIp4SwzTx0/x+jEVGTjb6bb1FjLhoZajccRkUVTwllmOhJJygxu2hpt+01Kqh1nz5EkU1Nq\nxxGRhVPCWWY6EknesKGe+trKvL1n2xUxzl0Y5/mXB/L2niJSfJRwlpHhsUmeOnE2b+03Kanu15o9\nWkQWQwlnGek81sf4pOd8/Zu5rKuvoaVppToOiMiiKOEsI+2JJBVlxo4t+Wm/Sdcaj7HvSB8Tk1N5\nf28RKQ5KOMtIRyLJ9ZsaWFkd6TJGGbXFmxganeDAqf68v7eIFAclnGViYGScZ0+ey3v7TUpqVgPd\nVhORhVLCWSb2H+ljyqNbjmAusVXVXL2uTh0HRGTBlHCWiY5EkqqKMm7cvKZgMbTGY+w/2sfoxGTB\nYhCR5UsJZ5loTyR50+Y11FSWFyyGtngToxNTPHX8XMFiEJHlK6uEY2ZvNrNfD7ebzWxrtGFJurPn\nxzh0eiDy9W/mctPWRspM43FEZGHmTDhm9ifAHwL3hrsqga9FGZRcau+RaJeTzlZ9bSXXbqhXwhGR\nBcnmCucXgHcD5wHc/SWgLsqg5FLtiSQrqsp548aGQofCzniMp06c5cLYRKFDEZFlJpuEM+bB6lsO\nYGYrow1JputIJNmxpZGqisI3ubXFmxifdDqPni10KCKyzGTzDfaomX0eaDCz3wS+A3wh2rAk5czg\nCC+eGSr47bSUHVvWUFFmGo8jIvM255B1d/+Emb0TGACuAu5z929HHpkArzbQF7rDQMqKqgpu2NxA\nR7cSjojMTzadBh5w92+7+0fc/Q/c/dtm9kA+ghPY052krqaCa9bXFzqUi1rjTRw4eY6BkfFChyIi\ny0g2t9TemWHfu3IdiGTWnkhy89YY5WVW6FAuam2JMeWwr7uv0KGIyDIyY8Ixs982swPAVWb2bNrj\nCPBs/kIsXafODXMseWHJ3E5LuWFzA9UVZWrHEZF5ma0N5xvAPwP/H3BP2v5Bd9eftnmQar9ZKh0G\nUmoqy9m+ZQ3tid5ChyIiy8iMVzju3u/uR939Dnc/BgwTdI1eZWabs6nczG41s8Nm1mVm92R4vdrM\nHglf32tmW9Jeuzfcf9jMbpmrTjP7opk9E16FPWZmq7I6A0tYe6KXxpVVXLV26Q17aos38cLLg/Sd\nHyt0KCKyTGTTaeDnzOxF4AjwfeAowZXPXMeVAw8StPdsA+4ws23Tir0fOOvuVwCfBB4Ij90G3A5c\nA9wKfNbMyueo8/fd/Tp3fyNwHLh7rhiXMnenI5GktSVG2RJqv0lJXXXtUW81EclSNp0GPg7sBH7s\n7luBtwN7sjjuJqDL3bvdfQzYBdw2rcxtwMPh9mPA283Mwv273H3U3Y8AXWF9M9bp7gMA4fG1hANV\nl6tjyQuc7h9h5xK7nZZy7YZ6VlaV67aaiGQtm4Qz7u5JoMzMytz9e8D2LI7bAJxIe34y3JexjLtP\nAP1AbJZjZ63TzL4MvAxcDfxVpqDM7INm1mlmnT09PVl8jMJoX2Ljb6arLC/jpq2N6jggIlnLJuGc\nC9tDfgB83cz+knBetaXG3X8dWA88D7xvhjIPuft2d9/e3Nyc1/jmo6M7ydrV1bQ0Ld2ZhNriTXT3\nnOeVgZFChyIiy0A2Cec24ALw+8C3gATwc1kcdwrYlPZ8Y7gvYxkzqwDqgeQsx85Zp7tPEtxq+w9Z\nxLgkBe03vbS2xAjuEC5NqXYczR4tItmYM+G4+3l3n3L3CXd/GPgMQUP+XPYDV5rZVjOrIugEsHta\nmd3AneH2e4DvhhOF7gZuD3uxbQWuBPbNVKcFroCLbTjvBl7IIsYl6cUzQ/QOjdEWbyp0KLPadvlq\n6msr1Y4jIlmZcRyOma0G7iJoI9kNfDt8/gfAM8DXZ6vY3SfM7G7gCaAc+JK7HzSzjwGd7r4b+CLw\nVTPrAvoIEghhuUeBQ8AEcFd45cIMdZYBD4cxWxjfby/khCwFS3X8zXRlZcbOFrXjiEh2Zhv4+VXg\nLNABfAD4rwRf5j/v7k9nU7m7Pw48Pm3ffWnbI8B7Zzj2fuD+LOucAn4ym5iWg/ZELxvX1LKpcUWh\nQ5lTW7yJJw6+wom+C8siXhEpnNkSTou7XwtgZl8ATgObwyQhEZmacvZ093HLNWsLHUpWUr3o2hO9\nvK8xq/HAIlKiZmvDuTgVcHg766SSTfQOnR6gf3h8yd9OS7nislU0rapWxwERmdNsVzjXmdlAuG1A\nbfjcAHf31ZFHV4Iutt+0LO0OAylmRls8Rnsiibsv6V51IlJYs82lVu7uq8NHnbtXpG0r2USkoztJ\nS/NK1tXXFDqUrLXFY5wZHCXRsySHZ4nIEpHNOBzJk/HJKfZ2B/OnLSevjsdR92gRmZkSzhJy4FQ/\n58cml/z4m+k2N65gQ0OtukeLyKyUcJaQVPvNzpbGAkcyP2ZGazzGnu4kU1PLes5UEYmQEs4S0pFI\ncvW6OmKrqgsdyry1xWOcvTDOCy8PFjoUEVmislkPZ9DMBqY9TpjZ35lZSz6CLAWjE5PsP9q3bLpD\nT9eaNh5HRCSTbK5wPgV8hGCKm40EU9t8g2CCzC9FF1ppefr4OUYnppZdh4GUy+tr2dq0UuNxRGRG\n2SScd7v759190N0H3P0h4BZ3fwRYE3F8JaM9kaTM4OZlmnAguMrZe6SPicmpQociIktQNgnngpn9\nopmVhY9fBFIzDqiFOEc6EknesKGe+trKQoeyYG3xGEOjEzz30sDchUWk5GSTcH4Z+FXgDPBKuP0r\nZlYL3B1hbCVjeGySp06cXba301J2tqgdR0Rmls16ON3u/nPu3uTuzeF2l7sPu/u/5SPIYtd5rI/x\nSV+2HQZSmlZVc/W6OrXjiEhGs82lBoCZNQO/CWxJL+/uvxFdWKWlI5GkoszYsWV5jb/JZGdLjF37\njzM6MUl1RXmhwxGRJSSbW2r/QLD083eAf0p7SI60J5Jct6mBldVz5v8lry0eY2R8iqePnyt0KCKy\nxGTzDbfC3f8w8khK1ODIOAdO9fM7b4kXOpScuLklRpkFk5Au5x53IpJ72Vzh/KOZ/UzkkZSo/Uf7\nmJzyZd9hIKW+tpI3bKjXvGoi8hrZJJwPESSd4XCWgcG0dXJkkdq7klRVlHHj64pnSFNrPMZTx88y\nPDZZ6FBEZAnJppdanbuXuXut1sPJvfZEkjdtXkNNZfE0sLe2xBifdDqP9RU6FBFZQmZMOGZ2dfjz\nxkyP/IVYvM6eH+P5lweWfXfo6XZsaaSizHRbTUQuMVungQ8DHwT+e4bXHHhbJBGVkL1HkrgHPbuK\nycrqCq7f1KDxOCJyiRkTjrt/MPz51vyFU1raE0lWVJXzxo0NhQ4l59riMT7zvS4GRsZZXbN8p+sR\nkdzJaj0cM2szs18ys19LPaIOrBR0JJJs39JIVUXxLUvUGm9iymH/EbXjiEggm/Vwvgp8AngzsCN8\nbI84rqJ3ZnCEF88MFd3ttJQbNjdQVVGmdhwRuSibgZ/bgW3urpmhc2hPd/CXf7EmnJrKcra/bo0S\njohclM29nOeAdVEHUmo6Er3U1VRwzfr6QocSmbZ4jOdPD3D2/FihQxGRJSCbhNMEHDKzJ8xsd+oR\ndWDFrj2R5OatMcrLrNChRKY13gTAnm5d5YhIdrfUPhp1EKXm1LlhjiUv8GutWwodSqTeuLGelVXl\ntCeSvOvaywsdjogU2KwJx8zKgY+qa3RupcanFGv7TUpleRk7tjZqQTYRAea4pebuk8CUmRVvQ0MB\ntCd6aVxZxVVr6wodSuTa4jESPed5ZWBk7sIiUtSyacMZAg6Y2RfN7NOpRzaVm9mtZnbYzLrM7J4M\nr1eb2SPh63vNbEvaa/eG+w+b2S1z1WlmXw/3P2dmXzKzJTna0N3Zk0iys6WRsiJuv0lpC9txNOuA\niGSTcL4J/DHwA+DJtMeswttxDwLvArYBd5jZtmnF3g+cdfcrgE8CD4THbgNuB64BbgU+a2blc9T5\ndeBq4FqgFvhAFp8t744lL/BS/8jFBvVi9xOXr6a+tlIJR0Tm7jTg7g8vsO6bgC537wYws13AbcCh\ntDK38WqnhMeAz5iZhft3ufsocMTMusL6mKlOd388VamZ7QM2LjDuSLWXSPtNSnmZsbOlkfZuteOI\nlLpsZhq40sweM7NDZtademRR9wbgRNrzk+G+jGXcfQLoB2KzHDtnneGttF8FvjXD5/mgmXWaWWdP\nT08WHyO3OrqTXFZXTUvTyry/d6G0tsQ40TfMib4LhQ5FRAoom1tqXwb+GpgA3gp8BfhalEEt0meB\nH7j7/8n0ors/5O7b3X17c3NzXgNzdzoSSdriMYILudLQdoXacUQku4RT6+7/Cpi7H3P3jwL/Povj\nTgGb0p5vDPdlLGNmFUA9kJzl2FnrNLM/AZoJllZYcrrODNE7NHqxIb1UXHnZKppWVal7tEiJyybh\njJpZGfCimd1tZr8ArMriuP3AlWa21cyqCDoBTJ+hYDdwZ7j9HuC74Zxtu4Hbw15sW4ErgX2z1Wlm\nHwBuAe5w96ks4su7VPtNsS24NhczozXeREd3Ek3JJ1K6skk4HwJWAL8HvAn4FV5NEjMK22TuBp4A\nngcedfeDZvYxM3t3WOyLQCzsFPBh4J7w2IPAowQdDL4F3OXukzPVGdb1OWAt0GFmT5vZfVl8trxq\nT/SycU0tmxpXFDqUvGuLx3hlYJTu3vOFDkVECiSbXmr7Acxsyt1/fT6Vhz3HHp+277607RHgvTMc\nez9wfzZ1hvuzmaanYKamnD3dffz0trWFDqUgWluCq7r2RJJ4czYXyCJSbLLppdZqZoeAF8Ln15nZ\nZyOPrMgcOj1A//A4bVeU1u20lNfFVrC+voYOteOIlKxsbql9iqBtJAng7s8A/y7KoIpRqodWa0tp\ndRhIudiOk0gyNaV2HJFSlNXaxu5+YtquyQhiKWod3Ulamlayrr6m0KEUTFs8xtkL4xx+ZbDQoYhI\nAWSTcE6YWRvgZlZpZn9A0GAvWZqYnGLfkb6S6502XerzaxVQkdKUTcL5LeAughH9p4Drgd+JMqhi\nc+BUP0OjEyU3/ma69Q21bImtUDuOSImaM+G4e6+7/7K7r3X3y9z9V4Bfy0NsRSP1F/3OlsYCR1J4\nrfEm9nb3MTG5JIdKiUiEsmrDyWBJjuRfqjoSSa5eV0dsVXWhQym4tniMwdEJnntpoNChiEieLTTh\nlM5EYIs0OjFJ57E+draUdvtNSuo8aF41kdKz0ISjfq1Zevr4OUbGp0pmOYK5NNdVc9XaOs2rJlKC\nZhydb2aDZE4sRrDAmWShPZGkzOBmXeFc1BqPsWv/ccYmpqiqWOjfPCKy3Mz42+7ude6+OsOjbqlP\nI7OUdHQnuWZ9PfW1S3LF64JojccYGZ/i6RPnCh2KiOSR/ryM0PDYJE8dP6vbadPs3BrDDN1WEykx\nSjgR6jzWx/ikl/yAz+nqV1TyhvX16jggUmKUcCLUkUhSUWbs2KLxN9O1xWM8dfwcw2OaJUmkVCjh\nRKg9keS6TQ2srFaT13Q74zHGJqd48tjZQociInmihBORwZFxDpzqV/vNDHZsaaSizNSOI1JClHAi\nsv9oH5NTfnHhMbnUquoKrtvUoIk8RUqIEk5E2ruSVFWUcePr1hQ6lCWrLR7jwKl+BkfGCx2KiOSB\nEk5EOrqT3Li5gZrK8kKHsmS1xmNMTjn7j/YVOhQRyQMlnAicPT/GodMDJb8cwVxu3LyGqooy2rt0\nW02kFCjhRGDvkSTuqMPAHGoqy3nT5jVqxxEpEUo4EehIJKmtLOeNGxsKHcqS1xaPcej0AGfPjxU6\nFBGJmBJOBNoTSXZsbdTElFlouyK4Ctx7RFc5IsVO34g5dmZwhBfPDOl2WpbeuLGBFVXluq0mUgKU\ncHJsT3fQ40rjb7JTWV7Gji2NSjgiJUAJJ8c6Er3U1VRwzfrVhQ5l2WiLx+g6M8SZgZFChyIiEVLC\nybGORJKbt8aoKNepzVaq+3hHt65yRIqZvhVz6NS5YY4mL2g5gnnatn41q2sqtFyBSJFTwsmh1Bem\nOgzMT3mZsbMlpnYckSKnhJNDHYkka1ZUctXaukKHsuy0xmMc77vAib4LhQ5FRCISacIxs1vN7LCZ\ndZnZPRlerzazR8LX95rZlrTX7g33HzazW+aq08zuDve5meV9Thl3pyPRS2s8RlmZ5fvtlz2144gU\nv8gSjpmVAw8C7wK2AXeY2bZpxd4PnHX3K4BPAg+Ex24DbgeuAW4FPmtm5XPU+UPgHcCxqD7TbI4l\nL/BS/witmj9tQV6/dhWxlVVqxxEpYlFe4dwEdLl7t7uPAbuA26aVuQ14ONx+DHi7mVm4f5e7j7r7\nEaArrG/GOt39KXc/GuHnmVXqL3ONv1kYM6M1HqMjkcTdCx2OiEQgyoSzATiR9vxkuC9jGXefAPqB\n2CzHZlNnQbQnklxWV028eWWhQ1m22uJNvDwwwpHe84UORUQiUHKdBszsg2bWaWadPT09OakzaL9J\n0haPEVygyUKkupOrt5pIcYoy4ZwCNqU93xjuy1jGzCqAeiA5y7HZ1Dkrd3/I3be7+/bm5ub5HDqj\nrjND9A6NavzNIm2JreDy+hq144gUqSgTzn7gSjPbamZVBJ0Adk8rsxu4M9x+D/BdD27g7wZuD3ux\nbQWuBPZlWWfetV8cf6MOA4txsR2nO8nUlNpxRIpNZAknbJO5G3gCeB541N0PmtnHzOzdYbEvAjEz\n6wI+DNwTHnsQeBQ4BHwLuMvdJ2eqE8DMfs/MThJc9TxrZl+I6rNN15FIsqGhlk2NK/L1lkWrLd5E\n3/kxfnxmsNChiEiOVURZubs/Djw+bd99adsjwHtnOPZ+4P5s6gz3fxr49CJDnrepKaejO8lPb1ub\n77cuShfbcbqSXL1OE6CKFJOS6zSQa4dOD9A/PH5xITFZnA0NtbwutkIdB0SKkBLOIu25OP5G7Te5\n0haPsbc7ycTkVKFDEZEcUsJZpPZEkpamlayrryl0KEWjNd7E4OgEB18aKHQoIpJDSjiLMDE5xb4j\nfeoOnWOp2Ro0r5pIcVHCWYQDp/oZGp1Qwsmx5rpqXr92ldpxRIqMEs4ipL4Qd2r+tJxrbYmx/0gf\nYxNqxxEpFko4i9CRSHL1ujrLTHcsAAAQfUlEQVSaVlUXOpSi0xpvYnh8kmdOnit0KCKSI0o4CzQ6\nMUnnsT5d3URkZ0sjZsF4HBEpDko4C/T08XOMjE9pOemINKyo4pr1q+no7i10KCKSI0o4C9TRncQM\nbt6qhBOVtngTPzp2jpHxyUKHIiI5oISzQO2JJG9YX0/9ispCh1K0WltijE1O8eSxs4UORURyQAln\nAYbHJnnq+FndTovYjq2NlJcZ7QndVhMpBko4C/DksbOMTzo7lXAitaq6gus21ms8jkiRUMJZgPZE\nLxVlxo4tjYUOpei1xZt49mQ///TsaTqP9nEseZ7zoxOFDktEFiDS5QmK1Y+On+W6TQ2sqtbpi9rb\nfuIyHvzfXdz1jR9dsn9FVTlNq6pprqumaVVV+LP6kp/N4c+ayvICRS8i6SxYYLM0bd++3Ts7O+d9\n3PjkFMmhMU3YmSd958d4uX+EnqFRegZH6Z3h59kL4xmPr6uuoClMQE11VcHPacmpKUxc1RVKTiJz\nMbMn3X37fI/Tn+gLUFlepmSTR40rq2hcWTVnudQfAqkElEpQ6Unp8MuD/NtgLwMjmW/Lra6puPQq\nKcMVU9OqamKrqqgs1x1pkflQwpGikfpDIJs/BkYnJukdGqN3MMPVUrh98KUBegZHGZqhzWjNisoZ\nb+WlX1HFVlZTXma5/rgiy44SjpSk6opyNjTUsqGhds6yw2OTlySi197SG+Op4+foGRxlOMMg1TIL\nrtJmSkjNq2outkWtWVFFmZKTFCklHJE51FaVs6lxBZsaV8xZ9vzoRMak1DM0dvF5d895eoZGM86E\nXV5mxFZm7gSR6hyRurVXX1uJmZKTLB9KOCI5tLK6gpXVFWxpWjlrOXdncHQi7ZbeGD2DQceI3sGx\n4OfQKD9+ZZDeoVHGJ1/buaey3C5NSumdItKvpOqqqauuUHKSglPCESkAM2N1TSWrayppaV41a1l3\np394nN6hUc5cTE5pV0+Do7wyMMJzp/pJnh9jcuq1yamqouySRBRcKWW+klqp7v4SEf3PElnizIyG\nFVU0rKjiisvqZi07NeWcvTD2mqSU3nPv5NkLPH3iHMnzo2QaFVFbWZ5xfNNrOkasqqa2St3IJXtK\nOCJFpKzMiK2qJraqmqvWzZ6cJian6Lsw9uotvDAhXfw5NMqR3vPsP3qWvvNjGetYVV3x2uQ07UpK\nY5wkRQlHpERVlJdxWV0Nl9XN3Y18fHKKvvNjM45v6h0Kxjj9cChJ/3DmAbjTxzhlGt/UXKcxTsVM\nCUdE5lRZXsba1TWsXZ3dGKfkLLf0egfHOPjSAL2DowzOc4zT9B57GuO0vCjhiEhOVVeUs76hlvVZ\njHEaGZ9MS0Sjab30Ri7e6pttjJMZxOYY45TquacxToWnhCMiBVNTOb8xTrONb+oZDNqcegZHGZ1l\njNNrbulpjFPeKOGIyLKQGuP0utjcY5yGwgG46WOcLum5NzTKi68M0jPHGKc5ZyTXGKd5UcIRkaJi\nZtTVVFKX5RingeEJeoZG6Alv4U1ve3plYISDL/XTO5TtGKcMg281xglQwhGREmZm1K+opH5FJVdc\nNnvZqSnn3PD4jJ0hegZHOXVumKdPnKPv/CgZctOMY5zSf15WV7xjnCJNOGZ2K/CXQDnwBXf/s2mv\nVwNfAd4EJIH3ufvR8LV7gfcDk8DvufsTs9VpZluBXUAMeBL4VXfPPHhARGSeysrs4lIZVzH7GKfJ\nKb/YjTzj+k1DoxztvbCoMU5Nae1Py2WMU2QJx8zKgQeBdwIngf1mttvdD6UVez9w1t2vMLPbgQeA\n95nZNuB24BpgPfAdM3t9eMxMdT4AfNLdd5nZ58K6/zqqzyciMpPyMrvYxjOXTGOc0mci7xkc4cev\nDPHDrtnHOGW6hbfUxjhFeYVzE9Dl7t0AZrYLuA1ITzi3AR8Ntx8DPmNB69ttwC53HwWOmFlXWB+Z\n6jSz54G3Ab8Ulnk4rFcJR0SWtIWMcerNOPg2SFrPvzTAD+YY49S0qprP/+qb5mzjyrUoE84G4ETa\n85PAzTOVcfcJM+snuCW2Adgz7dgN4XamOmPAOXefyFD+Emb2QeCDAJs3b57fJxIRKaCFjHGanpBS\nz+tqKvMQ8aVKrtOAuz8EPASwffv2DM16IiLL33zGOOVLlDfzTgGb0p5vDPdlLGNmFUA9QeeBmY6d\naX8SaAjrmOm9RESkgKJMOPuBK81sq5lVEXQC2D2tzG7gznD7PcB33d3D/bebWXXY++xKYN9MdYbH\nfC+sg7DOf4jws4mIyDxFdkstbJO5G3iCoAvzl9z9oJl9DOh0993AF4Gvhp0C+ggSCGG5Rwk6GEwA\nd7n7JECmOsO3/ENgl5l9HHgqrFtERJYI80wrMJWI7du3e2dnZ6HDEBFZVszsSXffPt/jtOiEiIjk\nhRKOiIjkhRKOiIjkhRKOiIjkRUl3GjCzHuDYAg9vAnpzGE4uKbaFUWwLo9gWZjnH9jp3b55vpSWd\ncBbDzDoX0ksjHxTbwii2hVFsC1OKsemWmoiI5IUSjoiI5IUSzsI9VOgAZqHYFkaxLYxiW5iSi01t\nOCIikhe6whERkbxQwhERkbxQwlkAM7vVzA6bWZeZ3ZOH99tkZt8zs0NmdtDMPhTubzSzb5vZi+HP\nNeF+M7NPh/E9a2Y3ptV1Z1j+RTO7c6b3XECM5Wb2lJn9Y/h8q5ntDWN4JFxOgnDJiUfC/XvNbEta\nHfeG+w+b2S05iqvBzB4zsxfM7Hkza10q583Mfj/893zOzP7WzGoKdd7M7EtmdsbMnkvbl7PzZGZv\nMrMD4TGfNjNbZGx/Ef6bPmtmf2dmDXOdj5l+b2c65wuNLe21/9vM3Myalsp5C/f/bnjuDprZn6ft\nj/68ubse83gQLIuQAFqAKuAZYFvE73k5cGO4XQf8GNgG/DlwT7j/HuCBcPtngH8GDNgJ7A33NwLd\n4c814faaHMX4YeAbwD+Gzx8Fbg+3Pwf8drj9O8Dnwu3bgUfC7W3huawGtobnuDwHcT0MfCDcrgIa\nlsJ5I1gC/QhQm3a+/mOhzhvw74AbgefS9uXsPBGsZ7UzPOafgXctMrafBirC7QfSYst4Ppjl93am\nc77Q2ML9mwiWUTkGNC2h8/ZW4DtAdfj8snyet8i+JIv1AbQCT6Q9vxe4N88x/APwTuAwcHm473Lg\ncLj9eeCOtPKHw9fvAD6ftv+ScouIZyPwr8DbgH8Mfzl6074QLp6z8JewNdyuCMvZ9POYXm4RcdUT\nfKnbtP0FP28ECedE+CVTEZ63Wwp53oAt076ccnKewtdeSNt/SbmFxDbttV8Avh5uZzwfzPB7O9v/\n1cXEBjwGXAcc5dWEU/DzRpAk3pGhXF7Om26pzV/qiyLlZLgvL8JbKTcAe4G17n46fOllYG24PVOM\nUcX+KeC/AFPh8xhwzt0nMrzPxRjC1/vD8lHEthXoAb5swe2+L5jZSpbAeXP3U8AngOPAaYLz8CRL\n47yl5Oo8bQi3o4gR4DcI/vpfSGyz/V9dEDO7DTjl7s9Me2kpnLfXA/9XeCvs+2a2Y4GxLei8KeEs\nI2a2CvifwH9294H01zz4MyPvfdzN7GeBM+7+ZL7fOwsVBLcU/trdbwDOE9wauqiA520NcBtBUlwP\nrARuzXcc2SrUeZqLmf0RwarAXy90LABmtgL4r8B9hY5lBhUEV9U7gY8Aj86nXWixlHDm7xTB/dmU\njeG+SJlZJUGy+bq7fzPc/YqZXR6+fjlwZo4Yo4j9J4F3m9lRYBfBbbW/BBrMLLWEefr7XIwhfL0e\nSEYU20ngpLvvDZ8/RpCAlsJ5ewdwxN173H0c+CbBuVwK5y0lV+fpVLid0xjN7D8CPwv8cpgQFxJb\nkpnP+ULECf6IeCb8ndgI/MjM1i0gtijO20ngmx7YR3BXomkBsS3svC3kXm8pPwj+Qugm+E+VakS7\nJuL3NOArwKem7f8LLm3U/fNw+99zaePkvnB/I0GbxprwcQRozGGcb+HVTgP/g0sbFH8n3L6LSxu/\nHw23r+HSRstuctNp4P8AV4XbHw3PWcHPG3AzcBBYEb7fw8DvFvK88dr7/Tk7T7y28ftnFhnbrcAh\noHlauYzng1l+b2c65wuNbdprR3m1DWcpnLffAj4Wbr+e4HaZ5eu8RfYlWcwPgt4mPybovfFHeXi/\nNxPczngWeDp8/AzBfdR/BV4k6HmS+k9qwINhfAeA7Wl1/QbQFT5+PcdxvoVXE05L+MvSFf7HTPWK\nqQmfd4Wvt6Qd/0dhzIeZR2+cOWK6HugMz93fh7/QS+K8AX8KvAA8B3w1/GUvyHkD/pagLWmc4K/g\n9+fyPAHbw8+ZAD7DtI4cC4iti+DLMvX78Lm5zgcz/N7OdM4XGtu014/yasJZCuetCvhaWOePgLfl\n87xpahsREckLteGIiEheKOGIiEheKOGIiEheKOGIiEheKOGIiEheKOFIUTOzmJk9HT5eNrNTac+z\nmhXYzL5sZlfNUeYuM/vlHMX8b2Z2vZmVWY5nIzez3wgHIaaez/nZRHJF3aKlZJjZR4Ehd//EtP1G\n8LswlfHAPDOzfwPuJhgr0evuDXMcMv34cnefnK1ud3968ZGKzI+ucKQkmdkVFqwv9HWCEf+Xm9lD\nZtYZrhNyX1rZ1BVHhZmdM7M/M7NnzKzDzC4Ly3zczP5zWvk/M7N94ToibeH+lWb2P8P3fSx8r+tn\nCfPPgLrwauwrYR13hvU+bWafDa+CUnF9ysyeBW4ysz81s/0WrLXzOQu8j2Ag7COpK7zUZwvr/hUL\n1l55zsz+W7hvts98e1j2GTP7Xo7/iaQIKeFIKbsa+KS7b/Ng9uZ73H07wbTy7zSzbRmOqQe+7+7X\nAR0EI8QzMXe/iWCCxFTy+l3gZXffBvy/BLN+z+YeYNDdr3f3XzOzNxBMxd/m7tcTTDtye1pcP3D3\nN7p7B/CX7r4DuDZ87VZ3f4RgVP77wjrHLgZrthH4OMF6KTcAPxlOzDrbZ/4T4O3h/l+Y47OIKOFI\nSUu4e2fa8zvM7EcEU378BMGiVNMNu3tqKvwnCeaqyuSbGcq8mWCCUzyYuv7gPON9B7AD6DSzp4Gf\nIpgsEmAM+Lu0sm83s30Ec1/9FMFcWbO5Gfiuu/d6MJnoNwgW8IKZP/MPga+Y2QfQd4lkoWLuIiJF\n63xqw8yuBD4E3OTu58zsawTzl003lrY9ycy/Q6NZlJkvA77k7n98yc5gxt5hT03YFUyR/xmCVWJP\nmdnHyfxZsjXTZ/5NgkT1swQzIt/g7mcX8T5S5PRXiUhgNTAIDIRT8d8yR/mF+CHwiwBmdi2Zr6Au\n8nBxq7Qp4L8D/KKZNYX7Y2a2OcOhtQTTzveaWR3wH9JeGyRYpny6vcBbwzpTt+q+P8fnaXH3PcAf\nA2fJ40KEsjzpCkck8COC6e5fIFiH/ocRvMdfEdyCOhS+1yGClTtn80XgWTPrDNtx/hT4jpmVEcwC\n/FvAS+kHuHvSzB4O6z9NkExSvgx8wcyGgZvSjjlpZn8M/G+CK6n/5e7/lJbsMvmkmW0Ny/+Luz83\nx2eREqdu0SJ5En55V7j7SHgL71+AK/3VZXpFipqucETyZxXwr2HiMeA/KdlIKdEVjoiI5IU6DYiI\nSF4o4YiISF4o4YiISF4o4YiISF4o4YiISF78/1Aq+cuK4Nc8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vETj8iOZLzc",
        "colab_type": "code",
        "outputId": "ce57c7df-13a9-427b-a730-67f85202b337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2236
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 256\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=True)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_15_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.8, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.load_weights('/content/gdrive/My Drive/tinyimagenet-model/model_14_0002.hdf5')\n",
        "\n",
        "\n",
        "wts = get_wts(model, factor=5)\n",
        "print(wts)\n",
        "loss = weighted_categorical_crossentropy(wts)\n",
        "model.compile(optimizer=op, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "100/100 [==============================] - 30s 296ms/step\n",
            "[1.85, 1.7000000000000002, 2.9, 3.4, 3.4, 3.6500000000000004, 2.3, 2.5999999999999996, 1.75, 2.65, 2.6999999999999997, 2.75, 2.3, 2.45, 2.5, 3.1999999999999997, 3.8000000000000003, 2.15, 3.1, 2.45, 2.35, 2.15, 1.9000000000000004, 1.9999999999999998, 3.55, 1.85, 2.9, 3.7, 2.85, 3.8000000000000003, 3.3499999999999996, 2.65, 3.8000000000000003, 3.25, 2.65, 2.5999999999999996, 1.9999999999999998, 2.85, 2.4000000000000004, 3.0500000000000003, 3.45, 3.25, 3.3499999999999996, 3.1500000000000004, 1.4499999999999997, 1.8000000000000003, 3.0, 2.75, 3.75, 3.8500000000000005, 2.5, 2.9, 2.0999999999999996, 2.85, 2.4000000000000004, 2.75, 2.65, 2.9, 2.15, 2.95, 2.4000000000000004, 3.1, 4.15, 3.45, 4.5, 4.15, 2.9, 3.6, 2.35, 3.1500000000000004, 3.1, 2.5999999999999996, 3.1500000000000004, 3.0, 2.5500000000000003, 3.25, 2.95, 4.25, 1.9999999999999998, 3.8500000000000005, 4.6, 1.9000000000000004, 2.5500000000000003, 3.3, 3.3, 3.4, 2.95, 3.3, 3.6500000000000004, 3.3, 2.3, 2.75, 2.9, 2.65, 3.8000000000000003, 3.1500000000000004, 3.1999999999999997, 3.1999999999999997, 2.95, 3.9000000000000004, 3.9000000000000004, 2.65, 3.1, 1.85, 3.45, 3.75, 3.25, 2.65, 2.2, 2.4000000000000004, 2.5500000000000003, 2.5500000000000003, 4.0, 3.4, 3.45, 1.9000000000000004, 3.3499999999999996, 2.9, 2.5500000000000003, 3.6, 3.7, 2.5500000000000003, 4.0, 4.05, 2.3, 3.3, 2.8, 3.1500000000000004, 3.3499999999999996, 2.9, 3.1999999999999997, 5.1000000000000005, 4.55, 1.8000000000000003, 3.55, 4.1, 3.1500000000000004, 3.6, 4.15, 4.15, 3.1, 3.1999999999999997, 3.45, 1.6, 3.5, 1.75, 2.15, 3.5, 2.95, 2.85, 2.8, 3.3, 2.9, 3.1, 2.8, 2.75, 3.4, 3.1, 3.8500000000000005, 4.449999999999999, 3.6, 2.5, 2.85, 3.0, 2.75, 1.6, 1.6, 3.45, 4.6, 2.95, 2.25, 2.3, 4.15, 2.0999999999999996, 2.95, 5.0, 2.45, 3.55, 2.2, 4.05, 4.05, 2.5, 3.3499999999999996, 2.5, 1.9499999999999997, 2.65, 2.45, 2.25, 3.0, 1.7000000000000002, 3.1999999999999997, 2.3, 2.65, 2.05, 2.65, 3.1, 2.8, 3.7, 3.3499999999999996, 2.8]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "--->params {'epochs': 20, 'steps': 390, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "390/390 [==============================] - 276s 708ms/step - loss: 10.9181 - acc: 0.4513 - val_loss: 9.3518 - val_acc: 0.5832\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58323, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 2/20\n",
            "390/390 [==============================] - 267s 683ms/step - loss: 10.7202 - acc: 0.4470 - val_loss: 9.5615 - val_acc: 0.5660\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58323\n",
            "Epoch 3/20\n",
            "390/390 [==============================] - 265s 680ms/step - loss: 10.7497 - acc: 0.4399 - val_loss: 9.4270 - val_acc: 0.5701\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.58323\n",
            "Epoch 4/20\n",
            "390/390 [==============================] - 268s 688ms/step - loss: 10.4919 - acc: 0.4584 - val_loss: 9.2711 - val_acc: 0.5895\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00004: val_acc improved from 0.58323 to 0.58955, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 5/20\n",
            "390/390 [==============================] - 265s 679ms/step - loss: 10.3769 - acc: 0.4656 - val_loss: 9.1858 - val_acc: 0.5946\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00005: val_acc improved from 0.58955 to 0.59461, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 6/20\n",
            "390/390 [==============================] - 266s 682ms/step - loss: 10.3417 - acc: 0.4684 - val_loss: 9.2456 - val_acc: 0.5892\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59461\n",
            "Epoch 7/20\n",
            "390/390 [==============================] - 264s 678ms/step - loss: 10.3284 - acc: 0.4708 - val_loss: 9.2185 - val_acc: 0.5919\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59461\n",
            "Epoch 8/20\n",
            "390/390 [==============================] - 264s 678ms/step - loss: 10.2743 - acc: 0.4751 - val_loss: 9.2306 - val_acc: 0.5951\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00008: val_acc improved from 0.59461 to 0.59512, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 9/20\n",
            "390/390 [==============================] - 263s 675ms/step - loss: 10.2470 - acc: 0.4746 - val_loss: 9.1726 - val_acc: 0.5952\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.59512 to 0.59522, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 10/20\n",
            "390/390 [==============================] - 261s 669ms/step - loss: 10.2331 - acc: 0.4771 - val_loss: 9.2384 - val_acc: 0.5901\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.59522\n",
            "Epoch 11/20\n",
            "390/390 [==============================] - 260s 666ms/step - loss: 10.2070 - acc: 0.4784 - val_loss: 9.1768 - val_acc: 0.5954\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00011: val_acc improved from 0.59522 to 0.59542, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 12/20\n",
            "390/390 [==============================] - 259s 664ms/step - loss: 10.1739 - acc: 0.4795 - val_loss: 9.1886 - val_acc: 0.5968\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.59542 to 0.59684, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 13/20\n",
            "390/390 [==============================] - 260s 665ms/step - loss: 10.2116 - acc: 0.4772 - val_loss: 9.1781 - val_acc: 0.5946\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59684\n",
            "Epoch 14/20\n",
            "390/390 [==============================] - 258s 662ms/step - loss: 10.1981 - acc: 0.4809 - val_loss: 9.2393 - val_acc: 0.5934\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59684\n",
            "Epoch 15/20\n",
            "390/390 [==============================] - 258s 662ms/step - loss: 10.1800 - acc: 0.4806 - val_loss: 9.1810 - val_acc: 0.5964\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.59684\n",
            "Epoch 16/20\n",
            "390/390 [==============================] - 258s 662ms/step - loss: 10.1680 - acc: 0.4819 - val_loss: 9.1356 - val_acc: 0.5988\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00016: val_acc improved from 0.59684 to 0.59876, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 17/20\n",
            "390/390 [==============================] - 257s 660ms/step - loss: 10.1651 - acc: 0.4813 - val_loss: 9.2758 - val_acc: 0.5924\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.59876\n",
            "Epoch 18/20\n",
            "390/390 [==============================] - 258s 662ms/step - loss: 10.1690 - acc: 0.4793 - val_loss: 9.0705 - val_acc: 0.6014\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00018: val_acc improved from 0.59876 to 0.60140, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5\n",
            "Epoch 19/20\n",
            "390/390 [==============================] - 258s 662ms/step - loss: 10.1793 - acc: 0.4813 - val_loss: 9.1677 - val_acc: 0.5989\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60140\n",
            "Epoch 20/20\n",
            "390/390 [==============================] - 259s 664ms/step - loss: 10.1516 - acc: 0.4839 - val_loss: 9.2875 - val_acc: 0.5918\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60140\n",
            "LR Range :  5.0793017e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEWCAYAAAC0Q+rDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na99My8kKiRX",
        "colab_type": "code",
        "outputId": "011a308e-b959-445a-8d03-3c8584d8663d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2134
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Get data\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_VAL = 128\n",
        "train_generator, validation_generator = get_generators(BATCH_SIZE_TRAIN, BATCH_SIZE_VAL, no_aug=False, img_size_train=(64,64), img_size_val=(64,64), use_random_crop=False)\n",
        "\n",
        "\n",
        "# Training utils\n",
        "base_checkpoint_path='/content/gdrive/My Drive/tinyimagenet-model'\n",
        "filepath = base_checkpoint_path + \"/model_16_0002.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "loss = 'categorical_crossentropy'\n",
        "# MAX_LR = 0.0002\n",
        "# MAX_LR = 0.001*0.75\n",
        "MAX_LR = 0.001/2\n",
        "# MAX_LR = 0.01\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "clr_triangular = OneCycleLR(max_lr=MAX_LR,\n",
        "                            batch_size=BATCH_SIZE_TRAIN,\n",
        "                            samples=NUM_TRAIN_IMAGES,\n",
        "                            end_percentage=0.8, \n",
        "                            scale_percentage=0.1,\n",
        "                            maximum_momentum=None,\n",
        "                            minimum_momentum=None\n",
        "                           )\n",
        "\n",
        "model = ResnetBuilder.build_resnet_50((NUM_CHANNELS, None, None), NUM_CLASSES, weight_decay=1e-6)\n",
        "# op = SGD(0.1)\n",
        "# op = Adam()\n",
        "op = RMSprop(lr= 0.0001, epsilon=1e-08)\n",
        "# print(model.summary())\n",
        "# model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.load_weights('/content/gdrive/My Drive/tinyimagenet-model/model_15_0002.hdf5')\n",
        "\n",
        "\n",
        "wts = get_wts(model, factor=5)\n",
        "print(wts)\n",
        "loss = weighted_categorical_crossentropy(wts)\n",
        "model.compile(optimizer=op, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=NUM_TRAIN_IMAGES // BATCH_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=NUM_VAL_IMAGES // BATCH_SIZE_VAL,\n",
        "                    epochs=NUM_EPOCHS, \n",
        "                    callbacks=[clr_triangular, model_checkpoint],\n",
        "                    verbose=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "print(\"LR Range : \", min(clr_triangular.history['lr']), max(clr_triangular.history['lr']))\n",
        "\n",
        "\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR\")\n",
        "plt.plot(clr_triangular.history['lr'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100000 images belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "Found 10000 validated image filenames belonging to 200 classes.\n",
            "100/100 [==============================] - 25s 250ms/step\n",
            "[1.85, 1.65, 3.3499999999999996, 3.75, 3.1999999999999997, 3.25, 2.2, 3.1, 1.9999999999999998, 2.85, 2.85, 2.75, 2.45, 2.35, 2.5500000000000003, 3.1999999999999997, 3.9000000000000004, 2.25, 3.1, 2.5, 2.3, 2.05, 2.15, 2.0999999999999996, 3.7, 2.2, 3.1500000000000004, 4.0, 2.5999999999999996, 4.05, 3.0500000000000003, 2.75, 3.8500000000000005, 3.25, 2.65, 2.75, 2.2, 3.0500000000000003, 2.35, 3.7, 3.5, 3.1999999999999997, 3.4, 3.3499999999999996, 1.4, 1.8000000000000003, 3.0500000000000003, 2.75, 4.05, 3.8500000000000005, 2.4000000000000004, 2.95, 2.3, 2.95, 2.3, 2.75, 2.9, 2.85, 1.9999999999999998, 2.75, 2.3, 3.0, 4.3, 3.3, 4.6, 3.8000000000000003, 2.6999999999999997, 3.6, 2.65, 3.0500000000000003, 3.1, 2.5, 3.5, 3.1, 2.65, 3.25, 3.0500000000000003, 4.3, 2.25, 3.8500000000000005, 4.75, 1.9000000000000004, 2.65, 3.1500000000000004, 3.1500000000000004, 3.6, 3.1999999999999997, 3.5, 3.4, 3.5, 2.5999999999999996, 2.85, 2.8, 2.5500000000000003, 3.6500000000000004, 2.75, 3.45, 3.25, 3.1, 3.8000000000000003, 4.1, 2.5500000000000003, 3.1, 1.9000000000000004, 3.3499999999999996, 4.05, 3.25, 2.35, 2.45, 2.65, 2.35, 2.5, 3.9000000000000004, 3.8000000000000003, 3.4, 1.9499999999999997, 2.9, 3.4, 2.75, 3.8000000000000003, 4.05, 2.45, 3.55, 3.9000000000000004, 2.4000000000000004, 3.25, 2.85, 3.1, 3.1999999999999997, 2.85, 3.7, 4.9, 4.65, 1.65, 3.7, 4.05, 3.1999999999999997, 3.7, 4.1, 4.2, 3.25, 3.1, 3.6, 1.75, 3.7, 1.7000000000000002, 2.0999999999999996, 3.55, 2.8, 2.75, 2.85, 3.45, 3.1, 3.1500000000000004, 2.75, 3.0500000000000003, 3.1500000000000004, 3.1500000000000004, 3.7, 4.65, 3.7, 2.35, 2.6999999999999997, 2.6999999999999997, 2.95, 1.6, 1.65, 3.45, 4.8, 3.1999999999999997, 2.4000000000000004, 2.35, 4.55, 2.25, 2.85, 5.0, 2.15, 3.7, 2.15, 3.5, 4.0, 2.45, 3.3499999999999996, 2.45, 2.05, 2.65, 2.65, 2.2, 2.9, 1.6, 3.25, 2.35, 2.95, 2.5, 2.6999999999999997, 3.1, 2.85, 3.95, 3.55, 3.0]\n",
            "--->params {'epochs': 20, 'steps': 781, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n",
            "Epoch 1/20\n",
            "781/781 [==============================] - 779s 997ms/step - loss: 7.9226 - acc: 0.7059 - val_loss: 9.3256 - val_acc: 0.5817\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58173, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_16_0002.hdf5\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 783s 1s/step - loss: 8.0139 - acc: 0.6876 - val_loss: 9.5613 - val_acc: 0.5615\n",
            " - lr: 0.00050 \n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.58173\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 8.1210 - acc: 0.6792 - val_loss: 9.3598 - val_acc: 0.5845\n",
            " - lr: 0.00027 \n",
            "\n",
            "Epoch 00003: val_acc improved from 0.58173 to 0.58448, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_16_0002.hdf5\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 779s 998ms/step - loss: 7.8693 - acc: 0.7057 - val_loss: 9.2288 - val_acc: 0.5941\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00004: val_acc improved from 0.58448 to 0.59410, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_16_0002.hdf5\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 7.7227 - acc: 0.7228 - val_loss: 9.1524 - val_acc: 0.5997\n",
            " - lr: 0.00005 \n",
            "\n",
            "Epoch 00005: val_acc improved from 0.59410 to 0.59968, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_16_0002.hdf5\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 779s 998ms/step - loss: 7.7162 - acc: 0.7216 - val_loss: 9.2052 - val_acc: 0.5950\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.59968\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 7.6882 - acc: 0.7254 - val_loss: 9.1900 - val_acc: 0.5980\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.59968\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 781s 999ms/step - loss: 7.6695 - acc: 0.7283 - val_loss: 9.2143 - val_acc: 0.5976\n",
            " - lr: 0.00004 \n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.59968\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 780s 998ms/step - loss: 7.6521 - acc: 0.7290 - val_loss: 9.1468 - val_acc: 0.6021\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00009: val_acc improved from 0.59968 to 0.60211, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_16_0002.hdf5\n",
            "Epoch 10/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 7.6561 - acc: 0.7286 - val_loss: 9.2282 - val_acc: 0.5965\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60211\n",
            "Epoch 11/20\n",
            "781/781 [==============================] - 781s 1000ms/step - loss: 7.6310 - acc: 0.7323 - val_loss: 9.1478 - val_acc: 0.6019\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.60211\n",
            "Epoch 12/20\n",
            "781/781 [==============================] - 781s 999ms/step - loss: 7.6241 - acc: 0.7334 - val_loss: 9.1422 - val_acc: 0.6061\n",
            " - lr: 0.00003 \n",
            "\n",
            "Epoch 00012: val_acc improved from 0.60211 to 0.60606, saving model to /content/gdrive/My Drive/tinyimagenet-model/model_16_0002.hdf5\n",
            "Epoch 13/20\n",
            "781/781 [==============================] - 779s 998ms/step - loss: 7.6273 - acc: 0.7314 - val_loss: 9.1749 - val_acc: 0.6005\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.60606\n",
            "Epoch 14/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 7.6251 - acc: 0.7324 - val_loss: 9.2112 - val_acc: 0.5985\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.60606\n",
            "Epoch 15/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 7.6063 - acc: 0.7330 - val_loss: 9.1655 - val_acc: 0.6029\n",
            " - lr: 0.00002 \n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.60606\n",
            "Epoch 16/20\n",
            "781/781 [==============================] - 780s 998ms/step - loss: 7.5948 - acc: 0.7361 - val_loss: 9.1351 - val_acc: 0.6025\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.60606\n",
            "Epoch 17/20\n",
            "781/781 [==============================] - 781s 1s/step - loss: 7.6042 - acc: 0.7333 - val_loss: 9.2406 - val_acc: 0.5983\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.60606\n",
            "Epoch 18/20\n",
            "781/781 [==============================] - 779s 998ms/step - loss: 7.5872 - acc: 0.7380 - val_loss: 9.0485 - val_acc: 0.6060\n",
            " - lr: 0.00001 \n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.60606\n",
            "Epoch 19/20\n",
            "781/781 [==============================] - 780s 999ms/step - loss: 7.5972 - acc: 0.7366 - val_loss: 9.1546 - val_acc: 0.6027\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.60606\n",
            "Epoch 20/20\n",
            "781/781 [==============================] - 780s 998ms/step - loss: 7.5853 - acc: 0.7374 - val_loss: 9.2744 - val_acc: 0.5986\n",
            " - lr: 0.00000 \n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.60606\n",
            "LR Range :  5.039606e-07 0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEWCAYAAABSaiGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Aq3JkUKvrCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}